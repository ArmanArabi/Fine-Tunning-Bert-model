{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ca4b5520a43f45b89e58df5b9801e921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb05565de7934a089be5467bded24fa0",
              "IPY_MODEL_7b02c4e6e9e84f1aa42eb014a57b3912",
              "IPY_MODEL_f984eaa1d42949e1b4c3dd370e99a4bb"
            ],
            "layout": "IPY_MODEL_e7889f7f978f487cb3232fd616909250"
          }
        },
        "eb05565de7934a089be5467bded24fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b2b8f8f6b65464da4ce72ae281523d1",
            "placeholder": "​",
            "style": "IPY_MODEL_af8648ce146c4c038a895ba5aa625311",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7b02c4e6e9e84f1aa42eb014a57b3912": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d391c7e66b5a45fba2ffae2e719c500d",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2959e5cf24647508516bbd984d13ea9",
            "value": 48
          }
        },
        "f984eaa1d42949e1b4c3dd370e99a4bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4852ff90289e40d8977ad4cc80f80ccf",
            "placeholder": "​",
            "style": "IPY_MODEL_8a69116df3e0413b83ea6403928f7cb7",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.00kB/s]"
          }
        },
        "e7889f7f978f487cb3232fd616909250": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b2b8f8f6b65464da4ce72ae281523d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af8648ce146c4c038a895ba5aa625311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d391c7e66b5a45fba2ffae2e719c500d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2959e5cf24647508516bbd984d13ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4852ff90289e40d8977ad4cc80f80ccf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a69116df3e0413b83ea6403928f7cb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85075457aeb849f380df18db6a3c2cfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b7be2ad1ada462a8e79cd6ec3581467",
              "IPY_MODEL_629b762ec60645b6b1a06adc04e794f2",
              "IPY_MODEL_155db28de8aa43a0b4ec03c1c201d617"
            ],
            "layout": "IPY_MODEL_5e50c37906374440b7d6fbba7f206713"
          }
        },
        "9b7be2ad1ada462a8e79cd6ec3581467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0606d4303f804f2c89b0f498a17c0b19",
            "placeholder": "​",
            "style": "IPY_MODEL_23129f3980b648ea96147d6fa8db513c",
            "value": "vocab.txt: 100%"
          }
        },
        "629b762ec60645b6b1a06adc04e794f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efd689604f9747108387f83db089a662",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_690f5fa6fe84472a9d1d2289b35e0cad",
            "value": 231508
          }
        },
        "155db28de8aa43a0b4ec03c1c201d617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e088cad24a4944ed917eac23560efd14",
            "placeholder": "​",
            "style": "IPY_MODEL_af70b6091b18424d8d607516915a37a9",
            "value": " 232k/232k [00:00&lt;00:00, 1.44MB/s]"
          }
        },
        "5e50c37906374440b7d6fbba7f206713": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0606d4303f804f2c89b0f498a17c0b19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23129f3980b648ea96147d6fa8db513c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efd689604f9747108387f83db089a662": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "690f5fa6fe84472a9d1d2289b35e0cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e088cad24a4944ed917eac23560efd14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af70b6091b18424d8d607516915a37a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d821a4dd86349e49e5cbcd20a2704d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8fe34b49485a428080134b33d2aece12",
              "IPY_MODEL_79c7c9550cf148399e28809952aa14dc",
              "IPY_MODEL_052ced167fb34d5e9ab91546bf2145fa"
            ],
            "layout": "IPY_MODEL_12ae05af6d944e7788961f85f534af81"
          }
        },
        "8fe34b49485a428080134b33d2aece12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cf1a7a0fd674c479e7e7f0970e19d0e",
            "placeholder": "​",
            "style": "IPY_MODEL_079f75fc3bca48eaa41994a223143db4",
            "value": "tokenizer.json: 100%"
          }
        },
        "79c7c9550cf148399e28809952aa14dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5720c6530c78407e95d3be5c0ffefe96",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5246db88d33f405d8ba763638f54712f",
            "value": 466062
          }
        },
        "052ced167fb34d5e9ab91546bf2145fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45371a7cf1f34322816a24e7b7f217be",
            "placeholder": "​",
            "style": "IPY_MODEL_4f91cb8255eb49e1928e6837bc05d6f6",
            "value": " 466k/466k [00:00&lt;00:00, 8.90MB/s]"
          }
        },
        "12ae05af6d944e7788961f85f534af81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cf1a7a0fd674c479e7e7f0970e19d0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "079f75fc3bca48eaa41994a223143db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5720c6530c78407e95d3be5c0ffefe96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5246db88d33f405d8ba763638f54712f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45371a7cf1f34322816a24e7b7f217be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f91cb8255eb49e1928e6837bc05d6f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2db537ddb63448f2aee04b51b09f1ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4ca195b21044daa9209ee13b693876b",
              "IPY_MODEL_7dbe2de0f3504151bdc7fddca286e437",
              "IPY_MODEL_007d61256c6443e983de97e0927f23bf"
            ],
            "layout": "IPY_MODEL_0016c01a0f354abda8f746586facaef0"
          }
        },
        "e4ca195b21044daa9209ee13b693876b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52644a5e642d4b9dabd4e48b287d8111",
            "placeholder": "​",
            "style": "IPY_MODEL_a34042f4559646859f55d4f52c775c67",
            "value": "config.json: 100%"
          }
        },
        "7dbe2de0f3504151bdc7fddca286e437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4ab328a85ee490f8098c0dbd19bd196",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_688e39bce18945409d9904c2274d54fb",
            "value": 570
          }
        },
        "007d61256c6443e983de97e0927f23bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e2e2d5b8e7e41a39de76cc55f8ed825",
            "placeholder": "​",
            "style": "IPY_MODEL_054cec92fc8b41dcaf49531e647d5d8c",
            "value": " 570/570 [00:00&lt;00:00, 17.4kB/s]"
          }
        },
        "0016c01a0f354abda8f746586facaef0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52644a5e642d4b9dabd4e48b287d8111": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a34042f4559646859f55d4f52c775c67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4ab328a85ee490f8098c0dbd19bd196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "688e39bce18945409d9904c2274d54fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e2e2d5b8e7e41a39de76cc55f8ed825": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "054cec92fc8b41dcaf49531e647d5d8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03acda89295b4a3d842384049ed3ed9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1de0d1802d947669495c961e3ada510",
              "IPY_MODEL_18873deb1d6f4d3f9bf3f64436f072ba",
              "IPY_MODEL_057959767b4449459379d55776860aae"
            ],
            "layout": "IPY_MODEL_01ad09bbd733480289d14a720fe15ae7"
          }
        },
        "e1de0d1802d947669495c961e3ada510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be3821aa0af4445c85fcb0dc40452bc7",
            "placeholder": "​",
            "style": "IPY_MODEL_64ebac649fbc45faafbb58cafd498c6c",
            "value": "model.safetensors: 100%"
          }
        },
        "18873deb1d6f4d3f9bf3f64436f072ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d4c0876046543689a5e5c16545f3a8e",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_965f7fa431ad44cbaa296fbfa6393127",
            "value": 440449768
          }
        },
        "057959767b4449459379d55776860aae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3247900cb71425fb4d95112bce16aa4",
            "placeholder": "​",
            "style": "IPY_MODEL_e16e565bd7f34977bcbf79cee05c26af",
            "value": " 440M/440M [00:02&lt;00:00, 213MB/s]"
          }
        },
        "01ad09bbd733480289d14a720fe15ae7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be3821aa0af4445c85fcb0dc40452bc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64ebac649fbc45faafbb58cafd498c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d4c0876046543689a5e5c16545f3a8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "965f7fa431ad44cbaa296fbfa6393127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3247900cb71425fb4d95112bce16aa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e16e565bd7f34977bcbf79cee05c26af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6hOTEoD_ErbN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## determine use gpu or cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "if device == 'cpu' :\n",
        "    ## number of gpu\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    print(n_gpu)\n",
        "\n",
        "    ## name of current device\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    print(device_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ8vFsdGE8FH",
        "outputId": "6164bb7b-3b9c-4915-d4ec-0cd03476d78d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebln7cC9F1N2",
        "outputId": "ea1c50ca-4164-48f7-8945-4d7b7b74a467"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun  5 22:14:03 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/ArmanArabi/Fine-Tunning-Bert-model/main/in_domain_train.tsv\n"
      ],
      "metadata": {
        "id": "s4h1rQ-jZ8oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!curl -L https://raw.githubusercontent.com/ArmanArabi/Fine-Tunning-Bert-model/main/in_domain_train.tsv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrTXThCnN1m7",
        "outputId": "19cc3ae0-2c12-49e8-b947-223894620624"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "ks08\t1\t\the treats john very kindly .\n",
            "ks08\t0\t*\the treats john very kind .\n",
            "ks08\t0\t*\the treats john very shame .\n",
            "ks08\t1\t\the walked right into the wall .\n",
            "ks08\t0\t*\the walked right happy .\n",
            "ks08\t0\t*\the walked right the wall .\n",
            "ks08\t1\t\tjohn sang a song , mary played the piano .\n",
            "ks08\t1\t\twe found out that very lucrative jobs were in jeopardy .\n",
            "ks08\t0\t*\tmy these jobs are in jeopardy .\n",
            "ks08\t0\t*\tthe his jobs are in jeopardy .\n",
            "ks08\t1\t\ti think learning english is not easy at all .\n",
            "ks08\t1\t\ti doubt you can help me in understanding this .\n",
            "ks08\t1\t\ti think that learning english is not all that easy .\n",
            "ks08\t1\t\ti doubt if you can help me in understanding this .\n",
            "ks08\t1\t\ti am anxious for you to study english grammar hard .\n",
            "ks08\t0\t*\ti think that learning english to be not all that easy .\n",
            "ks08\t0\t*\ti doubt if you to help me in understanding this .\n",
            "ks08\t0\t*\ti am anxious for you should study english grammar hard .\n",
            "ks08\t1\t\tjohn not leave .\n",
            "ks08\t1\t\tjohn drink beer last night .\n",
            "ks08\t1\t\tjohn leave for seoul tomorrow ?\n",
            "ks08\t1\t\tjohn will study syntax , and mary , too .\n",
            "ks08\t1\t\the left .\n",
            "ks08\t1\t\the did not leave .\n",
            "ks08\t1\t\tstudents wanted to write a letter .\n",
            "ks08\t1\t\tstudents intended to surprise the teacher .\n",
            "ks08\t1\t\tstudents objected to the teacher .\n",
            "ks08\t1\t\tstudents sent letters to the teacher .\n",
            "ks08\t1\t\tit is crucial for john to show an interest .\n",
            "ks08\t1\t\tit is crucial that john should show an interest .\n",
            "ks08\t1\t\ti know i should go to the dentist 's , but i just do n't want to .\n",
            "ks08\t1\t\ti do n't really want to go to the dentist 's , but i know i should .\n",
            "ks08\t0\t*\tshe thought it was likely that everyone to fit into the car .\n",
            "ks08\t1\t\tshe thought it was likely that everyone might fit into the car .\n",
            "ks08\t1\t\tshe thought it was easy for everyone to fit into the car .\n",
            "ks08\t0\t*\tshe thought it was easy for everyone would fit into the car .\n",
            "ks08\t1\t\tthe umpire called off the game .\n",
            "ks08\t1\t\tthe umpire called the game off .\n",
            "ks08\t1\t\tthe two boys looked the word up .\n",
            "ks08\t1\t\tthe umpire fell off the deck .\n",
            "ks08\t1\t\tthe two boys looked up the high stairs .\n",
            "ks08\t1\t\tthe two boys looked up the high stairs from the floor .\n",
            "ks08\t0\t*\tthe umpire fell the deck off .\n",
            "ks08\t0\t*\tthe students looked the high stairs up from the floor .\n",
            "ks08\t0\t*\tthe students looked the high stairs up .\n",
            "ks08\t1\t\tthe umpire called it of .\n",
            "ks08\t0\t*\tthe umpire called off it .\n",
            "ks08\t0\t*\tthe umpire fell it off .\n",
            "ks08\t1\t\tthe umpire fell off it .\n",
            "ks08\t1\t\ta tall boy threw the ball .\n",
            "ks08\t1\t\tthe cat chased the long string .\n",
            "ks08\t1\t\tthat ball hit a student .\n",
            "ks08\t1\t\tthe piano played a song .\n",
            "ks08\t1\t\tthe piano kicked a student .\n",
            "ks08\t1\t\tthat ball sang a student .\n",
            "ks08\t1\t\tthe tall , handsome man kicked the ball .\n",
            "ks08\t1\t\tthe tall , kind , handsome man kicked the ball .\n",
            "ks08\t1\t\tthe happy , happy , happy , happy , happy , happy man sang a song .\n",
            "ks08\t1\t\tthe mother of the boy and the girl is arriving soon .\n",
            "ks08\t1\t\tthe mother of the boy and the girl are arriving soon .\n",
            "ks08\t1\t\tjohn saw the man with a telescope .\n",
            "ks08\t1\t\twe need more intelligent leaders .\n",
            "ks08\t1\t\tthe student enjoyed his english syntax class last semester .\n",
            "ks08\t1\t\tthe policeman met several young students in the park last night .\n",
            "ks08\t1\t\tit was the policeman that met several young students in the park last night .\n",
            "ks08\t1\t\tit was several young students that the policeman met in the park last night .\n",
            "ks08\t1\t\tit was last night that the policeman met several young students in the park .\n",
            "ks08\t0\t*\tit was several young students in that the policeman met the park last night .\n",
            "ks08\t0\t*\tit was in the park last night that the policeman met several young students .\n",
            "ks08\t1\t\twhere did the policeman meet several young students ?\n",
            "ks08\t1\t\twhat did you put in your box ?\n",
            "ks08\t1\t\twhere did you put the book ?\n",
            "ks08\t1\t\twhat did you do ?\n",
            "ks08\t1\t\tjohn looked up the inside of the chimney .\n",
            "ks08\t1\t\tjohn looked up the meaning of ` chanson ' .\n",
            "ks08\t1\t\twhat did he look up ?\n",
            "ks08\t1\t\twhere did he look ?\n",
            "ks08\t1\t\tup what did he look ?\n",
            "ks08\t1\t\twhat do you think the man who is standing by the door is doing now ?\n",
            "ks08\t1\t\twhat do you think he is doing now ?\n",
            "ks08\t1\t\thave you been to seoul ?\n",
            "ks08\t1\t\tjohn might go home , so might bill .\n",
            "ks08\t1\t\tjohn might pass the exam , and as might bill .\n",
            "ks08\t1\t\tif john can speak french fluently – which we all know he can – we will have no problems .\n",
            "ks08\t1\t\tjohn asked me to put the clothes in the cupboard , and to annoy him i really stuffed them there .\n",
            "ks08\t1\t\tjohn asked me to put the clothes in the cupboard , and to annoy him i stuffed them there .\n",
            "ks08\t0\t*\tjohn asked me to put the clothes in the cupboard , but i did so put the clothes in the suitcase .\n",
            "ks08\t1\t\tthe girls played in the water and swam under the bridge .\n",
            "ks08\t1\t\tthe children were neither in their rooms nor on the porch .\n",
            "ks08\t1\t\tmany people drink beer or wine .\n",
            "ks08\t0\t*\tmary waited for the bus and to go home .\n",
            "ks08\t0\t*\tlee went to the store and crazy .\n",
            "ks08\t1\t\tliked ice cream .\n",
            "ks08\t0\t*\tthe whistle tune was beautiful .\n",
            "ks08\t0\t*\tthe easily student finished his homework .\n",
            "ks08\t0\t*\tthe my dog is a terrier .\n",
            "ks08\t1\t\tthe monkey wants to leave the meeting .\n",
            "ks08\t0\t*\tthe monkey eager to leave the meeting .\n",
            "ks08\t1\t\tthe monkeys approved of their leader .\n",
            "ks08\t1\t\tthe men practice medicine .\n",
            "ks08\t0\t*\tthe men doctors of medicine .\n",
            "ks08\t1\t\tjohn read the book loudly .\n",
            "ks08\t1\t\tjohn sounded happy .\n",
            "ks08\t1\t\tjohn felt proud that his son won the game .\n",
            "ks08\t0\t*\tjohn sounded happily .\n",
            "ks08\t0\t*\tjohn sounded the student .\n",
            "ks08\t0\t*\tjohn sounded in the park .\n",
            "ks08\t0\t*\tthe monkeys seem want to leave the meeting .\n",
            "ks08\t1\t\tthe monkeys seem eager to leave the meeting .\n",
            "ks08\t0\t*\tjohn seems know about the bananas .\n",
            "ks08\t1\t\tjohn seems certain about the bananas .\n",
            "ks08\t1\t\tjohn came from seoul .\n",
            "ks08\t1\t\tthey put the book in the box .\n",
            "ks08\t1\t\tthey stayed in the hotel .\n",
            "ks08\t1\t\tthe fly fell into the soup .\n",
            "ks08\t1\t\tthe squirrel ran straight .\n",
            "ks08\t1\t\tthe squirrel ran right up the tree .\n",
            "ks08\t0\t*\tthe squirrel is right angry .\n",
            "ks08\t0\t*\tthe squirrel ran straight quickly .\n",
            "ks08\t0\t*\tthe squirrel ran right quickly .\n",
            "ks08\t1\t\tthis handsome man chased a dog .\n",
            "ks08\t1\t\ta man kicked that ball .\n",
            "ks08\t1\t\tthat tall woman chased a cat .\n",
            "ks08\t1\t\this friend kicked a ball .\n",
            "ks08\t1\t\tbill claims john believes mary thinks tom is honest .\n",
            "ks08\t1\t\tjane imagines bill claims john believes mary thinks tom is honest .\n",
            "ks08\t1\t\tthe little boy hit the child with a toy .\n",
            "ks08\t1\t\tchocolate cakes and pies are my favorite desserts .\n",
            "ks08\t0\t*\tthe children were in their rooms or happily .\n",
            "ks08\t1\t\tjohn suddenly got off the bus .\n",
            "ks08\t1\t\tjohn suddenly put off the customers .\n",
            "ks08\t0\t*\tjohn suddenly got the bus off .\n",
            "ks08\t1\t\tjohn suddenly put the customers off .\n",
            "ks08\t1\t\this second book came out earlier this year and became an instant best-seller .\n",
            "ks08\t1\t\twhen you book something such as a hotel room , you arrange to have it .\n",
            "ks08\t1\t\tprice quotes on selected categories will be sent out upon request .\n",
            "ks08\t1\t\tno doubt that he was forced to leave his family against his will .\n",
            "ks08\t1\t\the intended to will the large amount of money to frank .\n",
            "ks08\t1\t\tjane stood aside to let her pass .\n",
            "ks08\t1\t\the has a rail pass that 's right for you .\n",
            "ks08\t1\t\tit is important for us to spend time with children .\n",
            "ks08\t1\t\the was arrested for being drunk .\n",
            "ks08\t1\t\ti think that person we met last week is insane .\n",
            "ks08\t1\t\twe believe that he is quite reasonable .\n",
            "ks08\t1\t\ti forgot to return the book that i borrowed from the teacher .\n",
            "ks08\t1\t\ti am anxious that you should arrive on time .\n",
            "ks08\t1\t\ti am anxious for you to arrive on time .\n",
            "ks08\t0\t*\ti am anxious for you should arrive on time .\n",
            "ks08\t1\t\ti wonder whether you 'd be kind enough to give us information .\n",
            "ks08\t1\t\tif students study hard , teachers will be happy .\n",
            "ks08\t1\t\twhether they say it or not , most teachers expect their students to study hard .\n",
            "ks08\t1\t\tjohn put a book on the table .\n",
            "ks08\t1\t\tshe turned down his offer .\n",
            "ks08\t1\t\the looked at a book about swimming .\n",
            "ks08\t1\t\the talked to a girl about swimming .\n",
            "ks08\t1\t\the talked with a girl about swimming .\n",
            "ks08\t1\t\ti do n't know the people present .\n",
            "ks08\t0\t*\tcould you turn off the fire and on the light ?\n",
            "ks08\t0\t*\ti know the truth and that you are innocent .\n",
            "ks08\t1\t\tjohn refused the offer proudly .\n",
            "ks08\t1\t\ti consider john the best candidate .\n",
            "ks08\t1\t\ti saw him leaving the main building .\n",
            "ks08\t1\t\the took john to the school by the park .\n",
            "ks08\t1\t\tjohn sang a song and danced to the music .\n",
            "ks08\t1\t\tjohn wants to study linguistics in near future .\n",
            "ks08\t1\t\tthey told angelica to arrive early for the award .\n",
            "ks08\t1\t\tthat louise had abandoned the project surprised everyone .\n",
            "ks08\t1\t\ti know you like the back of my hand .\n",
            "ks08\t1\t\ttime flies like an arrow .\n",
            "ks08\t1\t\ti need to have that report on our web page by tomorrow .\n",
            "ks08\t1\t\tthe monkey scratched a boy on monday .\n",
            "ks08\t1\t\tjohn tagged the monkey in the forest .\n",
            "ks08\t1\t\tthe monkey was tagged in the forest by john .\n",
            "ks08\t1\t\tthe cat devoured the rat .\n",
            "ks08\t1\t\tthe rat devoured the cat .\n",
            "ks08\t1\t\tthis car stinks .\n",
            "ks08\t1\t\tit rains .\n",
            "ks08\t1\t\tthe committee disliked her proposal .\n",
            "ks08\t1\t\tthese books disappoint me .\n",
            "ks08\t1\t\tour neighbor takes his children to school in his car .\n",
            "ks08\t0\t*\tour neighbor take his children to school in his car .\n",
            "ks08\t1\t\tthe book , including all the chapters in the first section , is very interesting .\n",
            "ks08\t0\t*\tthe book , including all the chapters in the first section , are very interesting .\n",
            "ks08\t1\t\tthe effectiveness of teaching and learning depends on several factors .\n",
            "ks08\t0\t*\tthe effectiveness of teaching and learning depend on several factors .\n",
            "ks08\t1\t\tthe tornadoes that tear through this county every spring are more than just a nuisance .\n",
            "ks08\t0\t*\tthe tornadoes that tear through this county every spring is more than just a nuisance .\n",
            "ks08\t1\t\tthe lady singing with a boy is a genius , is n't he ?\n",
            "ks08\t0\t*\tthe lady singing with a boy is a genius , is n't she ?\n",
            "ks08\t1\t\twith their teacher , the kids have arrived safely , have n't they ?\n",
            "ks08\t0\t*\twith their teacher , the kids have arrived safely , has n't he ?\n",
            "ks08\t1\t\tthe kids have arrived safely .\n",
            "ks08\t1\t\tit could be more detrimental .\n",
            "ks08\t1\t\tis this teacher a genius ?\n",
            "ks08\t1\t\thave the kids arrived safely ?\n",
            "ks08\t1\t\tcould it be more detrimental ?\n",
            "ks08\t1\t\tthe kids in our class have arrived safely .\n",
            "ks08\t0\t*\thave in our class the kids arrived safely ?\n",
            "ks08\t1\t\this girlfriend bought this computer .\n",
            "ks08\t1\t\tthunder frightens the dog .\n",
            "ks08\t1\t\tthe dog fears thunder .\n",
            "ks08\t1\t\this girlfriend bought this computer for him .\n",
            "ks08\t1\t\tthe child broke the teapot by accident .\n",
            "ks08\t1\t\tthis computer was bought for him by his girlfriend .\n",
            "ks08\t1\t\tthe teapot was broken by the child by accident .\n",
            "ks08\t1\t\tthis item belongs to the student .\n",
            "ks08\t0\t*\tthe student is belonged to by this item .\n",
            "ks08\t1\t\the remained a good friend to me .\n",
            "ks08\t0\t*\ta good friend is remained to me .\n",
            "ks08\t1\t\tjohn gave the boys the cds .\n",
            "ks08\t1\t\tmy mother baked me a birthday cake .\n",
            "ks08\t1\t\tshe was sent a review copy of the book by the publisher .\n",
            "ks08\t1\t\tshe was sent a review copy of the book .\n",
            "ks08\t1\t\tjohn gave the cds to the boys .\n",
            "ks08\t1\t\tthe publisher sent a review copy of the book to her .\n",
            "ks08\t1\t\tmy mother baked a cake for me .\n",
            "ks08\t1\t\tthe cds were given to the boys by john .\n",
            "ks08\t1\t\ta review copy of the book was sent to her by the publisher .\n",
            "ks08\t1\t\tthis nice cake was baked for me by my mother .\n",
            "ks08\t1\t\tthis is my ultimate goal .\n",
            "ks08\t1\t\tmichelle became an architect .\n",
            "ks08\t1\t\tthey elected graham chairman .\n",
            "ks08\t0\t*\tchairman was elected graham .\n",
            "ks08\t0\t*\tthe best writer was considered andrew .\n",
            "ks08\t1\t\tjohn made kim a great doll .\n",
            "ks08\t1\t\tthe situation became terrible .\n",
            "ks08\t1\t\tthis map is what he wants .\n",
            "ks08\t1\t\tthe message was that you should come on time .\n",
            "ks08\t1\t\ti made kim angry .\n",
            "ks08\t1\t\ti consider him immoral .\n",
            "ks08\t1\t\ti regard andrew as the best writer .\n",
            "ks08\t1\t\tthey spoil their kids rotten .\n",
            "ks08\t1\t\tjohn put books in the box .\n",
            "ks08\t1\t\tjohn talked to bill about the exam .\n",
            "ks08\t1\t\tshe reminded him of the last time they met .\n",
            "ks08\t1\t\tthey would inform mary of any success they have made .\n",
            "ks08\t1\t\tjohn gave a book to the student .\n",
            "ks08\t1\t\tjohn bought a book for the student .\n",
            "ks08\t1\t\tthe bus stopped suddenly .\n",
            "ks08\t1\t\tshakespeare wrote his plays a long time ago .\n",
            "ks08\t1\t\tthey went to the theater in london .\n",
            "ks08\t1\t\the failed chemistry because he ca n't understand it .\n",
            "ks08\t0\t*\tjohn gave tom a book a record .\n",
            "ks08\t1\t\ti saw this film several times last year during the summer .\n",
            "ks08\t1\t\tmy uncle visited today .\n",
            "ks08\t0\t*\ttoday was visited by my uncle .\n",
            "ks08\t1\t\tthe termites destroyed the sand castle .\n",
            "ks08\t1\t\tbeing honest is not an easy task .\n",
            "ks08\t1\t\tthat john passed surprised her .\n",
            "ks08\t1\t\tto finish this work on time is almost unexpected .\n",
            "ks08\t1\t\tunder the bed is a safe place to hide .\n",
            "ks08\t1\t\ti sent a surprise present to john .\n",
            "ks08\t1\t\tthey wondered what she did yesterday .\n",
            "ks08\t1\t\tthey believed that everybody would pass the test .\n",
            "ks08\t1\t\tare you going on holiday before or after easter ? i prefer after easter .\n",
            "ks08\t1\t\tthat john passed surprised her , did n't it ?\n",
            "ks08\t1\t\tthat the march should go ahead and that it should be cancelled have been argued by different people at different times .\n",
            "ks08\t0\t*\tthat the march should go ahead and that it should be cancelled has been argued by different people at different times .\n",
            "ks08\t1\t\tto finish it on time made quite a statement , did n't it ?\n",
            "ks08\t1\t\tto delay the march and to go ahead with it have been argued by different people at different times .\n",
            "ks08\t0\t*\tto delay the march and to go ahead with it has been argued by different people at different times .\n",
            "ks08\t1\t\tthe little cat devoured a mouse last night .\n",
            "ks08\t1\t\tjohn left very early .\n",
            "ks08\t1\t\tjohn studied hard to pass the exam .\n",
            "ks08\t1\t\tshe disappeared when the main party arrived .\n",
            "ks08\t1\t\ta boy hit the ball .\n",
            "ks08\t1\t\tthe students felt comfortable in the class .\n",
            "ks08\t1\t\tjohn gave a book to the students .\n",
            "ks08\t1\t\tjohn died last night .\n",
            "ks08\t1\t\tjohn bought a lot of books for his sons .\n",
            "ks08\t1\t\tjohn promised bill to leave tomorrow morning .\n",
            "ks08\t1\t\tjohn deprived his sons of game cards .\n",
            "ks08\t1\t\tmary received an award from the department .\n",
            "ks08\t1\t\tjohn told the rumor to his friend .\n",
            "ks08\t1\t\tjohn put his books in the attic .\n",
            "ks08\t1\t\tthe government kept all the money .\n",
            "ks08\t1\t\tjohn hit the ball with a bat .\n",
            "ks08\t1\t\tjohn wiped the window with a towel .\n",
            "ks08\t1\t\tthe cat chased pat the mouse .\n",
            "ks08\t1\t\tthe mouse was chased by the cat .\n",
            "ks08\t1\t\tthere still remains an issue to be solved .\n",
            "ks08\t1\t\tthere lived a man with his grandson .\n",
            "ks08\t1\t\tthere arrived a tall , red haired and incredibly well dressed man .\n",
            "ks08\t0\t*\tthere sang a man with a pipe .\n",
            "ks08\t0\t*\tthere dances a man with an umbrella .\n",
            "ks08\t1\t\tjohn resembles his mother .\n",
            "ks08\t1\t\ta is similar to b .\n",
            "ks08\t1\t\tjohn runs into the house .\n",
            "ks08\t1\t\tmary looked at the sky .\n",
            "ks08\t1\t\tthe school awarded a few of the girls in miss kim 's class scholarships .\n",
            "ks08\t1\t\tshe was the nicest teacher in the senior school .\n",
            "ks08\t1\t\tthey elected him america 's 31st president .\n",
            "ks08\t1\t\tthe next morning we set out for seoul .\n",
            "ks08\t1\t\tdoing syntax is not easy .\n",
            "ks08\t1\t\the saw the man with the stick .\n",
            "ks08\t1\t\tthey parted the best of friends .\n",
            "ks08\t1\t\tin the summer we always go to france .\n",
            "ks08\t1\t\tlast year i saw this film several times .\n",
            "ks08\t1\t\the baked tom the bread last night .\n",
            "ks08\t1\t\tthat they have completed the course is amazing .\n",
            "ks08\t1\t\tthe teacher made students happy .\n",
            "ks08\t1\t\twe reminded him of the agreement .\n",
            "ks08\t1\t\tin the garden stands a statue .\n",
            "ks08\t0\t*\tin the garden stand a statue .\n",
            "ks08\t1\t\tamong the guests was sitting my friend louise .\n",
            "ks08\t0\t*\tamong the guests were sitting my friend louise .\n",
            "ks08\t1\t\tthis proved my hypothesis .\n",
            "ks08\t1\t\tthe students all enjoyed that summer .\n",
            "ks08\t1\t\tthe students all worked that summer .\n",
            "ks08\t1\t\tthe scientist made her a robot .\n",
            "ks08\t1\t\tthe students called me a teacher .\n",
            "ks08\t1\t\ta big green insect flew into the soup .\n",
            "ks08\t1\t\tjohn 's mother sent a letter to mary .\n",
            "ks08\t1\t\twe placed the cheese in the refrigerator .\n",
            "ks08\t1\t\tfrank threw himself into the sofa .\n",
            "ks08\t1\t\tthe ice melted .\n",
            "ks08\t1\t\tthe vacuum cleaner frightens the child .\n",
            "ks08\t1\t\tscientists found that the birds sang well in the evenings , but performed badly in the mornings .\n",
            "ks08\t0\t*\tjohn put his gold .\n",
            "ks08\t0\t*\tjohn put his gold safe .\n",
            "ks08\t0\t*\tjohn put his gold to be under the bathtub .\n",
            "ks08\t1\t\tjohn put his gold under the bathtub .\n",
            "ks08\t1\t\tthis is the box in which john put his gold .\n",
            "ks08\t1\t\tthis is the gold that john put under the bathtub .\n",
            "ks08\t0\t*\tthe king kept put his gold under the bathtub .\n",
            "ks08\t1\t\tthe king kept putting his gold under the bathtub .\n",
            "ks08\t1\t\tthe defendant denied the accusation .\n",
            "ks08\t0\t*\tthe defendant denied .\n",
            "ks08\t1\t\tthe teacher handed the student a book .\n",
            "ks08\t0\t*\tthe teacher handed the student .\n",
            "ks08\t1\t\tthey want to leave the meeting .\n",
            "ks08\t0\t*\tthey eager to leave the meeting .\n",
            "ks08\t1\t\tthe senators know that the president is telling a lie .\n",
            "ks08\t0\t*\tthe senators certain that the president is telling a lie .\n",
            "ks08\t1\t\tbe eager to leave the meeting .\n",
            "ks08\t0\t*\tthe senators to be certain that the president is telling a lie .\n",
            "ks08\t0\t*\tthe senators be certain that the president is telling a lie .\n",
            "ks08\t1\t\ttom offered advice to his students in his office .\n",
            "ks08\t1\t\ttom offered advice to his students with love .\n",
            "ks08\t1\t\tjohn kept him behind the garage .\n",
            "ks08\t0\t*\tjohn stayed kim behind the garage .\n",
            "ks08\t0\t*\tjohn placed him busy .\n",
            "ks08\t1\t\tjohn kept him busy .\n",
            "ks08\t0\t*\tjohn stayed him busy .\n",
            "ks08\t0\t*\tjohn placed behind the counter .\n",
            "ks08\t0\t*\tjohn kept behind the counter .\n",
            "ks08\t1\t\tjohn stayed behind the counter .\n",
            "ks08\t1\t\tjohn deposited some money in the bank .\n",
            "ks08\t1\t\tjohn deposited some money in the bank on friday .\n",
            "ks08\t0\t*\tthe un blamed global warming on humans on natural causes .\n",
            "ks08\t1\t\tkim and sandy met in seoul in the lobby of the lotte hotel in march .\n",
            "ks08\t1\t\tjohn deposited some money in the checking account and mary did the same thing .\n",
            "ks08\t1\t\tjohn deposited some money in the checking account on friday and mary did the same thing .\n",
            "ks08\t1\t\tjohn deposited some money in the checking account on friday and mary did the same thing on monday .\n",
            "ks08\t0\t*\tjohn deposited some money in the checking account and mary did the same thing in the savings account .\n",
            "ks08\t0\t*\tjohn gave a present to the student and mary did the same thing to the teacher .\n",
            "ks08\t0\t*\tjohn locked fido in the garage and mary did so in the room .\n",
            "ks08\t0\t*\tjohn ate a carrot and mary did so a radish .\n",
            "ks08\t1\t\tkim jogs on the hill .\n",
            "ks08\t1\t\tkim jogs under the hill .\n",
            "ks08\t1\t\tkim jogs over the hill .\n",
            "ks08\t1\t\tkim depends .\n",
            "ks08\t1\t\tkim relies on sandy .\n",
            "ks08\t1\t\tkim depends on sandy .\n",
            "ks08\t0\t*\tkim depends at sandy .\n",
            "ks08\t1\t\tjohn met a student in the park .\n",
            "ks08\t0\t*\tjohn met in the park a student .\n",
            "ks08\t0\t*\tthe problem disappeared the accusation .\n",
            "ks08\t1\t\tthe problem disappeared .\n",
            "ks08\t0\t*\tthe boy gave the book .\n",
            "ks08\t1\t\tthe boy gave the baby the book .\n",
            "ks08\t1\t\tthe bird devours the worm .\n",
            "ks08\t1\t\tthe birds devour the worm .\n",
            "ks08\t1\t\tevery photo of max and sketch by his students appeared in the magazine .\n",
            "ks08\t1\t\tno photo of max and sketch by his students appeared in the magazine .\n",
            "ks08\t0\t*\tsketch by his students appeared in the magazine .\n",
            "ks08\t1\t\tthe present king of country music is more popular than the last one .\n",
            "ks08\t0\t*\tthe king of rock and roll is more popular than the one of country music .\n",
            "ks08\t1\t\twhich student were you talking about ?\n",
            "ks08\t0\t*\tjohn put in the box .\n",
            "ks08\t0\t*\tin the box put john the book .\n",
            "ks08\t1\t\tthe election results surprised everybody .\n",
            "ks08\t1\t\tthat he won the election surprised everybody .\n",
            "ks08\t1\t\tjohn disappeared .\n",
            "ks08\t0\t*\tjohn disappeared bill .\n",
            "ks08\t1\t\tjohn coughed .\n",
            "ks08\t0\t*\tjohn coughed the money .\n",
            "ks08\t1\t\tthe president looked weary .\n",
            "ks08\t1\t\tthe teacher became tired of the students .\n",
            "ks08\t1\t\tthe lasagna tasted delicious .\n",
            "ks08\t1\t\tjohn remained somewhat calm .\n",
            "ks08\t1\t\tthe jury seemed ready to leave .\n",
            "ks08\t1\t\tjohn became a success .\n",
            "ks08\t1\t\tjohn seemed a fool .\n",
            "ks08\t1\t\tjohn remained a student .\n",
            "ks08\t1\t\tjohn saw fred .\n",
            "ks08\t1\t\talice typed the letter .\n",
            "ks08\t1\t\tclinton supported the health care bill .\n",
            "ks08\t1\t\traccoons destroyed the garden .\n",
            "ks08\t1\t\tthe school board leader asked the students a question .\n",
            "ks08\t1\t\tjohn taught new students english syntax .\n",
            "ks08\t1\t\tthe school board leader asked a question of the students .\n",
            "ks08\t1\t\tthe sexual revolution makes some people uncomfortable .\n",
            "ks08\t1\t\tad agencies call young people generation x-ers .\n",
            "ks08\t1\t\thistorians believe fdr to be our most effective president .\n",
            "ks08\t0\t*\tjohn carried to the door .\n",
            "ks08\t1\t\ttom locked fido in the garage .\n",
            "ks08\t1\t\ttom bathed fido in the garage .\n",
            "ks08\t1\t\ttom placed it under the table .\n",
            "ks08\t1\t\ttom played it under the table .\n",
            "ks08\t1\t\ti wonder if you will come back tomorrow .\n",
            "ks08\t1\t\tyou would have a reply if you come back tomorrow .\n",
            "ks08\t1\t\ttom hid the manuscript in the cupboard .\n",
            "ks08\t1\t\tfred hired sharon to change the oil .\n",
            "ks08\t1\t\tthey pushed the prisoners into the truck .\n",
            "ks08\t1\t\tfrank hopes to persuade harry to make the cook wash the dishes .\n",
            "ks08\t1\t\tgeorge mailed the attorney his photograph of the accident .\n",
            "ks08\t1\t\ttom keeps asking karen 's sister to buy the car .\n",
            "ks08\t1\t\tjane left the book on the table .\n",
            "ks08\t1\t\twe have not confirmed whether the flight had been booked .\n",
            "ks08\t1\t\twe saw him beaten by the champion .\n",
            "ks08\t1\t\tthey confined his remarks to the matter under discussion .\n",
            "ks08\t0\t*\toliver ascribed his longevity there .\n",
            "ks08\t0\t*\toliver mentioned charles the problem .\n",
            "ks08\t0\t*\toliver fined ten pounds to the prisoner .\n",
            "ks08\t0\t*\toliver drove me a lunatic .\n",
            "ks08\t0\t*\toliver addressed the king the letter .\n",
            "ks08\t1\t\tthe students of english from seoul faced many issues in the process of interpreting , transcribing , and editing the poems .\n",
            "ks08\t1\t\tthe love of my life and father of my children would never do such a thing .\n",
            "ks08\t1\t\tthe museum displayed no painting by miro or drawing by klee .\n",
            "ks08\t1\t\tby law , every dog and cat in the area has to be neutered .\n",
            "ks08\t1\t\tlearning to use a language freely and fully is a lengthy and arduous process .\n",
            "ks08\t1\t\tkim put the book in the box .\n",
            "ks08\t0\t*\tkim put the book .\n",
            "ks08\t0\t*\tis putting the book in the box .\n",
            "ks08\t0\t*\ttalked with bill about the exam .\n",
            "ks08\t1\t\tthey wrote to her .\n",
            "ks08\t1\t\tthey are kind to her .\n",
            "ks08\t1\t\tthey want to write to her .\n",
            "ks08\t0\t*\tthey want to wrote to her .\n",
            "ks08\t1\t\tthey want to be kind to her .\n",
            "ks08\t0\t*\tthey want to are kind to her .\n",
            "ks08\t1\t\tthe student knows the answers .\n",
            "ks08\t1\t\tthe student knew the answers .\n",
            "ks08\t1\t\tthe students know the answers .\n",
            "ks08\t0\t*\tthe student knowing the answers .\n",
            "ks08\t0\t*\tthe student known the answers .\n",
            "ks08\t1\t\the is writing another long book about beavers .\n",
            "ks08\t1\t\tbroadly speaking , the project was successful .\n",
            "ks08\t1\t\the is proud of his son 's passing the bar exam .\n",
            "ks08\t1\t\tthe chicken has eaten .\n",
            "ks08\t1\t\tthe chicken was eaten .\n",
            "ks08\t1\t\tseen from this perspective , there is no easy solution .\n",
            "ks08\t1\t\tthe monkeys kept forgetting their lines .\n",
            "ks08\t0\t*\tthe monkeys kept forgot their lines .\n",
            "ks08\t0\t*\tthe monkeys kept forgotten their lines .\n",
            "ks08\t0\t*\twe caught them ate the bananas .\n",
            "ks08\t0\t*\twe caught them eat the bananas .\n",
            "ks08\t0\t*\twe caught them eaten the bananas .\n",
            "ks08\t1\t\tjohn made mary cook korean food .\n",
            "ks08\t0\t*\tjohn made mary to cook korean food .\n",
            "ks08\t0\t*\tjohn made mary cooking korean food .\n",
            "ks08\t1\t\tthe monkey seems despondent that it is in a cage .\n",
            "ks08\t1\t\tthe monkey seems despondent .\n",
            "ks08\t1\t\the seems intelligent to study medicine .\n",
            "ks08\t0\t*\the seems intelligent to study medicine .\n",
            "ks08\t1\t\tmonkeys are eager to leave .\n",
            "ks08\t0\t*\tmonkeys are eager leaving the compound .\n",
            "ks08\t0\t*\tthe chickens seem fond with the farmer .\n",
            "ks08\t1\t\tthe foxes seem compatible with the chickens .\n",
            "ks08\t0\t*\tthe foxes seem compatible for the chickens .\n",
            "ks08\t1\t\tthese are similar to the bottles .\n",
            "ks08\t0\t*\tthese are similar with the bottles .\n",
            "ks08\t1\t\tthe teacher is proud of his students .\n",
            "ks08\t0\t*\tthe teacher is proud with his students .\n",
            "ks08\t1\t\tthe contract is subject to approval by my committee .\n",
            "ks08\t0\t*\tthe contract is subject for approval by my committee .\n",
            "ks08\t1\t\tthere exists only one truly amphibian mammal .\n",
            "ks08\t1\t\tthere arose a great storm .\n",
            "ks08\t1\t\tthere exist few solutions which are cost-effective .\n",
            "ks08\t1\t\tthere is a riot in the park .\n",
            "ks08\t1\t\tthere remained just a few problems to be solved .\n",
            "ks08\t0\t*\tthere runs a man in the park .\n",
            "ks08\t0\t*\tthere sings a man loudly .\n",
            "ks08\t1\t\tthey believe that charles darwin 's theory of evolution is just a scientific theory .\n",
            "ks08\t1\t\tthey believe charles darwin 's theory of evolution is just a scientific theory .\n",
            "ks08\t1\t\tjohn demanded that she stop phoning him .\n",
            "ks08\t1\t\tjoe warned the class that the exam would be difficult .\n",
            "ks08\t1\t\twe told tom that he should consult an accountant .\n",
            "ks08\t1\t\tmary convinced me that the argument was sound .\n",
            "ks08\t1\t\ttom intends for sam to review that book .\n",
            "ks08\t1\t\tjohn would prefer for the children to finish the oatmeal .\n",
            "ks08\t1\t\tfor john to either make up such a story or repeat it is outrageous .\n",
            "ks08\t1\t\tfor john either to make up such a story or to repeat it is outrageous .\n",
            "ks08\t1\t\tfor john to tell bill such a lie and bill to believe it is outrageous .\n",
            "ks08\t1\t\tjohn intends to review the book .\n",
            "ks08\t1\t\tjohn would prefer to finish the oatmeal .\n",
            "ks08\t1\t\ttom tried to ask a question .\n",
            "ks08\t0\t*\ttom tried for bill to ask a question .\n",
            "ks08\t1\t\ttom tends to avoid confrontations .\n",
            "ks08\t0\t*\ttom tends for mary to avoid confrontations .\n",
            "ks08\t1\t\tjoe hoped to find a solution .\n",
            "ks08\t0\t*\tjoe hoped for beth to find a solution .\n",
            "ks08\t1\t\tjohn believed it .\n",
            "ks08\t1\t\tjohn believed that he is honest .\n",
            "ks08\t1\t\tjohn mentioned the issue to me .\n",
            "ks08\t1\t\tjohn mentioned to me that the question is an issue .\n",
            "ks08\t1\t\tshe pinched his arm as hard as she could .\n",
            "ks08\t0\t*\tshe pinched that he feels pain .\n",
            "ks08\t1\t\twe hope that such a vaccine could be available in ten years .\n",
            "ks08\t0\t*\twe hope the availability of such a vaccine in ten years .\n",
            "ks08\t1\t\tcohen proved the independence of the continuum hypothesis .\n",
            "ks08\t1\t\tcohen proved that the continuum hypothesis was independent .\n",
            "ks08\t1\t\tjohn bothers me .\n",
            "ks08\t1\t\tthat john coughed bothers me .\n",
            "ks08\t1\t\tjohn loves bill .\n",
            "ks08\t0\t*\tthat john coughs loves bill .\n",
            "ks08\t1\t\tthat john sold the ostrich surprised bill .\n",
            "ks08\t1\t\tfor john to train his horse would be desirable .\n",
            "ks08\t1\t\tto train his horse would be desirable .\n",
            "ks08\t1\t\tthat the king or queen be present is a requirement on all royal weddings .\n",
            "ks08\t1\t\twhich otter you should adopt first is unclear .\n",
            "ks08\t0\t*\tthat tom missed the lecture was enjoyable .\n",
            "ks08\t0\t*\tfor john to remove the mother is undeniable .\n",
            "ks08\t0\t*\thow much money gordon spent is true .\n",
            "ks08\t1\t\ttom is confident that the elephants respect him .\n",
            "ks08\t1\t\ttom is insistent that the defendants be truthful .\n",
            "ks08\t1\t\ttom seems eager for her brother to catch a cold .\n",
            "ks08\t1\t\ttom seems eager to catch a cold .\n",
            "ks08\t1\t\ti am ashamed that i neglected you .\n",
            "ks08\t1\t\ti am delighted that mary finished his thesis .\n",
            "ks08\t1\t\twe were thankful that no one had been hurt .\n",
            "ks08\t1\t\twe were glad it was over .\n",
            "ks08\t1\t\tbill alleged that fred signed the check .\n",
            "ks08\t1\t\twe believe that the directors were present .\n",
            "ks08\t1\t\twe convinced him that the operation is safe .\n",
            "ks08\t0\t*\talan is thinking about that his students are eager to learn english .\n",
            "ks08\t0\t*\tfred is counting on for tom to make an announcement .\n",
            "ks08\t1\t\tthe outcome depends on how many candidates participate in the election .\n",
            "ks08\t1\t\tfred is thinking about whether he should stay in seoul .\n",
            "ks08\t1\t\tthe offer made smith admire the administrators .\n",
            "ks08\t1\t\tjohn tried to make sam let george ask bill to keep delivering the mail .\n",
            "ks08\t1\t\tjohn enjoyed drawing trees for his syntax homework .\n",
            "ks08\t1\t\tthe picture on the wall reminded him of his country .\n",
            "ks08\t1\t\tfree enterprise is compatible with american values and traditions .\n",
            "ks08\t1\t\twe need to be in frequent contact with the clients .\n",
            "ks08\t1\t\tacknowledge that everyone has limits .\n",
            "ks08\t1\t\twe are aware of the existing problems .\n",
            "ks08\t0\t*\twhy do n't you leaving me concentrate on my work ?\n",
            "ks08\t0\t*\tthe general commended that all troops was in dress uniform .\n",
            "ks08\t0\t*\tmy morning routine features swim free styles slowly for one hour .\n",
            "ks08\t0\t*\tyou should avoid to travel in the rush hour .\n",
            "ks08\t0\t*\tyou should attempt answering every question .\n",
            "ks08\t0\t*\tthe authorities blamed greenpeace with the bombing .\n",
            "ks08\t0\t*\tthe authorities charged the students of the cheating .\n",
            "ks08\t0\t*\tsharon has been eager finishing the book .\n",
            "ks08\t0\t*\twe respect mary 's desire for becoming famous .\n",
            "ks08\t0\t*\tjohn referred from the building .\n",
            "ks08\t0\t*\tjohn died to heart disease .\n",
            "ks08\t0\t*\twe were glad what to do .\n",
            "ks08\t0\t*\tshe was busy to make lunch .\n",
            "ks08\t1\t\tthe constant rain forced the abandonment of the next day 's competitions .\n",
            "ks08\t1\t\taloe may have an analgesic effect on inflammation and minor skin irritations .\n",
            "ks08\t1\t\tthe public never had faith in his ability to handle the job .\n",
            "ks08\t1\t\the repeated his claim that the people backed his action .\n",
            "ks08\t1\t\twe made them take the money .\n",
            "ks08\t0\t*\twe made them are rude .\n",
            "ks08\t1\t\tdo not use these words in the beginning of a sentence .\n",
            "ks08\t1\t\twe know the defendants seem eager to testify against the criminal .\n",
            "ks08\t1\t\tjane is n't sure whether the students keep the books .\n",
            "ks08\t0\t*\tbook is available in most countries .\n",
            "ks08\t0\t*\tstudent studies english for 4 hours a day .\n",
            "ks08\t1\t\tstudents study english for 4 hours a day .\n",
            "ks08\t1\t\this friend learned dancing .\n",
            "ks08\t1\t\tmy bother 's friend learned dancing .\n",
            "ks08\t1\t\tthe president 's bodyguard learned surveillance .\n",
            "ks08\t1\t\tthe king of rock and roll 's records led to dancing .\n",
            "ks08\t1\t\tpresident lincoln delivered his gettysburg address in 1863 .\n",
            "ks08\t0\t*\tpresident lincoln delivered her gettysburg address in 1863 .\n",
            "ks08\t0\t*\tafter reading the pamphlet , judy threw them into the garbage can .\n",
            "ks08\t1\t\tafter the party , i asked myself why i had faxed invitations to everyone in my office building .\n",
            "ks08\t1\t\tedward usually remembered to send a copy of his e-mail to himself .\n",
            "ks08\t1\t\tno john smiths attended the meeting .\n",
            "ks08\t1\t\tthis john smith lives in seoul .\n",
            "ks08\t1\t\tthere are three davids in my class .\n",
            "ks08\t1\t\tit 's nothing like the america i remember .\n",
            "ks08\t1\t\tmy brother is an einstein at maths .\n",
            "ks08\t1\t\tin the book , he talks about his ups and downs at school .\n",
            "ks08\t1\t\tif john wants to succeed in corporate life , he has to know the rules of the game .\n",
            "ks08\t1\t\tthe critique of plato 's republic was written from a contemporary point of view .\n",
            "ks08\t1\t\tthe characters in shakespeare 's twelfth night live in a world that has been turned upside-down .\n",
            "ks08\t0\t*\tthe characters in shakespeare 's twelfth night lives in a world that has been turned upside-down .\n",
            "ks08\t1\t\tstudents studying english read conrad 's heart of darkness while at university .\n",
            "ks08\t1\t\tyou is the only person that i can rely on .\n",
            "ks08\t0\t*\tyou are the only person that i can rely on .\n",
            "ks08\t1\t\the is the only person that i can rely on .\n",
            "ks08\t0\t*\the are the only person that i can rely on .\n",
            "ks08\t1\t\tthe boy swims .\n",
            "ks08\t0\t*\tthe boys swim .\n",
            "ks08\t1\t\tking prawns cooked in chili salt and pepper was very much better , a simple dish deliciously executed .\n",
            "ks08\t1\t\tfour pounds was quite a bit of money in 1950 and it was not easy to come by .\n",
            "ks08\t1\t\tfive pounds is a lot of money .\n",
            "ks08\t0\t*\tfive pounds are a lot of money .\n",
            "ks08\t1\t\ttwo drops sanitize anything in your house .\n",
            "ks08\t0\t*\ttwo drops sanitize anything in your house .\n",
            "ks08\t1\t\tfifteen dollars in a week is much .\n",
            "ks08\t0\t*\tfifteen dollars in a week are not much .\n",
            "ks08\t1\t\tfifteen years represents a long period of his life .\n",
            "ks08\t0\t*\tfifteen years represent a long period of his life .\n",
            "ks08\t1\t\ttwo miles is as far as they can walk .\n",
            "ks08\t0\t*\ttwo miles are as far as they can walk .\n",
            "ks08\t1\t\tthis government have been more transparent in the way they have dealt with public finances than any previous government .\n",
            "ks08\t1\t\tthis government has been more transparent in the way they have dealt with public finances than any previous government .\n",
            "ks08\t1\t\tin preparation for the return fixture this team has trained more efficiently than they had in recent months .\n",
            "ks08\t1\t\tshe does n't believe much of that story .\n",
            "ks08\t1\t\twe listened to as little of his speech as possible .\n",
            "ks08\t1\t\thow much of the fresco did the flood damage ?\n",
            "ks08\t0\t*\tshe does n't believe much story .\n",
            "ks08\t0\t*\twe listened to as little speech as possible .\n",
            "ks08\t0\t*\thow much fresco did the flood damage ?\n",
            "ks08\t0\t*\ti read some book .\n",
            "ks08\t1\t\tone of the people was dying of thirst .\n",
            "ks08\t1\t\tmany of the people were dying of thirst .\n",
            "ks08\t0\t*\tone people was dying of thirst .\n",
            "ks08\t1\t\tmany people were dying of thirst .\n",
            "ks08\t1\t\teach of the suggestions is acceptable .\n",
            "ks08\t1\t\tneither of the cars has air conditioning .\n",
            "ks08\t1\t\tnone of these men wants to be president .\n",
            "ks08\t1\t\tmost of the children are here .\n",
            "ks08\t1\t\tsome of the soup needs more salt .\n",
            "ks08\t1\t\tsome of the diners need menus .\n",
            "ks08\t1\t\tall of the land belongs to the government .\n",
            "ks08\t1\t\tall of these cars belong to me .\n",
            "ks08\t1\t\tjohn is in the room .\n",
            "ks08\t1\t\ti am fond of him .\n",
            "ks08\t1\t\tmost of john 's boat has been repainted .\n",
            "ks08\t1\t\tsome of the record contains evidence of wrongdoing .\n",
            "ks08\t1\t\tmuch of that theory is unfounded .\n",
            "ks08\t0\t*\tone of the story has appeared in your newspaper .\n",
            "ks08\t1\t\the is afraid of foxes .\n",
            "ks08\t1\t\tit is a wooden desk .\n",
            "ks08\t1\t\tit is the main street .\n",
            "ks08\t0\t*\tit is an alive fish .\n",
            "ks08\t0\t*\tthey are afraid people .\n",
            "ks08\t0\t*\tthis objection is main .\n",
            "ks08\t0\t*\tthis fact is key .\n",
            "ks08\t1\t\tthe man eager to start the meeting is john 's sister .\n",
            "ks08\t1\t\tthe man holding the bottle disappeared .\n",
            "ks08\t1\t\tthe papers removed from the safe have not been found .\n",
            "ks08\t1\t\tthe money that you gave me disappeared last night .\n",
            "ks08\t0\t*\tjohn in the doorway waved to his father .\n",
            "ks08\t0\t*\the in the doorway waved to his father .\n",
            "ks08\t1\t\tand index values of the subject and the main verb .\n",
            "ks08\t1\t\tneither of these men is worthy to lead italy .\n",
            "ks08\t1\t\tnone of his customary excuses suffices edgar now .\n",
            "ks08\t1\t\tone of the problems was the robins .\n",
            "ks08\t1\t\tall of the plant virus web sites have been conveniently collected in one central location .\n",
            "ks08\t1\t\tsome of the water from melted snow also goes into the ground for plants .\n",
            "ks08\t1\t\tmost of the milk your baby consumes during breastfeeding is produced during nursing .\n",
            "ks08\t1\t\tall special rights of voting in the election were abolished .\n",
            "ks08\t1\t\tone of major factors affecting the value of diamonds was their weight .\n",
            "ks08\t1\t\teach of these stones has to be cut and polished .\n",
            "ks08\t1\t\tmost of her free time was spent attending concerts and plays or visiting museums and art galleries .\n",
            "ks08\t1\t\tthe committee was unanimous in their decision .\n",
            "ks08\t1\t\tthe committee have all now resigned .\n",
            "ks08\t0\t*\tthe committee has all now resigned .\n",
            "ks08\t1\t\tthe crew have both agreed to change sponsor .\n",
            "ks08\t0\t*\tthe crew has both agreed to change sponsor .\n",
            "ks08\t1\t\ther family is all avid skiers .\n",
            "ks08\t0\t*\ther family are all avid skiers .\n",
            "ks08\t0\t*\ta variety of styles have been in vogue for the last year .\n",
            "ks08\t1\t\tboth of the workers will wear carnations .\n",
            "ks08\t1\t\tboth the workers will wear carnations .\n",
            "ks08\t1\t\tboth will wear carnations .\n",
            "ks08\t1\t\tfew doctors approve of our remedy .\n",
            "ks08\t1\t\tfew approve of our remedy .\n",
            "ks08\t1\t\tan example of these substances be tobacco .\n",
            "ks08\t1\t\tthe effectiveness of teaching and learning depend on several factors .\n",
            "ks08\t1\t\tone of the most serious problems that some students have be lack of motivation .\n",
            "ks08\t1\t\tten years be a long time to spend in prison .\n",
            "ks08\t1\t\teveryone of us be given a prize .\n",
            "ks08\t1\t\tsome of the fruit be going bad .\n",
            "ks08\t1\t\tall of his wealth come from real estate investments .\n",
            "ks08\t1\t\tdo some of your relatives live nearby ?\n",
            "ks08\t1\t\tfifty pounds seem like a lot of weight to lose in one year .\n",
            "ks08\t1\t\tnews of persephone and demeter reach the great gods and goddesses of olympus .\n",
            "ks08\t1\t\thalf of the year be dark and wintry .\n",
            "ks08\t1\t\tsome of the promoters of ostrich meat compare its taste to beef tenderloin .\n",
            "ks08\t1\t\tthe committee has n't yet made up its mind .\n",
            "ks08\t0\t*\tthe committee has n't yet made up their mind .\n",
            "ks08\t1\t\tthe committee have n't yet made up their mind .\n",
            "ks08\t0\t*\tthe committee have n't yet made up its mind .\n",
            "ks08\t0\t*\tthat dog is so ferocious , it even tried to bite himself .\n",
            "ks08\t0\t*\ti washed me .\n",
            "ks08\t1\t\ti washed myself .\n",
            "ks08\t0\t*\tyou washed myself .\n",
            "ks08\t1\t\ti washed you .\n",
            "ks08\t1\t\the kicked you .\n",
            "ks08\t0\t*\ti washed yourself .\n",
            "ks08\t1\t\tyou washed yourself .\n",
            "ks08\t1\t\tharry says that sally dislikes him .\n",
            "ks08\t0\t*\tharry says that sally dislikes himself .\n",
            "ks08\t0\t*\tsally wishes that everyone would praise herself .\n",
            "ks08\t1\t\tsally believes that she is brilliant .\n",
            "ks08\t0\t*\tsally believes that herself is brilliant .\n",
            "ks08\t1\t\tthe power of your mind and the power of your body have a tight connection .\n",
            "ks08\t1\t\tjohn tries to fix the computer .\n",
            "ks08\t1\t\tjohn seems to fix the computer .\n",
            "ks08\t1\t\tmary persuaded john to fix the computer .\n",
            "ks08\t1\t\tmary expected john to fix the computer .\n",
            "ks08\t0\t*\tjohn to fix the computer .\n",
            "ks08\t0\t*\tseems john to fix the computer .\n",
            "ks08\t1\t\tjohn tries to be honest .\n",
            "ks08\t1\t\tjohn seems to be honest .\n",
            "ks08\t1\t\tjohn makes efforts for himself to be honest .\n",
            "ks08\t1\t\tit seems that john is honest .\n",
            "ks08\t1\t\tit tends to be warm in september .\n",
            "ks08\t1\t\tit seems to bother kim that they resigned .\n",
            "ks08\t0\t*\tit tries to be warm in september .\n",
            "ks08\t0\t*\tit hopes to bother kim that they resigned .\n",
            "ks08\t1\t\tit is easy to please kim .\n",
            "ks08\t1\t\tjohn is eager to please kim .\n",
            "ks08\t0\t*\tthere tries to be warm in september .\n",
            "ks08\t0\t*\tthere hopes to bother kim that they resigned .\n",
            "ks08\t0\t*\tit is eager to please kim .\n",
            "ks08\t1\t\tstephen seemed to be intelligent .\n",
            "ks08\t1\t\tit seems to be easy to fool ben .\n",
            "ks08\t1\t\tthere is likely to be a letter in the mailbox .\n",
            "ks08\t1\t\ttabs are likely to be kept on participants .\n",
            "ks08\t0\t*\tjohn seems to be easy to fool ben .\n",
            "ks08\t0\t*\tjohn is likely to be kept on participants .\n",
            "ks08\t1\t\tsandy tried to eat oysters .\n",
            "ks08\t0\t*\tthere tried to be riots in seoul .\n",
            "ks08\t0\t*\tit tried to bother me that chris lied .\n",
            "ks08\t0\t*\ttabs try to be kept on bob by the fbi .\n",
            "ks08\t0\t*\tthat he is clever is eager to be obvious .\n",
            "ks08\t1\t\tthe king thanked the man .\n",
            "ks08\t1\t\tthe color red seems to be his favorite color .\n",
            "ks08\t1\t\tthe cat seems to be out of the bag .\n",
            "ks08\t1\t\tthe dentist is likely to examine pat .\n",
            "ks08\t1\t\tpat is likely to be examined by the dentist .\n",
            "ks08\t1\t\tthe dentist is eager to examine pat .\n",
            "ks08\t1\t\tpat is eager to be examined by the dentist .\n",
            "ks08\t1\t\tstephen believed ben to be careful .\n",
            "ks08\t1\t\tstephen persuaded ben to be careful .\n",
            "ks08\t1\t\tstephen believed it to be easy to please kim .\n",
            "ks08\t0\t*\tstephen persuaded it to be easy to please kim .\n",
            "ks08\t1\t\tstephen believed there to be a fountain in the park .\n",
            "ks08\t0\t*\tstephen persuaded there to be a fountain in the park .\n",
            "ks08\t1\t\tstephen believed the cat to be out of the bag .\n",
            "ks08\t0\t*\tstephen persuaded the cat to be out of the bag .\n",
            "ks08\t1\t\tthe dentist was believed to have examined pat .\n",
            "ks08\t1\t\tpat was believed to have been examined by the dentist .\n",
            "ks08\t1\t\tthe dentist was persuaded to examine pat .\n",
            "ks08\t1\t\tstephen seems to be irritating .\n",
            "ks08\t1\t\ttom believes stephen to be irritating .\n",
            "ks08\t1\t\tjohn persuaded stephen to be more careful .\n",
            "ks08\t0\t*\tit seemed to be intelligent .\n",
            "ks08\t1\t\tit seemed to rain .\n",
            "ks08\t1\t\tthere seemed to be a fountain in the park .\n",
            "ks08\t1\t\tstephen tried to be intelligent .\n",
            "ks08\t0\t*\tit tried to be intelligent .\n",
            "ks08\t0\t*\tthere tried to be intelligent .\n",
            "ks08\t0\t*\tit tried to rain .\n",
            "ks08\t0\t*\tthere tried to be a fountain in the park .\n",
            "ks08\t1\t\tsomeone tried to leave the town .\n",
            "ks08\t1\t\tthere seems to be a fountain in the park .\n",
            "ks08\t0\t*\tit seems to be a fountain in the park .\n",
            "ks08\t0\t*\tjohn seems to be a fountain in the park .\n",
            "ks08\t1\t\twe believed there to be a fountain in the park .\n",
            "ks08\t0\t*\twe believed it to be a fountain in the park .\n",
            "ks08\t0\t*\tthere tries to leave the country .\n",
            "ks08\t0\t*\twe believed it to try to leave the country .\n",
            "ks08\t0\t*\twe believed there to try to leave the country .\n",
            "ks08\t1\t\twe believed john to try to leave the country .\n",
            "ks08\t1\t\tthe cat tries to be out of the bag .\n",
            "ks08\t1\t\tthey persuaded me to leave .\n",
            "ks08\t1\t\tthey promised me to leave .\n",
            "ks08\t0\t*\tthey persuaded it to rain .\n",
            "ks08\t0\t*\tthey promised it to rain .\n",
            "ks08\t1\t\tunder the bed is a fun place to hide .\n",
            "ks08\t0\t*\tunder the bed wants to be a fun place to hide .\n",
            "ks08\t1\t\tkim may have admitted to let mary mow the lawn .\n",
            "ks08\t1\t\tgregory appears to have wanted to be loyal to the company .\n",
            "ks08\t1\t\tjones would prefer for it to be clear to barry that the city plans to sue him .\n",
            "ks08\t1\t\tjohn continues to avoid the conflict .\n",
            "ks08\t1\t\tthe captain ordered the troops to proceed .\n",
            "ks08\t1\t\the coaxed his brother to give him the candy .\n",
            "ks08\t1\t\tjohn wants it to be clear to ben that the city plans to honor him .\n",
            "ks08\t0\t*\tjohn seems to rain .\n",
            "ks08\t0\t*\tjohn is likely to appear that he will win the game .\n",
            "ks08\t0\t*\tbeth tried for bill to ask a question .\n",
            "ks08\t0\t*\the believed there to be likely that he won the game .\n",
            "ks08\t0\t*\tit is likely to seem to be arrogant .\n",
            "ks08\t0\t*\tsandy appears that kim is happy .\n",
            "ks08\t0\t*\tdana would be unlikely for pat to be called upon .\n",
            "ks08\t0\t*\trobin is nothing in the box .\n",
            "ks08\t0\t*\tit said that kim was happy .\n",
            "ks08\t0\t*\tthere preferred for sandy to get the job .\n",
            "ks08\t1\t\tthere is only one chemical substance involved in nerve transmission .\n",
            "ks08\t0\t*\tthere are only one chemical substance involved in nerve transmission .\n",
            "ks08\t0\t*\tthere is more chemical substances involved in nerve transmission .\n",
            "ks08\t1\t\tthere are more chemical substances involved in nerve transmission .\n",
            "ks08\t1\t\tthere is believed to be a sheep in the park .\n",
            "ks08\t0\t*\tthere is believed to be a sheep in the park .\n",
            "ks08\t1\t\tthere are believed to be sheep in the park .\n",
            "ks08\t1\t\tthere seems to be no student absent .\n",
            "ks08\t0\t*\tthere are likely to be no student absent .\n",
            "ks08\t1\t\tthere is likely to be no student absent .\n",
            "ks08\t1\t\tpat expected leslie to be aggressive .\n",
            "ks08\t1\t\tpat persuaded leslie to be aggressive .\n",
            "ks08\t1\t\tpat promised leslie to be aggressive .\n",
            "ks08\t1\t\tkevin urged anne to be loyal to her .\n",
            "ks08\t1\t\twe expect the dentist to examine us .\n",
            "ks08\t0\t*\twe expect the dentist to examine ourselves .\n",
            "ks08\t1\t\twe expect them to examine themselves .\n",
            "ks08\t1\t\twe persuaded the dentist to examine us .\n",
            "ks08\t0\t*\twe persuaded the dentist to examine ourselves .\n",
            "ks08\t1\t\twe persuaded them to examine themselves .\n",
            "ks08\t0\t*\twe persuaded them to examine them .\n",
            "ks08\t1\t\tjohn may drink water , and bill drink beer .\n",
            "ks08\t1\t\ttom will not leave .\n",
            "ks08\t0\t*\ttom kicked not a ball .\n",
            "ks08\t1\t\twill tom leave the party now ?\n",
            "ks08\t0\t*\tleft tom the party already ?\n",
            "ks08\t1\t\tjohn could n't leave the party .\n",
            "ks08\t0\t*\tjohn left n't the party early .\n",
            "ks08\t1\t\tif anybody is spoiling the children , john is .\n",
            "ks08\t0\t*\tif anybody keeps spoiling the children , john keeps .\n",
            "ks08\t1\t\tyou should leave , should n't you ?\n",
            "ks08\t0\t*\tyou did n't leave , left you ?\n",
            "ks08\t1\t\tshe would never believe that story .\n",
            "ks08\t0\t*\tshe believed never his story .\n",
            "ks08\t1\t\tthe boys will all be there .\n",
            "ks08\t0\t*\tour team played all well .\n",
            "ks08\t1\t\tthe children will have been being entertained .\n",
            "ks08\t0\t*\tthe house is been remodelling .\n",
            "ks08\t0\t*\tmargaret has had already left .\n",
            "ks08\t0\t*\the has will seeing his children .\n",
            "ks08\t0\t*\the has been must being interrogated by the police at that very moment .\n",
            "ks08\t1\t\tmary solved the problem .\n",
            "ks08\t1\t\tmary would solve the problem .\n",
            "ks08\t1\t\tmary was solving the problem .\n",
            "ks08\t1\t\tmary would easily solve the problem .\n",
            "ks08\t0\t*\tmary not avoided bill .\n",
            "ks08\t1\t\tmary did not avoid bill .\n",
            "ks08\t1\t\tfred must have been singing songs and probably was drinking beer .\n",
            "ks08\t1\t\tfred must both have been singing songs and have been drinking beer .\n",
            "ks08\t1\t\tfred must have both been singing songs and been drinking beer .\n",
            "ks08\t1\t\tfred must have been both singing songs and drinking beer .\n",
            "ks08\t1\t\tthere might be a unicorn in the garden .\n",
            "ks08\t1\t\tit will rain tomorrow .\n",
            "ks08\t1\t\tjohn will leave the party earlier .\n",
            "ks08\t0\t*\tthere hopes to finish the project .\n",
            "ks08\t0\t*\tthe bus hopes to be here at five .\n",
            "ks08\t0\t*\ti hope to can study in france .\n",
            "ks08\t1\t\ti hope to study in france .\n",
            "ks08\t0\t*\tjohn stopped can to sign in tune .\n",
            "ks08\t0\t*\tjohn stopped canning to sign in tune .\n",
            "ks08\t0\t*\tjohn wills leave the party early .\n",
            "ks08\t0\t*\tjohn can kicked the ball .\n",
            "ks08\t0\t*\tjohn can kicking the ball .\n",
            "ks08\t0\t*\tjohn can to kick the ball .\n",
            "ks08\t1\t\tjohn will kick the ball .\n",
            "ks08\t0\t*\tjohn will kicked the ball .\n",
            "ks08\t0\t*\tjohn will to kick the ball .\n",
            "ks08\t0\t*\tkim must bakes a cake .\n",
            "ks08\t0\t*\tkim must baked a cake .\n",
            "ks08\t0\t*\tkim must will bake a cake .\n",
            "ks08\t1\t\tthere may exist a man in the park .\n",
            "ks08\t0\t*\tit may exist a man in the park .\n",
            "ks08\t0\t*\tit is vital that we will study everyday .\n",
            "ks08\t1\t\the is a fool .\n",
            "ks08\t1\t\the has a car .\n",
            "ks08\t1\t\tjohn is running to the car .\n",
            "ks08\t1\t\twas the child in the school ?\n",
            "ks08\t1\t\twas the child running to the car ?\n",
            "ks08\t1\t\twas the child found ?\n",
            "ks08\t1\t\tthe child never became crazy .\n",
            "ks08\t1\t\tthe child was never crazy .\n",
            "ks08\t1\t\tthe child was never running to the car .\n",
            "ks08\t1\t\tthe child was never deceived .\n",
            "ks08\t1\t\tjohn is happy about the outcome .\n",
            "ks08\t1\t\tjohn was seeing his children .\n",
            "ks08\t1\t\tthe children are seen in the yard .\n",
            "ks08\t1\t\tjohn has not sung a song .\n",
            "ks08\t1\t\thas john sung a song ?\n",
            "ks08\t1\t\tjohn has n't been singing a song .\n",
            "ks08\t1\t\tjohn has sung a song and mary has too .\n",
            "ks08\t1\t\tjohn can have danced .\n",
            "ks08\t1\t\tjohn can be dancing .\n",
            "ks08\t1\t\the has seen his children .\n",
            "ks08\t1\t\the will have been seeing his children .\n",
            "ks08\t0\t*\tamericans have paying income tax ever since 1913 .\n",
            "ks08\t0\t*\tgeorge has went to america .\n",
            "ks08\t1\t\tis out since the following is finite .\n",
            "ks08\t1\t\tyou are a student .\n",
            "ks08\t1\t\tyou have not enough money .\n",
            "ks08\t1\t\thave you enough money ?\n",
            "ks08\t1\t\tjohn does not like this town .\n",
            "ks08\t1\t\tin no other circumstances does that distinction matter .\n",
            "ks08\t1\t\tthey did n't leave any food .\n",
            "ks08\t0\t*\tthey expected us to do leave him .\n",
            "ks08\t0\t*\tthey expected us to should leave him .\n",
            "ks08\t0\t*\ti found myself doing need sleep .\n",
            "ks08\t0\t*\the does be leaving .\n",
            "ks08\t0\t*\the does have been eating .\n",
            "ks08\t0\t*\tthey will do come .\n",
            "ks08\t1\t\tjohn did leave .\n",
            "ks08\t1\t\tdid john find the solution ?\n",
            "ks08\t1\t\thow long did it last ?\n",
            "ks08\t1\t\tjohn may leave .\n",
            "ks08\t1\t\tit may rain .\n",
            "ks08\t0\t*\tjohn may rain .\n",
            "ks08\t1\t\tjohn did not leave .\n",
            "ks08\t0\t*\tjohn did not rain .\n",
            "ks08\t1\t\the might have left .\n",
            "ks08\t0\t*\the might do leave .\n",
            "ks08\t0\t*\the does can leave here .\n",
            "ks08\t0\t*\the does may leave here .\n",
            "ks08\t0\t*\tjim does have supported the theory .\n",
            "ks08\t0\t*\tthe proposal did be endorsed by clinton .\n",
            "ks08\t0\t*\ti do not have sung .\n",
            "ks08\t0\t*\ti do not be happy .\n",
            "ks08\t1\t\tdo be honest !\n",
            "ks08\t1\t\tdo n't be silly !\n",
            "ks08\t0\t*\tjohn believed kim to do not leave here .\n",
            "ks08\t1\t\tjohn believes kim not to leave here .\n",
            "ks08\t0\t*\tjohn believed kim to leaving here .\n",
            "ks08\t0\t*\tjohn did not leaving here .\n",
            "ks08\t0\t*\tjohn expect to must leave .\n",
            "ks08\t0\t*\tjohn did not may leave .\n",
            "ks08\t1\t\ttom wanted to go home , but peter did n't want to .\n",
            "ks08\t1\t\tlee voted for bill because his father told him to .\n",
            "ks08\t1\t\tkim regrets not having seen the movie .\n",
            "ks08\t1\t\tkim regrets never having seen the movie .\n",
            "ks08\t1\t\twe asked him not to try to call us again .\n",
            "ks08\t1\t\twe asked him never to try to call us again .\n",
            "ks08\t1\t\tduty made them not miss the weekly meetings .\n",
            "ks08\t1\t\tduty made them never miss the weekly meetings .\n",
            "ks08\t1\t\tnot speaking english is a disadvantage .\n",
            "ks08\t0\t*\tspeaking not english is a disadvantage .\n",
            "ks08\t0\t*\tlee likes not kim .\n",
            "ks08\t1\t\tlee is believed not to like kim .\n",
            "ks08\t1\t\tlee is believed to not like kim .\n",
            "ks08\t0\t*\tlee is believed to like not kim .\n",
            "ks08\t1\t\tthe president could not approve the bill .\n",
            "ks08\t1\t\tit would be possible for the president not to approve the bill .\n",
            "ks08\t1\t\tit would not be possible for the president to approve the bill .\n",
            "ks08\t0\t*\tlee not left .\n",
            "ks08\t1\t\tlee will never leave .\n",
            "ks08\t1\t\tlee will not leave .\n",
            "ks08\t1\t\tjohn could not leave the town .\n",
            "ks08\t0\t*\tjohn not left the town .\n",
            "ks08\t0\t*\tjohn not could leave the town .\n",
            "ks08\t1\t\tmary sang a song , but lee never did .\n",
            "ks08\t0\t*\tmary sang a song , but lee did never .\n",
            "ks08\t1\t\tmary sang a song , but lee did not .\n",
            "ks08\t1\t\tthe president could not approve the bill , could n't he ?\n",
            "ks08\t0\t*\tthe president could not approve the bill , could he ?\n",
            "ks08\t1\t\tare you studying english syntax ?\n",
            "ks08\t1\t\twhat are you studying nowadays ?\n",
            "ks08\t1\t\ti shall go downtown .\n",
            "ks08\t1\t\tshall i go downtown ?\n",
            "ks08\t1\t\tmay she live forever !\n",
            "ks08\t1\t\twas i that stupid ?\n",
            "ks08\t1\t\tdo n't you even touch that !\n",
            "ks08\t1\t\tyou better not drink .\n",
            "ks08\t1\t\tyou can do it , but you better not .\n",
            "ks08\t0\t*\tbetter you not drink .\n",
            "ks08\t1\t\tthey 'd leave soon .\n",
            "ks08\t1\t\tthey would n't leave soon .\n",
            "ks08\t1\t\tthey should n't leave soon .\n",
            "ks08\t1\t\tthey can do it , ca n't they ?\n",
            "ks08\t1\t\tthey ca n't do it , can they ?\n",
            "ks08\t0\t*\tthey ca n't do it , ca n't they ?\n",
            "ks08\t0\t*\tthey ca n't do it , can he ?\n",
            "ks08\t1\t\tkim can dance , and sandy can , too .\n",
            "ks08\t1\t\tkim has danced , and sandy has , too .\n",
            "ks08\t1\t\tkim was dancing , and sandy was , too .\n",
            "ks08\t0\t*\tkim considered joining the navy , but i never considered .\n",
            "ks08\t0\t*\tkim wanted to go and sandy wanted , too .\n",
            "ks08\t1\t\tkim is happy and sandy is too .\n",
            "ks08\t1\t\twhen kim was in china , i was too .\n",
            "ks08\t1\t\thave you anything to share with the group ?\n",
            "ks08\t1\t\thave you brought anything to share with the group ?\n",
            "ks08\t1\t\tsandy must have been , too .\n",
            "ks08\t1\t\tsandy must have , too .\n",
            "ks08\t1\t\tsandy must , too .\n",
            "ks08\t1\t\tbecause john persuaded sally to , he did n't have to talk to the reporters .\n",
            "ks08\t0\t*\tmary sang a song , but lee could never .\n",
            "ks08\t1\t\tmary sang a song , but lee could not .\n",
            "ks08\t1\t\tjohn got sent to prison .\n",
            "ks08\t1\t\the ought to leave his luggage here .\n",
            "ks08\t1\t\the dared not argue against his parents .\n",
            "ks08\t1\t\the used to go there very often .\n",
            "ks08\t1\t\tthe gardener must trim the rose bushes today .\n",
            "ks08\t1\t\tthis should be the beginning of a beautiful friendship .\n",
            "ks08\t1\t\ti am removing the shovel from the shed .\n",
            "ks08\t1\t\tthe travelers have returned from their vacation .\n",
            "ks08\t1\t\tspringfield would have built a police station with the federal grant .\n",
            "ks08\t1\t\tsharks could have been cruising near the beach .\n",
            "ks08\t1\t\tshe seem to have given financial assistance to an important french art dealer .\n",
            "ks08\t0\t*\tann may spending her vacation in italy .\n",
            "ks08\t0\t*\tann may spends her vacation in italy .\n",
            "ks08\t0\t*\tann may spent her vacation in italy .\n",
            "ks08\t1\t\tit has rained every day for the last week .\n",
            "ks08\t0\t*\tit has raining every day for the last week .\n",
            "ks08\t0\t*\tit has rains every day for the last week .\n",
            "ks08\t0\t*\tit has rain every day for the last week .\n",
            "ks08\t1\t\ttagalog is spoken in the philippines .\n",
            "ks08\t0\t*\ttagalog is speak in the philippines .\n",
            "ks08\t0\t*\ttagalog is speaks in the philippines .\n",
            "ks08\t0\t*\ttagalog is spoke in the philippines .\n",
            "ks08\t1\t\tthe roof is leaking .\n",
            "ks08\t0\t*\tthe roof is leaked .\n",
            "ks08\t0\t*\tthe roof is leaks .\n",
            "ks08\t0\t*\tgeorge is having lived in toledo for thirty years .\n",
            "ks08\t0\t*\tthe house is been remodeling .\n",
            "ks08\t0\t*\ta medal was been given to the mayor by the sewer commissioner .\n",
            "ks08\t0\t*\tdoes john have gone to the library ?\n",
            "ks08\t0\t*\tjohn seems fond of ice cream , and bill seems , too .\n",
            "ks08\t1\t\tsam may have been being interrogated by the fbi .\n",
            "ks08\t0\t*\tsam may have been being interrogating by the fbi .\n",
            "ks08\t0\t*\tsam may be had been interrogating by the fbi .\n",
            "ks08\t1\t\thave social problems made police work difficult ?\n",
            "ks08\t1\t\tthe senator should not have forgotten the concerns of her constituents .\n",
            "ks08\t1\t\ttokyo has not loosened trade restrictions .\n",
            "ks08\t1\t\tdid the doctor prescribe aspirin ?\n",
            "ks08\t1\t\tsandy will read your reports , but harold will not .\n",
            "ks08\t1\t\the can hardly believe that it 's already over .\n",
            "ks08\t1\t\ti could have little known that more trouble was just around the corner .\n",
            "ks08\t1\t\ti have never been spoken to so rudely !\n",
            "ks08\t1\t\thardly was there any rain falling .\n",
            "ks08\t1\t\tlittle did i know that more trouble was just around the corner .\n",
            "ks08\t1\t\tnever have i been spoken to so rudely !\n",
            "ks08\t1\t\the had hardly collected the papers on his desk , had he ?\n",
            "ks08\t0\t*\the had hardly collected the papers on his desk , had n't he ?\n",
            "ks08\t1\t\the never achieved anything , did he ?\n",
            "ks08\t0\t*\the never achieved anything , did n't he ?\n",
            "ks08\t1\t\tas a statesman , he scarcely could do anything worth mentioning .\n",
            "ks08\t0\t*\tas a statesman , scarcely he could do anything worth mentioning .\n",
            "ks08\t0\t*\tany zebras ca n't fly .\n",
            "ks08\t0\t*\tanything has n't happened to his optimism .\n",
            "ks08\t0\t*\tany of the citizens hardly ever say anything .\n",
            "ks08\t1\t\ti did n't find any bugs in my bed .\n",
            "ks08\t1\t\tnobody told them anything .\n",
            "ks08\t1\t\tnever have i stolen from any members of your family .\n",
            "ks08\t1\t\twhy have n't any books been returned ?\n",
            "ks08\t1\t\thardly any of the citizens ever say anything .\n",
            "ks08\t1\t\tthese lines were written by one of korea 's most famous poets .\n",
            "ks08\t1\t\tthe unidentified victim was apparently struck during the early morning hours .\n",
            "ks08\t1\t\ttargets can be observed at any angle .\n",
            "ks08\t1\t\tduring the early evening , saturn can be found in the north , while jupiter rises in the east .\n",
            "ks08\t1\t\ti poured 20 liters of acid into the beaker .\n",
            "ks08\t1\t\tabout 20 liters of acid was poured into the beaker .\n",
            "ks08\t1\t\tthe executive committee approved the new policy .\n",
            "ks08\t1\t\tthe new policy was approved by the executive committee .\n",
            "ks08\t1\t\tjohn has taken bill to the library .\n",
            "ks08\t1\t\tjohn has chosen bill for the position .\n",
            "ks08\t0\t*\tjohn has taken to the library .\n",
            "ks08\t0\t*\tjohn has chosen for the position .\n",
            "ks08\t0\t*\tthe guide has been taken john to the library .\n",
            "ks08\t0\t*\tthe department has been chosen john for the position .\n",
            "ks08\t1\t\tjohn has been taken to the library .\n",
            "ks08\t1\t\tjohn has been chosen for the position .\n",
            "ks08\t1\t\tpat handed a book to chris .\n",
            "ks08\t0\t*\tpat handed to chris .\n",
            "ks08\t0\t*\tpat handed a book .\n",
            "ks08\t1\t\ta book was handed to chris by pat .\n",
            "ks08\t0\t*\ta book was handed by pat .\n",
            "ks08\t1\t\ta book was handed to chris .\n",
            "ks08\t0\t*\ta book was handed .\n",
            "ks08\t1\t\tthey believe it to be easy to annoy ben .\n",
            "ks08\t0\t*\tthey believe stephen to be easy to annoy ben .\n",
            "ks08\t1\t\tthey believe there to be a dragon in the wood .\n",
            "ks08\t1\t\tit is believed to be easy to annoy ben .\n",
            "ks08\t0\t*\tstephen is believed to be easy to annoy ben .\n",
            "ks08\t1\t\tthere is believed to be a dragon in the wood .\n",
            "ks08\t1\t\tno one believes that he is a fool .\n",
            "ks08\t1\t\tno one suspects that he is a fool .\n",
            "ks08\t1\t\tthat he is a fool is suspected by no one .\n",
            "ks08\t1\t\tthey believe the cat to be out of the bag .\n",
            "ks08\t1\t\tthe cat is believed to be out of the bag .\n",
            "ks08\t1\t\tjohn drove the car .\n",
            "ks08\t1\t\tjohn was driving the car .\n",
            "ks08\t1\t\tthe car was being driven .\n",
            "ks08\t1\t\tjohn will drive the car .\n",
            "ks08\t1\t\tthe car will be driven .\n",
            "ks08\t1\t\tjohn has driven the car .\n",
            "ks08\t1\t\tthe car has been driven .\n",
            "ks08\t1\t\tjohn has been driving the car .\n",
            "ks08\t1\t\tthe car has been being driven .\n",
            "ks08\t1\t\tthe car will have been being driven .\n",
            "ks08\t1\t\tpat handed chris a note .\n",
            "ks08\t1\t\tchris was handed a note .\n",
            "ks08\t1\t\tchris was handed a note by pat .\n",
            "ks08\t1\t\tideas are put into children 's heads by tv .\n",
            "ks08\t1\t\tyesterday , the child really kicked a monkey in the street .\n",
            "ks08\t1\t\tthe model resembles kim in nearly every detail .\n",
            "ks08\t0\t*\tkim is resembled by the model in nearly every detail .\n",
            "ks08\t0\t*\tyou are not fitted by the coat .\n",
            "ks08\t1\t\ti was born in 1970 .\n",
            "ks08\t1\t\tit is rumored that he is on his way out .\n",
            "ks08\t1\t\tjohn is said to be rich .\n",
            "ks08\t1\t\the is reputed to be a good scholar .\n",
            "ks08\t0\t*\tmy mother bore me in 1970 .\n",
            "ks08\t0\t*\teveryone rumored that he was on his way out .\n",
            "ks08\t0\t*\tthey said him to be rich .\n",
            "ks08\t0\t*\tthey reputed him to be a good scholar .\n",
            "ks08\t1\t\the kicked the ball .\n",
            "ks08\t1\t\tthe ball was kicked by him .\n",
            "ks08\t1\t\tjohn kicked him .\n",
            "ks08\t1\t\the was kicked by john .\n",
            "ks08\t1\t\tjohn sent her to seoul .\n",
            "ks08\t1\t\tshe was sent to seoul .\n",
            "ks08\t1\t\tthey widely believed that john was ill .\n",
            "ks08\t1\t\tthat john was ill was widely believed .\n",
            "ks08\t1\t\tthey have n't decided which attorney will give the closing argument .\n",
            "ks08\t1\t\twhich attorney will give the closing argument has n't been decided .\n",
            "ks08\t1\t\twhich attorney will give the closing argument has n't been decided by them .\n",
            "ks08\t1\t\tyou can rely on ben .\n",
            "ks08\t1\t\tben can be relied on .\n",
            "ks08\t1\t\tthey talked about the scandal for days .\n",
            "ks08\t1\t\tthe scandal was talked about for days .\n",
            "ks08\t1\t\tthe issue was dealt with promptly .\n",
            "ks08\t1\t\tthat 's not what 's asked for .\n",
            "ks08\t1\t\tthis should be attended to immediately .\n",
            "ks08\t0\t*\tthe capital was gathered near by a crowd of people .\n",
            "ks08\t0\t*\tthe hot sun was played under by the children .\n",
            "ks08\t1\t\tthat 's something i would have paid twice for .\n",
            "ks08\t1\t\tthese are the books that we have gone most thoroughly over .\n",
            "ks08\t1\t\tthey look generally on john as selfish .\n",
            "ks08\t0\t*\teverything was paid twice for .\n",
            "ks08\t0\t*\tyour books were gone most thoroughly over .\n",
            "ks08\t0\t*\the is looked generally on as selfish .\n",
            "ks08\t1\t\tpavarotti relied on loren and bond on hepburn .\n",
            "ks08\t0\t*\tpavarotti relied on loren and bond hepburn .\n",
            "ks08\t1\t\tloren was relied on by pavarotti and hepburn by bond .\n",
            "ks08\t0\t*\tloren was relied on by pavarotti and hepburn on by bond .\n",
            "ks08\t1\t\tthe lawyer looked into the document .\n",
            "ks08\t1\t\tthe document was looked into by the lawyer .\n",
            "ks08\t1\t\tpeter has been asked to resign .\n",
            "ks08\t1\t\ti assume the matter to have been filed in the appropriate records .\n",
            "ks08\t1\t\tsmith wants the picture to be removed from the office .\n",
            "ks08\t1\t\tthe events have been described well .\n",
            "ks08\t1\t\tover 120 different contaminants have been dumped into the river .\n",
            "ks08\t1\t\tthe balloon is positioned in an area of blockage and is inflated .\n",
            "ks08\t1\t\tcancer is now thought to be unlikely to be caused by hot dogs .\n",
            "ks08\t1\t\twhether this is feasible has n't yet been determined .\n",
            "ks08\t1\t\tpaying taxes ca n't be avoided .\n",
            "ks08\t1\t\tit has n't yet been determined whether this is feasible .\n",
            "ks08\t1\t\tfrances has had the drapes cleaned .\n",
            "ks08\t1\t\tshirley seems to have fred promoted .\n",
            "ks08\t1\t\tnina got bill elected to the committee .\n",
            "ks08\t1\t\twe got our car radio stolen twice on holiday .\n",
            "ks08\t1\t\tfrances has had her clean the drapes .\n",
            "ks08\t1\t\tnina got them to elect bill .\n",
            "ks08\t1\t\tthe news was dealt with carefully .\n",
            "ks08\t1\t\tthe tree was looked after by kim .\n",
            "ks08\t1\t\twe can not put up with the noise anymore .\n",
            "ks08\t1\t\the will keep up with their expectations .\n",
            "ks08\t1\t\tthis noise can not be put up with .\n",
            "ks08\t1\t\ttheir expectations will be kept up with .\n",
            "ks08\t1\t\tthey paid a lot of attention to the matter .\n",
            "ks08\t1\t\tthe son took care of his parents .\n",
            "ks08\t1\t\tthe matter was paid a lot of attention to .\n",
            "ks08\t1\t\ta lot of attention was paid to the matter .\n",
            "ks08\t0\t*\tnew york was slept in .\n",
            "ks08\t0\t*\tthe lake was camped beside by my sister .\n",
            "ks08\t1\t\tthe lake is not to be camped beside by anybody .\n",
            "ks08\t0\t*\tsix inches were grown by the boy .\n",
            "ks08\t0\t*\ta mile to work was run by him .\n",
            "ks08\t1\t\tthe beans were grown by the gardener .\n",
            "ks08\t1\t\tthe plums were weighed by the grocer .\n",
            "ks08\t0\t*\tsan francisco has been lived in by my brother .\n",
            "ks08\t1\t\tthe house has been lived in by several famous personages .\n",
            "ks08\t0\t*\tseoul was slept in by the businessman last night .\n",
            "ks08\t1\t\tthis bed was surely slept in by a huge guy last night .\n",
            "ks08\t1\t\trosie got struck by lightning .\n",
            "ks08\t1\t\ti got phoned by a woman friend .\n",
            "ks08\t1\t\the got hit in the face with the tip of a surfboard .\n",
            "ks08\t1\t\tjohn 's bike got fixed or got stolen .\n",
            "ks08\t0\t*\tthe lesson got read by a priest .\n",
            "ks08\t0\t*\tthe letter got written by a poet .\n",
            "ks08\t0\t*\ttom got understood to have asked for a refund .\n",
            "ks08\t0\t*\tmary got heard to insult her parents .\n",
            "ks08\t1\t\tis john clever ?\n",
            "ks08\t1\t\twho is clever ?\n",
            "ks08\t1\t\thow clever you are !\n",
            "ks08\t1\t\tbe very clever .\n",
            "ks08\t1\t\ti ask you if this is what you want .\n",
            "ks08\t1\t\twould you mind taking out the garbage ?\n",
            "ks08\t1\t\tcan the child read the book ?\n",
            "ks08\t1\t\twhat can the child read ?\n",
            "ks08\t1\t\twhich version did they recommend ?\n",
            "ks08\t1\t\twith what did the baby eat the food ?\n",
            "ks08\t1\t\thow did he eat the food ?\n",
            "ks08\t1\t\twhich man did you talk to ?\n",
            "ks08\t1\t\tto which man did you talk ?\n",
            "ks08\t1\t\thow ill has hobbs been ?\n",
            "ks08\t0\t*\twhich man did you talk ?\n",
            "ks08\t0\t*\tto which man did you talk to ?\n",
            "ks08\t1\t\twho do you think hobbs imagined mary said tom saw ?\n",
            "ks08\t1\t\twho did kim work for and sandy rely on ?\n",
            "ks08\t0\t*\twho did kim work for and sandy rely ?\n",
            "ks08\t0\t*\twho did kim work for and sandy rely on mary ?\n",
            "ks08\t1\t\tyou can rely on edward 's help .\n",
            "ks08\t1\t\tedward 's help , you can rely on .\n",
            "ks08\t1\t\twe talked about the fact that he was sick for days .\n",
            "ks08\t1\t\tthe fact that he was sick for days , we talked about .\n",
            "ks08\t0\t*\tyou can rely on that he will help you .\n",
            "ks08\t0\t*\twe talked about that he was sick for days .\n",
            "ks08\t1\t\tthat he was sick , we talked about for days .\n",
            "ks08\t1\t\tthat arrows do n't stop in midair is captured by this theory .\n",
            "ks08\t0\t*\twho did you see and a picture of ?\n",
            "ks08\t1\t\tthese qualities recommended him to oliver .\n",
            "ks08\t1\t\tthe un recommended an enlarged peacekeeping force .\n",
            "ks08\t1\t\tthis is the book which the teacher recommended .\n",
            "ks08\t1\t\twho will they recommend ?\n",
            "ks08\t1\t\tjohn put the books in a box .\n",
            "ks08\t1\t\twhich books did john put in the box ?\n",
            "ks08\t1\t\twhere did john put the books ?\n",
            "ks08\t1\t\tin which box did john put the book ?\n",
            "ks08\t1\t\thow happy has john been ?\n",
            "ks08\t1\t\twho put the book in the box ?\n",
            "ks08\t1\t\twho did put the book in the box ?\n",
            "ks08\t1\t\twho can put the book in the box ?\n",
            "ks08\t1\t\twho do you think visited seoul last year ?\n",
            "ks08\t1\t\tthat 's the un delegate that the government thinks visits seoul last year .\n",
            "ks08\t1\t\twho do you believe that sara invited ?\n",
            "ks08\t1\t\twho do you believe invited sara ?\n",
            "ks08\t0\t*\twho do you believe that invited sara ?\n",
            "ks08\t0\t*\twho do you think that would be nominated for the position ?\n",
            "ks08\t1\t\tthis is the kind of person who i doubt that under normal circumstances would have anything to do with such a scheme .\n",
            "ks08\t1\t\tjohn asks whose book his son likes .\n",
            "ks08\t1\t\tjohn has forgotten which player his son shouted at .\n",
            "ks08\t1\t\the told me how many employees karen introduced to the visitors .\n",
            "ks08\t1\t\the had been reading the article .\n",
            "ks08\t0\t*\ttom denied which book he had been reading .\n",
            "ks08\t0\t*\ttom claimed how much money she had spent .\n",
            "ks08\t0\t*\tjohn inquired that he should read it .\n",
            "ks08\t0\t*\tpeter will decide that we should review the book .\n",
            "ks08\t1\t\tjohn inquired which book he should read .\n",
            "ks08\t1\t\tpeter will decide which book we should review .\n",
            "ks08\t1\t\tjohn told us that we should review the book .\n",
            "ks08\t1\t\tjohn told us which book we should review .\n",
            "ks08\t1\t\tin which box did he put the book ?\n",
            "ks08\t1\t\twhich book by his father did he read ?\n",
            "ks08\t1\t\tjohn asks in which box he put the book .\n",
            "ks08\t1\t\tjohn asks which book by his father he read .\n",
            "ks08\t1\t\tkim has wondered in which room gary stayed .\n",
            "ks08\t1\t\tlee asked me how fond of chocolates the monkeys are .\n",
            "ks08\t0\t*\tkim has wondered that gary stayed in the room .\n",
            "ks08\t0\t*\tkim asked me that the monkeys are very fond of chocolates .\n",
            "ks08\t1\t\tjohn knows whose book mary bought and tom borrowed from her .\n",
            "ks08\t0\t*\tjohn knows whose book mary bought and tom talked .\n",
            "ks08\t1\t\ti do n't know whether i should agree .\n",
            "ks08\t1\t\tshe gets upset if i exclude her from anything .\n",
            "ks08\t1\t\tshe gets upset whether i exclude her from anything .\n",
            "ks08\t1\t\ti wonder if you 'd be kind enough to give us information .\n",
            "ks08\t1\t\ti am not certain about when he will come .\n",
            "ks08\t1\t\ti am not certain about whether he will go or not .\n",
            "ks08\t0\t*\ti am not certain about if he will come .\n",
            "ks08\t0\t*\ti am not certain about if he will go or not .\n",
            "ks08\t1\t\ti do n't know where to go .\n",
            "ks08\t1\t\ti do n't know what to do .\n",
            "ks08\t1\t\ti do n't know how to do it .\n",
            "ks08\t1\t\ti do n't know whether to agree with him or not .\n",
            "ks08\t0\t*\ti do n't know if to agree with him .\n",
            "ks08\t0\t*\ti do n't know that to agree with him or not .\n",
            "ks08\t1\t\tfred knows which politician to support .\n",
            "ks08\t1\t\tkaren asked where to put the chairs .\n",
            "ks08\t1\t\tthe student protected him .\n",
            "ks08\t1\t\twho protected him ?\n",
            "ks08\t1\t\tto protect him is not an easy task .\n",
            "ks08\t0\t*\tfred knows which politician for karen to vote for .\n",
            "ks08\t0\t*\tfred knows which politician for her to vote for .\n",
            "ks08\t0\t*\tkaren asked where for jerry to put the chairs .\n",
            "ks08\t0\t*\tkaren asked where for him to put the chairs .\n",
            "ks08\t1\t\thow carefully have you considered your future career ?\n",
            "ks08\t1\t\twhen can we register for graduation ?\n",
            "ks08\t1\t\twhere do we go to register for graduation ?\n",
            "ks08\t1\t\twhy have you borrowed my pencil ?\n",
            "ks08\t1\t\twhen did he say that he was fired ?\n",
            "ks08\t1\t\twhere did he tell you that he met mary ?\n",
            "ks08\t1\t\twhy do you wonder whether she will invite me ?\n",
            "ks08\t1\t\thow often did he ask when she will meet at the party ?\n",
            "ks08\t1\t\twhat causes students to select particular majors ?\n",
            "ks08\t1\t\twho will john ask for information about summer courses ?\n",
            "ks08\t1\t\twhich textbook did the teacher use in the class last summer ?\n",
            "ks08\t1\t\twhose car is blocking the entrance to the store ?\n",
            "ks08\t1\t\twhy do you think he left ?\n",
            "ks08\t1\t\twho do you guess will be here ?\n",
            "ks08\t1\t\twho do you think borrowed my book ?\n",
            "ks08\t1\t\twhich city does fred think that you believe that john lives in ?\n",
            "ks08\t1\t\ti wonder on which shelf john will put the book ?\n",
            "ks08\t1\t\twhat proof that he has implicated have you found ?\n",
            "ks08\t1\t\tjoseph has forgotten how many matches he has won .\n",
            "ks08\t1\t\tfred will warn martha that she should claim that her brother is patriotic .\n",
            "ks08\t1\t\tthat bill tried to discover which drawer alice put the money in made us realize that we should have left him in seoul .\n",
            "ks08\t1\t\tjasper wonders which book he should attempt to persuade his students to buy .\n",
            "ks08\t0\t*\ti wonder if on which shelve john will put the book .\n",
            "ks08\t0\t*\ti wonder what city that romans destroyed .\n",
            "ks08\t0\t*\tjohn was wondering to whom he was referring to .\n",
            "ks08\t0\t*\twho do you think that has given the tickets to bill ?\n",
            "ks08\t0\t*\twhat city will fred say that mary thinks that john lives ?\n",
            "ks08\t0\t*\ton whom does dana believe chris knows sandy trusts ?\n",
            "ks08\t0\t*\tthe politician denied how the opponent was poisoned .\n",
            "ks08\t0\t*\tfred knows which book for the children to read during the summer vacation .\n",
            "ks08\t1\t\tthis needs mending .\n",
            "ks08\t0\t*\tthis needs mending the shoe .\n",
            "ks08\t0\t*\the mended .\n",
            "ks08\t1\t\the mended the shoe .\n",
            "ks08\t1\t\tthis needs investigating .\n",
            "ks08\t0\t*\tthis needs investigating the problem .\n",
            "ks08\t0\t*\tthey investigated .\n",
            "ks08\t1\t\tthey investigated the problem .\n",
            "ks08\t1\t\tthe video which you recommended was really terrific .\n",
            "ks08\t1\t\tthe video which i thought you recommended was really terrific .\n",
            "ks08\t1\t\tthe video which i thought john told us you recommended was really terrific .\n",
            "ks08\t1\t\tthe student who won the prize left .\n",
            "ks08\t1\t\tthe student who everyone likes left .\n",
            "ks08\t1\t\tthe person whom john gave the book to left .\n",
            "ks08\t1\t\tthe day when i met her was sunny .\n",
            "ks08\t1\t\tthe president who fred voted for has resigned .\n",
            "ks08\t1\t\tthe president that fred voted for dislikes his opponents .\n",
            "ks08\t1\t\tthe president fred voted for has resigned .\n",
            "ks08\t1\t\thas no relative pronoun at all .\n",
            "ks08\t1\t\the is the kind of person with whom to consult .\n",
            "ks08\t1\t\tthese are the things for which to be thankful .\n",
            "ks08\t1\t\twe will invite volunteers on whom to work .\n",
            "ks08\t1\t\tthis is the student pictures of whom appeared in the newspaper .\n",
            "ks08\t0\t*\tpictures of whom appeared in the newspaper ?\n",
            "ks08\t1\t\tthe people happy with the proposal left .\n",
            "ks08\t1\t\tthe person standing on my foot is heavy .\n",
            "ks08\t0\t*\tthe paper to finish by tomorrow is too long .\n",
            "ks08\t0\t*\tthe person stand on my foot is heavy .\n",
            "ks08\t0\t*\tthe person stood on my foot is heavy .\n",
            "ks08\t0\t*\tthe student met the senator john met bill .\n",
            "ks08\t0\t*\tthe student met the senator that john met bill .\n",
            "ks08\t0\t*\tthe student met the senator for john to meet bill .\n",
            "ks08\t1\t\tjack is the person whom jenny fell in love with .\n",
            "ks08\t1\t\tjack is the person with whom jenny fell in love .\n",
            "ks08\t0\t*\tjack is the person whom jenny fell in love .\n",
            "ks08\t1\t\ti met the critic whose remarks i wanted to object to .\n",
            "ks08\t1\t\tthis is the friend for whose mother kim gave a party .\n",
            "ks08\t1\t\tthe teacher set us a problem the answer to which we can find in the textbook .\n",
            "ks08\t1\t\twe called the senators who met fred .\n",
            "ks08\t1\t\tthe kid picked up the apple that fell down on the ground .\n",
            "ks08\t0\t*\tthe student met john came .\n",
            "ks08\t0\t*\tthe problem intrigued us bothered me .\n",
            "ks08\t1\t\the made a statement which everyone thought was really interesting and important .\n",
            "ks08\t1\t\tthey all agreed to include those matters which everyone believed had been excluded from the treaty .\n",
            "ks08\t1\t\tmary knows that john was elected .\n",
            "ks08\t1\t\tthat john was elected surprised frank .\n",
            "ks08\t1\t\tmary told bill that john was elected .\n",
            "ks08\t1\t\tthis is the book that we had read .\n",
            "ks08\t1\t\tthe president abandoned the people that voted for him .\n",
            "ks08\t1\t\tit is an argument that people think will never end in egypt .\n",
            "ks08\t0\t*\tevery essay she 's written and which i 've read is on that pile .\n",
            "ks08\t0\t*\tevery essay she 's written and that i 've read is on that pile .\n",
            "ks08\t1\t\tevery essay which she 's written and that i 've read is on that pile .\n",
            "ks08\t1\t\tevery essay that she 's written and which i 've read is on that pile .\n",
            "ks08\t1\t\tthe student whose turn it was left .\n",
            "ks08\t0\t*\tthe student that 's turn it was left .\n",
            "ks08\t1\t\tthe pencil with which he is writing broke .\n",
            "ks08\t0\t*\tthe pencil with that he is writing broke .\n",
            "ks08\t1\t\ta pencil with which to write broke .\n",
            "ks08\t0\t*\ta pencil with that to write broke .\n",
            "ks08\t0\t*\tthe people in who we placed our trust left .\n",
            "ks08\t0\t*\tthe person with who we were talking left .\n",
            "ks08\t1\t\tthe company in which they have invested left .\n",
            "ks08\t1\t\tthe people in whose house we stayed left .\n",
            "ks08\t1\t\tthe person with whom he felt most comfortable left .\n",
            "ks08\t1\t\the bought a bench on which to sit .\n",
            "ks08\t1\t\the bought a refrigerator in which to put the beer .\n",
            "ks08\t1\t\tthere is a bench for you to sit on .\n",
            "ks08\t0\t*\tkaren asked where for washington to put the chairs .\n",
            "ks08\t1\t\tthe person i met is from boston .\n",
            "ks08\t1\t\tthe box we put the books in is sealed .\n",
            "ks08\t1\t\the made a statement everyone thought was interesting and important .\n",
            "ks08\t1\t\tthey all agreed to include those matters everyone believed had been excluded from the treaty .\n",
            "ks08\t1\t\ti just know that the big 12 south teams everyone knew would win actually won the game .\n",
            "ks08\t1\t\tthe person who john asked for help thinks he is foolish .\n",
            "ks08\t1\t\tmary , who john asked for help , thinks he is foolish .\n",
            "ks08\t1\t\tjohn has two sisters , who became lawyers .\n",
            "ks08\t1\t\ti met the lady from france who grows peaches .\n",
            "ks08\t0\t*\ti met john who grows peaches .\n",
            "ks08\t0\t*\ti met her who grows peaches .\n",
            "ks08\t1\t\tin the classroom , the teacher praised john , whom i also respect .\n",
            "ks08\t1\t\treagan , whom the republicans nominated in 1980 , lived most of his life in california .\n",
            "ks08\t1\t\tevery student who attended the party had a good time .\n",
            "ks08\t0\t*\tevery student , who attended the party , had a good time .\n",
            "ks08\t1\t\tno student who scored 80 or more in the exam was ever failed .\n",
            "ks08\t0\t*\tno student , who scored 80 or more in the exam , was ever failed .\n",
            "ks08\t1\t\tthe contestant who won the first prize , who is the judge 's brother-in-law , sang dreadfully .\n",
            "ks08\t0\t*\tthe contestant , who is the judge 's brother-in-law , who won the first prize sang dreadfully .\n",
            "ks08\t1\t\the who laughs last laughs best .\n",
            "ks08\t1\t\the who is without sin among you , let him cast the first stone .\n",
            "ks08\t1\t\twho did he believe that he would one day meet ?\n",
            "ks08\t1\t\twhich celebrity did he mention that he had run into ?\n",
            "ks08\t0\t*\twho did he believe the claim that he had never met ?\n",
            "ks08\t0\t*\twhich celebrity did he mention the fact that he had run into ?\n",
            "ks08\t1\t\tthe knife which he threw into the sea had a gold handle .\n",
            "ks08\t1\t\tthe knife that he threw into the sea had a gold handle .\n",
            "ks08\t1\t\tthe knife , which he threw into the sea had a gold handle .\n",
            "ks08\t0\t??\tthe knife , that he threw into the sea had a gold handle .\n",
            "ks08\t1\t\tbill cooked supper and washed the dishes .\n",
            "ks08\t0\t*\twhat did bill cook and wash the dishes ?\n",
            "ks08\t0\t*\twhat did bill cook supper and wash ?\n",
            "ks08\t1\t\the refuted the proof that you can not square it .\n",
            "ks08\t0\t*\twhat did he refute the proof that you can not square ?\n",
            "ks08\t1\t\tthey met someone who knows the professor .\n",
            "ks08\t0\t*\twhich professor did they meet someone who knows ?\n",
            "ks08\t1\t\tthat he has met the professor is extremely unlikely .\n",
            "ks08\t0\t*\twho is that he has met extremely unlikely ?\n",
            "ks08\t1\t\tshe bought john 's book .\n",
            "ks08\t1\t\tdid john wonder who would win the game ?\n",
            "ks08\t0\t*\twhat did john wonder who would win ?\n",
            "ks08\t1\t\twhat did he get the impression that the problem really was ?\n",
            "ks08\t1\t\tthis is the paper that we really need to find the linguist who understands .\n",
            "ks08\t0\t*\twhich rebel leader did you hear cheney 's rumor that the cia assassinated ?\n",
            "ks08\t1\t\tstudents enter high-level educational institutions might face many problems relating to study habits .\n",
            "ks08\t1\t\ta fellow student saw this felt sorry for miss kim and offered her his own book .\n",
            "ks08\t1\t\texperts all agree that dreams cause great anxiety and stress are called nightmares .\n",
            "ks08\t1\t\tthe victims of the earthquake their property was destroyed in the disaster were given temporary housing by the government .\n",
            "ks08\t1\t\tthis is the book which i need to read .\n",
            "ks08\t1\t\tthe person whom they intended to speak with agreed to reimburse us .\n",
            "ks08\t1\t\tthe motor that martha thinks that joe replaced costs thirty dollars .\n",
            "ks08\t1\t\tthe official to whom smith loaned the money has been indicted .\n",
            "ks08\t1\t\tthe man on whose lap the puppet is sitting is ventriloquist .\n",
            "ks08\t1\t\twe just finished the final exam the result of which we can find out next week .\n",
            "ks08\t0\t*\twhat did herb start to play only after he drank ?\n",
            "ks08\t0\t*\twho did herb believe the claim that cheated ?\n",
            "ks08\t0\t*\twhat was that the vikings ate a real surprise to you ?\n",
            "ks08\t0\t*\twhat did you meet someone who understands ?\n",
            "ks08\t1\t\tthe fact that scientists have now established all the genes in the human body is still not widely known .\n",
            "ks08\t1\t\tthe fact that the scientists used the latest technology to verify was reported at the recent conference .\n",
            "ks08\t1\t\tthey ignored the suggestion that lee made .\n",
            "ks08\t1\t\tthey ignored the suggestion that lee lied .\n",
            "ks08\t1\t\tthey denied the claim that we had advanced by ourselves .\n",
            "ks08\t1\t\tthey denied the claim that they should report only to us .\n",
            "ks08\t1\t\tthe hotel where gloria stays is being remodelled .\n",
            "ks08\t1\t\tthe day when jim got fired was a sad day for everyone .\n",
            "ks08\t1\t\tjohn is tough to persuade .\n",
            "ks08\t1\t\tjohn made it clear that he would finish it on time .\n",
            "ks08\t1\t\tit is john that i met last night in the park .\n",
            "ks08\t1\t\ti wonder whom sandy loves .\n",
            "ks08\t1\t\tthis is the politician on whom sandy relies .\n",
            "ks08\t1\t\the is hard to love .\n",
            "ks08\t1\t\tit is easy to please john .\n",
            "ks08\t1\t\tjohn is easy to please .\n",
            "ks08\t0\t*\tto please john is eager .\n",
            "ks08\t0\t*\tit is eager to please john .\n",
            "ks08\t1\t\tjohn is eager to please .\n",
            "ks08\t1\t\tto please john is tough .\n",
            "ks08\t1\t\tit is tough to please john .\n",
            "ks08\t1\t\tjohn is tough to please .\n",
            "ks08\t0\t*\tto please john is ready .\n",
            "ks08\t0\t*\tit is ready to please john .\n",
            "ks08\t1\t\tjohn is ready to please .\n",
            "ks08\t1\t\tkim is easy to please .\n",
            "ks08\t1\t\tkim is eager to please .\n",
            "ks08\t1\t\tthis doll is hard to see .\n",
            "ks08\t1\t\tthe child is impossible to teach .\n",
            "ks08\t1\t\tthe problem is easy to solve .\n",
            "ks08\t0\t*\tthis doll is hard to see it .\n",
            "ks08\t0\t*\tthe child is impossible to teach him .\n",
            "ks08\t0\t*\tthe problem is easy to solve the question .\n",
            "ks08\t1\t\tjohn is eager to examine the patient .\n",
            "ks08\t1\t\tjohn is eager to find a new home .\n",
            "ks08\t0\t*\tjohn is eager to examine .\n",
            "ks08\t0\t*\tjohn is eager to find .\n",
            "ks08\t1\t\thei is easy to please i .\n",
            "ks08\t1\t\tthis theorem will take only five minutes to prove .\n",
            "ks08\t1\t\tthis theorem will take only five minutes to establish that he proved in 1930 .\n",
            "ks08\t1\t\tthis scratch will cost kim $ 500 to fix .\n",
            "ks08\t1\t\tthis $ 500 bribe will cost the government $ 500,000 to prove that senator jones accepted .\n",
            "ks08\t0\t*\tkim is eager to recommend .\n",
            "ks08\t1\t\twho is kim eager to recommend ?\n",
            "ks08\t1\t\tthis sonata is easy to play on this piano .\n",
            "ks08\t1\t\twhich piano is this sonata easy to play on ?\n",
            "ks08\t1\t\tthat dogs bark annoys people .\n",
            "ks08\t1\t\tit annoys people that dogs bark .\n",
            "ks08\t1\t\twhy she told him is unclear .\n",
            "ks08\t1\t\tit is unclear why she told him .\n",
            "ks08\t1\t\tto leave so soon would be inconvenience .\n",
            "ks08\t1\t\tit would be inconvenience to leave so soon .\n",
            "ks08\t1\t\tit would be inconvenience for you to leave so soon .\n",
            "ks08\t1\t\tthat the dalai lama claims tibet independence disturbs the chinese government .\n",
            "ks08\t1\t\tit disturbs the chinese government that the dalai lama claims tibet independence .\n",
            "ks08\t1\t\ti believe the problem to be obvious .\n",
            "ks08\t0\t*\ti believe that the problem is not easy to be obvious .\n",
            "ks08\t1\t\ti believe it to be obvious that the problem is not easy .\n",
            "ks08\t1\t\ti do not think it unreasonable to ask for the return of my subscription .\n",
            "ks08\t1\t\the made it clear he would continue to co-operate with the united nations .\n",
            "ks08\t1\t\tthey 're not finding it a stress being in the same office .\n",
            "ks08\t1\t\tthat you came early surprised me .\n",
            "ks08\t1\t\tit surprised me that you came early .\n",
            "ks08\t0\t*\tsurprised me that you came early .\n",
            "ks08\t1\t\tthat chris knew the answer occurred to pat .\n",
            "ks08\t1\t\tit occurred to pat that chris knew the answer .\n",
            "ks08\t1\t\tit really freaks me out that we invaded iraq .\n",
            "ks08\t1\t\tthat we invaded iraq really creeps me out .\n",
            "ks08\t1\t\tthat we invaded iraq really freaks me out .\n",
            "ks08\t1\t\tit really bites that we invaded iraq .\n",
            "ks08\t1\t\tthat fido barks annoys me .\n",
            "ks08\t1\t\ta man came into the room that no one knew .\n",
            "ks08\t1\t\ta man came into the room with blond hair .\n",
            "ks08\t1\t\ti read a book during the vacation which was written by chomsky .\n",
            "ks08\t1\t\tray found the outcome frustrating .\n",
            "ks08\t1\t\tray found it frustrating that his policies made little impact on poverty .\n",
            "ks08\t0\t*\ti made to settle the matter my objective .\n",
            "ks08\t1\t\ti made it my objective to settle the matter .\n",
            "ks08\t1\t\ti made the settlement of the matter my objective .\n",
            "ks08\t0\t*\ti owe that the jury acquitted me to you .\n",
            "ks08\t1\t\ti owe it to you that the jury acquitted me .\n",
            "ks08\t1\t\ti owe my acquittal to you .\n",
            "ks08\t1\t\ti believe strongly that the world is round .\n",
            "ks08\t0\t*\ti believe that the world is round strongly .\n",
            "ks08\t1\t\tit 's their teaching material that we 're using .\n",
            "ks08\t1\t\twhat we 're using is their teaching material .\n",
            "ks08\t1\t\ttheir teaching material is what we are using .\n",
            "ks08\t1\t\twe are using their teaching material .\n",
            "ks08\t1\t\ti share your view but i just wonder why you think that 's good .\n",
            "ks08\t1\t\tit was the man that bought the articles from him .\n",
            "ks08\t1\t\tit was then that he felt a sharp pain .\n",
            "ks08\t1\t\tit was to the student that the teacher gave the best advice .\n",
            "ks08\t1\t\tit was not until i was perhaps twenty-five or thirty that i read and enjoyed them .\n",
            "ks08\t0\t*\tit was to finish the homework that john tried .\n",
            "ks08\t0\t*\tit is that bill is honest that john believes .\n",
            "ks08\t1\t\tit 's the second monday that we get back from easter holiday .\n",
            "ks08\t1\t\tit was the girl who kicked the ball .\n",
            "ks08\t1\t\tit 's mainly his attitude which convinced the teacher .\n",
            "ks08\t1\t\twhat you want is a little greenhouse .\n",
            "ks08\t1\t\twhat 's actually happening in london at the moment is immensely exciting .\n",
            "ks08\t1\t\twhat is to come is in this document .\n",
            "ks08\t1\t\twhat i 've always tended to do is to do my own stretches at home .\n",
            "ks08\t1\t\twhat i meant was that you have done it really well .\n",
            "ks08\t1\t\twhat happened is they caught her without a license .\n",
            "ks08\t1\t\twhat the gentleman seemed to be asking is how policy would have differed .\n",
            "ks08\t1\t\tinsensitive is how i would describe him .\n",
            "ks08\t1\t\tin the early morning is when i do my best research .\n",
            "ks08\t0\t*\twear it like that is what you do .\n",
            "ks08\t0\t*\tthey caught her without a license is what happened .\n",
            "ks08\t0\t*\tthat you have done it really well is what i meant .\n",
            "ks08\t1\t\tthat 's when i read .\n",
            "ks08\t1\t\tthat was why she looked so nice .\n",
            "ks08\t1\t\tthat 's how they do it .\n",
            "ks08\t1\t\tthat 's who i played with over christmas .\n",
            "ks08\t1\t\twhat you heard was an explosion .\n",
            "ks08\t1\t\tit was an explosion that you heard .\n",
            "ks08\t1\t\twhat you should do is order one first .\n",
            "ks08\t0\t*\tit is order one first that you should do first .\n",
            "ks08\t0\t*\torder one first is what you should do .\n",
            "ks08\t1\t\tit was not until i was perhaps twenty-five or thirty that i read them and enjoyed them .\n",
            "ks08\t0\t*\twhen i read them and enjoyed them was not until i was perhaps twenty-five .\n",
            "ks08\t0\t*\tnot until i was perhaps twenty-five was when i read them and enjoyed them .\n",
            "ks08\t1\t\tit 's the writer that gets you so involved .\n",
            "ks08\t0\t*\tthat gets you so involved is the writer .\n",
            "ks08\t0\t*\tthe writer is that gets you so involved .\n",
            "ks08\t1\t\tand it was this matter on which i consulted with the chairman of the select committee .\n",
            "ks08\t0\t*\ton which i consulted with the chairman of the select committee was this matter .\n",
            "ks08\t0\t*\tthis matter was on which i consulted with the chairman of the select committee .\n",
            "ks08\t1\t\twhat i ate is an apple .\n",
            "ks08\t1\t\twhat we are using is their teaching material .\n",
            "ks08\t1\t\tthe student who got a in the class was very happy .\n",
            "ks08\t1\t\tthe one who broke the window was mr. kim .\n",
            "ks08\t1\t\the got what he wanted .\n",
            "ks08\t1\t\the put the money where lee told him to put it .\n",
            "ks08\t1\t\tthe concert started when the bell rang .\n",
            "ks08\t0\t*\tlee wants to meet who kim hired .\n",
            "ks08\t0\t*\tlee solved the puzzle how kim solved it .\n",
            "ks08\t0\t*\twhich book he read the book was that one .\n",
            "ks08\t1\t\ti ate what john ate .\n",
            "ks08\t1\t\ti ate an apple .\n",
            "ks08\t0\t*\tto whom i gave the cake is john .\n",
            "ks08\t0\t*\tthat brought the letter is bill .\n",
            "ks08\t1\t\tthis is how he did it .\n",
            "ks08\t1\t\tthis is why he came early .\n",
            "ks08\t1\t\ttype a : it is on bill that john relies .\n",
            "ks08\t1\t\ttype b : it is bill on whom john relies .\n",
            "ks08\t1\t\tit was then when we all went to bed .\n",
            "ks08\t0\t*\tjohn that we are looking for showed up .\n",
            "ks08\t1\t\tit 's the second monday that we get back from easter .\n",
            "ks08\t1\t\tit was in 1997 when the in introduced the alien registration receipt card .\n",
            "ks08\t1\t\tit is uncle john whose address i lost .\n",
            "ks08\t0\t*\tit is kim on whom that sandy relies .\n",
            "ks08\t0\t*\tit is kim on whom sandy relies on .\n",
            "ks08\t0\t*\tit is kim whom sandy relies .\n",
            "ks08\t1\t\tit was the director that she wants to meet .\n",
            "ks08\t1\t\tit was the director that she said she wants to meet .\n",
            "ks08\t1\t\tit was the director that i think she said she wants to meet .\n",
            "ks08\t1\t\ti wonder who it was who saw you .\n",
            "ks08\t1\t\ti wonder who it was you saw .\n",
            "ks08\t1\t\ti wonder in which pocket it was that kim had hidden the jewels .\n",
            "ks08\t1\t\twho do you think it is that mary met ?\n",
            "ks08\t0\t*\tto whom do you think it is the book that mary gave ?\n",
            "ks08\t1\t\tit is difficult for me to concentrate on calculus .\n",
            "ks08\t1\t\tfor me to concentrate on calculus is difficult .\n",
            "ks08\t1\t\tcalculus is difficult for me to concentrate on .\n",
            "ks08\t1\t\tbeing lovely to look at has its advantages .\n",
            "ks08\t1\t\tletters to grandma are easy to help the children to write .\n",
            "ks08\t1\t\tit was to boston that they decided to take the patient .\n",
            "ks08\t1\t\tit was with a great deal of regret that i vetoed your proposal .\n",
            "ks08\t1\t\tit was tom who spilled beer on this couch .\n",
            "ks08\t1\t\tit is martha whose work critics will praise .\n",
            "ks08\t1\t\tit was john on whom the sheriff placed the blame .\n",
            "ks08\t1\t\ti wondered who it was you saw .\n",
            "ks08\t1\t\ti was wondering in which pocket it was that kim had hidden the jewels .\n",
            "ks08\t0\t*\tit is on kim on whom sandy relies .\n",
            "ks08\t1\t\twas it for this that we suffered and toiled ?\n",
            "ks08\t1\t\twho was it who interviewed you ?\n",
            "ks08\t1\t\ti believe it to be her father who was primarily responsible .\n",
            "ks08\t1\t\ti believe it to be the switch that is defective .\n",
            "ks08\t1\t\ttom ate what mary offered to him .\n",
            "ks08\t1\t\ti wonder what mary offered to him .\n",
            "ks08\t1\t\twhat mary offered to him is unclear .\n",
            "kl93\t1\t\ti do n't have any potatoes .\n",
            "kl93\t0\t*\ti have any potatoes .\n",
            "kl93\t1\t\tat most three girls saw anything .\n",
            "kl93\t0\t*\tat least three girls saw anything .\n",
            "kl93\t1\t\tevery girl who saw anything was happy .\n",
            "kl93\t0\t*\tsome girl who saw anything was happy .\n",
            "kl93\t1\t\tany owl hunts mice .\n",
            "kl93\t1\t\tany lawyer could tell you that .\n",
            "kl93\t1\t\ti would dance with anybody .\n",
            "kl93\t1\t\talmost every lawyer could answer that question .\n",
            "kl93\t1\t\talmost no lawyer could answer that question .\n",
            "kl93\t1\t\talmost any lawyer could answer that question .\n",
            "kl93\t0\t*\ti do n't have almost any potatoes .\n",
            "kl93\t1\t\ti would dance with mary or sue .\n",
            "kl93\t1\t\tmary or sue could tell you that .\n",
            "kl93\t1\t\tdo you have dry socks ? claim .\n",
            "kl93\t1\t\tperhaps some dry socks would help ?\n",
            "kl93\t1\t\tan owl hunts mice .\n",
            "kl93\t1\t\tgenerics allow exceptions .\n",
            "kl93\t1\t\ta poodle gives live birth .\n",
            "kl93\t1\t\tevery poodle gives live birth .\n",
            "kl93\t1\t\ti do n't have potatoes .\n",
            "kl93\t1\t\tevery man who has matches is happy .\n",
            "kl93\t1\t\tevery man who has any matches is happy .\n",
            "kl93\t1\t\tcould we make some french fries ?\n",
            "kl93\t1\t\twhy do n't we make some french fries ?\n",
            "kl93\t1\t\tare you prepared for school tomorrow ?\n",
            "kl93\t1\t\tand then all the owls go on a mice hunt .\n",
            "kl93\t1\t\tif you take a dry match and strike it , it lights .\n",
            "kl93\t1\t\tat most three teachers assigned homework .\n",
            "kl93\t1\t\tat most three teachers assigned any homework .\n",
            "kl93\t1\t\tevery student who handed in some homework will get a prize .\n",
            "kl93\t1\t\tevery student who handed in any homework will get a prize .\n",
            "kl93\t1\t\tbefore you make plans , consult the secretary .\n",
            "kl93\t1\t\tbefore you make any plans , consult the secretary .\n",
            "kl93\t1\t\tis there anything i can do for you ?\n",
            "kl93\t1\t\ta professional dancer would be able to do it .\n",
            "kl93\t1\t\tany professional dancer would be able to do it .\n",
            "kl93\t1\t\twe do n't have potatoes , or at least not enough .\n",
            "kl93\t1\t\tevery man who has any matches is happy . happy .\n",
            "kl93\t0\t*\tevery boy has any potatoes .\n",
            "kl93\t0\t*\tit 's not the case that every boy has any potatoes .\n",
            "kl93\t1\t\ti 'm surprised we had any potatoes .\n",
            "kl93\t1\t\tat most three boys did n't see anything .\n",
            "kl93\t0\t*\teven sue said anything .\n",
            "kl93\t1\t\tsue was the most likely not to say anything .\n",
            "kl93\t1\t\tsue said something although she was the most likely not to say anything .\n",
            "kl93\t1\t\tcows fly more often than john visits any relatives .\n",
            "kl93\t0\t*\teach candidate who has any interest in semantics will be admit ted to the department .\n",
            "kl93\t1\t\tevery child should have a daily glass of milk .\n",
            "kl93\t1\t\teach child should have a daily glass of milk .\n",
            "kl93\t1\t\ti 'm surprised that he ever said anything .\n",
            "kl93\t1\t\ti 'm sorry that he ever said anything .\n",
            "kl93\t0\t*\ti 'm glad that i ever met him .\n",
            "kl93\t0\t*\ti 'm sure that i ever met him .\n",
            "kl93\t1\t\ti 'm surprised he bought a car .\n",
            "kl93\t1\t\tbut these tickets are terrible !\n",
            "kl93\t1\t\ti was surprised that he stole the watch , in as far as that was a daring thing to do .\n",
            "kl93\t1\t\tgiven my high opinion on his moral character , i was surprised that he stole the watch .\n",
            "kl93\t1\t\twere you surprised that he stole the watch ?\n",
            "kl93\t1\t\ti 'm sorry that anybody hates me .\n",
            "kl93\t1\t\ti want for nobody to hate me .\n",
            "kl93\t1\t\ti 'm glad he bought a car .\n",
            "kl93\t1\t\ti 'm sorry he bought a car .\n",
            "kl93\t1\t\the bought a honda .\n",
            "kl93\t0\t*\ti 'm glad i saw anybody .\n",
            "kl93\t1\t\ti 'm glad anybody likes me !\n",
            "kl93\t1\t\tcould n't you get any tickets better than this ?\n",
            "kl93\t1\t\tit 's fine that he paid and apologized , but i do n't really care about his gratitude , or the money , or anything .\n",
            "kl93\t0\t*\ti 'm sure we got any tickets !\n",
            "kl93\t1\t\ti 'm sure he speaks to me !\n",
            "kl93\t1\t\ti 'm glad a linguist likes me .\n",
            "kl93\t1\t\ti did n't help him because i have any sympathy for urban guerillas .\n",
            "kl93\t0\t*\tit is n't because sue said anything bad about me that i 'm angry , although she did say some bad things about me .\n",
            "kl93\t1\t\ti do n't have any sympathy for urban guerillas .\n",
            "kl93\t0\t*\talmost an owl hunts mice .\n",
            "kl93\t0\t*\tabsolutely an owl hunts mice .\n",
            "kl93\t1\t\talmost any owl hunts mice .\n",
            "kl93\t1\t\tabsolutely any owl hunts mice .\n",
            "b_82\t1\t\the began writing poems .\n",
            "b_82\t1\t\the kept writing poems .\n",
            "b_82\t1\t\the continued writing poems .\n",
            "b_82\t1\t\the stopped writing poems .\n",
            "b_82\t1\t\tthe men would have all been working .\n",
            "b_82\t1\t\tthe men would have been all working .\n",
            "b_82\t1\t\twould the men each have been working ?\n",
            "b_82\t0\t*\twould each the men have been working ?\n",
            "b_82\t1\t\tthe men would not enjoy that .\n",
            "b_82\t0\t*\twould not the men enjoy that ?\n",
            "b_82\t1\t\twould the men not enjoy that ?\n",
            "b_82\t0\t*\tthe men would all not have been working .\n",
            "b_82\t1\t\tthe men all would not have been working .\n",
            "b_82\t1\t\tthe men would not have all been working .\n",
            "b_82\t1\t\tthe men would not all have been working .\n",
            "b_82\t1\t\tthe men would not have been all working .\n",
            "b_82\t1\t\tthat john is a fool is obvious .\n",
            "b_82\t1\t\tit is obvious that john is a fool .\n",
            "b_82\t0\t*\tjohn believes that fred likes steak that joe likes pizza .\n",
            "b_82\t1\t\tjohn whined that he was hungry .\n",
            "b_82\t0\t*\tthat he was hungry was whined by john .\n",
            "b_82\t1\t\tjohn is certain that the mets will win .\n",
            "b_82\t1\t\tthat he has blood on his hands proves that john is the murderer .\n",
            "b_82\t0\t*\tit proves that john is the murderer that he has blood on his hands .\n",
            "b_82\t1\t\tto please john would be difficult .\n",
            "b_82\t1\t\tit would be difficult to please john .\n",
            "b_82\t1\t\tit is believed to be obvious by everyone that fred is crazy .\n",
            "b_82\t0\t*\tjohn is believed to be certain by everyone that fred is crazy .\n",
            "b_82\t1\t\tit disturbed him that people did n't like fred .\n",
            "b_82\t1\t\tit was believed to have disturbed him that people did n't like fred .\n",
            "b_82\t0\t*\thow easy to please john is it ?\n",
            "b_82\t0\t*\thow difficult to study for the exam was it ?\n",
            "b_82\t0\t*\thow hard to read the book was it ?\n",
            "b_82\t0\t*\thow easy to tease john it is !\n",
            "b_82\t0\t*\thow hard to read the book it was !\n",
            "b_82\t1\t\thow certain that the mets will win are you ?\n",
            "b_82\t1\t\thow likely to win is he ?\n",
            "b_82\t1\t\tthis book i enjoyed .\n",
            "b_82\t0\t*\tto whom the book did you give .\n",
            "b_82\t0\t*\tthe book to whom did you give .\n",
            "b_82\t1\t\the 's a man to whom liberty we could never grant .\n",
            "b_82\t1\t\tit 's obvious that mary , he ca n't stand .\n",
            "b_82\t1\t\ti think that the trolls will take the shepherd tomorrow .\n",
            "b_82\t1\t\tas for max , i really like him .\n",
            "b_82\t0\t*\the 's a man to whom as for liberty , we could never grant it .\n",
            "b_82\t0\t*\the 's a man to whom liberty , we could never grant it .\n",
            "b_82\t1\t\tjohn would like that because he 's such a nice guy .\n",
            "b_82\t1\t\tjohn , because he 's such a nice guy , would like that .\n",
            "b_82\t1\t\tbecause he 's such a nice guy , john would like that .\n",
            "b_82\t1\t\tjohn would , because he 's such a nice guy , like that .\n",
            "b_82\t1\t\tbecause he 's such a nice guy , what would john like ?\n",
            "b_82\t1\t\tit 's obvious that , although he 's a nice guy , john is n't too bright .\n",
            "b_82\t0\t*\tjohn ate after getting home the steak .\n",
            "b_82\t1\t\ti gave mary a book .\n",
            "b_82\t1\t\ti considered fred crazy .\n",
            "b_82\t1\t\ti put the book on the table .\n",
            "b_82\t1\t\ti worded the telegram tersely .\n",
            "b_82\t0\t*\ti considered fred after the party crazy .\n",
            "b_82\t0\t*\t1 put the book after the party on the table .\n",
            "b_82\t0\t*\ti worded the telegram after the party tersely .\n",
            "b_82\t0\t*\tbecause she 's so pleasant , mary i really like her .\n",
            "b_82\t1\t\tbecause she 's so pleasant , mary i really like .\n",
            "b_82\t1\t\tthough he may seem intelligent , he does not seem deep .\n",
            "b_82\t1\t\tintelligent though he may seem , he does not seem deep .\n",
            "b_82\t1\t\tthough i may love her , that wo n't affect the grade .\n",
            "b_82\t1\t\tlove her though i may , that wo n't affect the grade .\n",
            "b_82\t0\t*\thandsome though i believe the claim that tom is , i still wo n't date him .\n",
            "b_82\t0\t*\thandsome though they told me that tom is , i still wo n't date him .\n",
            "b_82\t0\t*\thandsome though my friends suggested that mary thinks that tom is , i still wo n't date him .\n",
            "b_82\t1\t\thate those who criticize carter though he may , it does n't matter .\n",
            "b_82\t1\t\twould john hate that ?\n",
            "b_82\t1\t\twould john hate that !\n",
            "b_82\t0\t*\twill , after john comes home , sally take a shower ?\n",
            "b_82\t1\t\twill sally , after john comes home , take a shower ?\n",
            "b_82\t1\t\tafter john comes home , will sally take a shower ?\n",
            "b_82\t1\t\ti would prefer that he not have finished .\n",
            "b_82\t0\t*\ti would prefer that he have not finished .\n",
            "b_82\t1\t\the has not finished .\n",
            "b_82\t1\t\the is not finishing .\n",
            "b_82\t1\t\the would not finish .\n",
            "b_82\t1\t\the does not finish .\n",
            "b_82\t0\t*\tthose people will , after the party , not come home .\n",
            "b_82\t1\t\tthose people , after the party , will not come home .\n",
            "b_73\t1\t\ti 've never seen a man taller than my father .\n",
            "b_73\t1\t\ti 've never seen a taller man than my father .\n",
            "b_73\t1\t\ti 've never seen a man taller than my mother .\n",
            "b_73\t1\t\ti 've never seen a taller man than my mother .\n",
            "b_73\t1\t\tjack eats caviar more than he eats mush .\n",
            "b_73\t1\t\tjack eats more caviar than he eats mush .\n",
            "b_73\t1\t\tjack eats caviar more than he sleeps .\n",
            "b_73\t0\t*\tjack eats more caviar than he sleeps .\n",
            "b_73\t1\t\ti am more angry today than i was yesterday .\n",
            "b_73\t1\t\ti am more angry than sad .\n",
            "b_73\t0\t*\ti am angrier than sad .\n",
            "b_73\t1\t\tmary is more than six feet tall .\n",
            "b_73\t1\t\tmary is taller than six feet .\n",
            "b_73\t0\t*\tmary is more than five feet short .\n",
            "b_73\t1\t\tmary is shorter than five feet .\n",
            "b_73\t1\t\tthey think she has too much independence .\n",
            "b_73\t0\t*\tthey think she is too much happy .\n",
            "b_73\t0\t*\tmary speaks so much gently .\n",
            "b_73\t0\t*\ta tangerine is n't as much different from an orange as i 'd thought .\n",
            "b_73\t1\t\ta tangerine is n't as different from an orange as i 'd thought .\n",
            "b_73\t1\t\tyou and i are as much alike as a horse and a cow .\n",
            "b_73\t1\t\tyou and i are as alike as a horse and a cow .\n",
            "b_73\t1\t\tyou and i are as little alike as a horse and a cow .\n",
            "b_73\t0\t*\tjohn is as much intelligent as mary .\n",
            "b_73\t1\t\tjohn is as intelligent as mary .\n",
            "b_73\t1\t\tjohn is more than 6 feet tall .\n",
            "b_73\t1\t\tjohn is taller than 6 feet .\n",
            "b_73\t1\t\tthese plants may grow as much as 6 feet high .\n",
            "b_73\t1\t\tthese plants may grow as high as 6 feet .\n",
            "b_73\t1\t\tmore has happened in the last week than will happen in the next year .\n",
            "b_73\t1\t\the offers more than we had hoped for .\n",
            "b_73\t1\t\the was hoping for more than we offered .\n",
            "b_73\t1\t\tenough is going on to keep them confused .\n",
            "b_73\t1\t\tyou 've said enough to convince me .\n",
            "b_73\t1\t\tsally eats caviar more than i had expected .\n",
            "b_73\t1\t\tsusan does n't eat her vegetables enough .\n",
            "b_73\t1\t\tsally eats the stuff pretty often .\n",
            "b_73\t0\t*\tsally eats pretty often the stuff .\n",
            "b_73\t1\t\tsally eats the stuff more .\n",
            "b_73\t0\t*\tsally eats more the stuff .\n",
            "b_73\t0\t*\tsusan does n't eat enough her vegetables .\n",
            "b_73\t1\t\tjohn eats more .\n",
            "b_73\t1\t\tjohn does n't eat enough .\n",
            "b_73\t1\t\tjohn eats more than he sleeps .\n",
            "b_73\t1\t\the gave me more of his marbles than i wanted .\n",
            "b_73\t0\t*\tsally enough eats caviar .\n",
            "b_73\t0\t*\tenough sally eats caviar .\n",
            "b_73\t1\t\tjack is more tall than thin .\n",
            "b_73\t1\t\ti did it more in jest than in anger .\n",
            "b_73\t1\t\tthere is enough of the bread left to have tomorrow .\n",
            "b_73\t1\t\tthere is enough bread for all of you .\n",
            "b_73\t1\t\tthere is bread enough for all of you .\n",
            "b_73\t1\t\tshe has enough of a problem as it is .\n",
            "b_73\t0\t*\tshe has enough a problem as it is .\n",
            "b_73\t0\t*\tshe has enough problem as it is .\n",
            "b_73\t0\t*\tshe has problem enough as it is .\n",
            "b_73\t0\t*\tshe has enough of problems as it is .\n",
            "b_73\t1\t\tshe has enough problems as it is .\n",
            "b_73\t1\t\the looks more formidable than he is .\n",
            "b_73\t0\t*\the seems enough intelligent for you .\n",
            "b_73\t1\t\the seems intelligent enough for you .\n",
            "b_73\t1\t\tshe writes more clearly than she speaks .\n",
            "b_73\t0\t*\tshe speaks enough clearly to be understood .\n",
            "b_73\t1\t\the 's enough of a fool to try it .\n",
            "b_73\t1\t\the 's fool enough to try it .\n",
            "b_73\t1\t\ti saw more of the man than you did .\n",
            "b_73\t1\t\ti saw enough of the fool to be convinced .\n",
            "b_73\t1\t\tharry got to be more of a celebrity .\n",
            "b_73\t1\t\tharry got to be more of the celebrity .\n",
            "b_73\t0\t*\the 's enough of the coward to pull the trigger .\n",
            "b_73\t1\t\twhat his father wants him to be is more of a man .\n",
            "b_73\t0\t*\tmore of a man is here .\n",
            "b_73\t0\t*\ti 've kicked more of a man than you have .\n",
            "b_73\t0\t*\ti 've known more of a man than frank .\n",
            "b_73\t1\t\ti 've never known more of a man than frank .\n",
            "b_73\t1\t\the was hoping for too much .\n",
            "b_73\t1\t\tsally eats caviar too much for her own good .\n",
            "b_73\t1\t\tjohn eats so much .\n",
            "b_73\t1\t\the gave me many marbles .\n",
            "b_73\t1\t\ti have much typing to do .\n",
            "b_73\t0\t*\the looks so much formidable .\n",
            "b_73\t1\t\the looks so formidable .\n",
            "b_73\t0\t*\tshe speaks too much clearly .\n",
            "b_73\t1\t\tshe speaks too clearly .\n",
            "b_73\t1\t\ti 'm as much of a man as you are , my dear .\n",
            "b_73\t1\t\tharry got to be as much of a celebrity as his father .\n",
            "b_73\t0\t*\tharry got to be as much of the celebrity as his father .\n",
            "b_73\t0\t*\tas much of a man is here .\n",
            "b_73\t0\t*\ti 've seen as much of a coward as frank .\n",
            "b_73\t1\t\tmany are called ; few are chosen .\n",
            "b_73\t1\t\tmore are called than are ever chosen .\n",
            "b_73\t1\t\twe made enough pudding to last for days .\n",
            "b_73\t0\t*\twe ate enough a pudding to satisfy us .\n",
            "b_73\t1\t\twe made enough puddings to last for days .\n",
            "b_73\t0\t*\twe ate enough the puddings to satisfy us .\n",
            "b_73\t1\t\tjohn is the kind of a fool that i told you about .\n",
            "b_73\t1\t\tjohn is the kind of the fool that i told you about .\n",
            "b_73\t1\t\the 's a bit of a gossip .\n",
            "b_73\t0\t*\the 's the bit of a gossip .\n",
            "b_73\t1\t\the 's something of a gossip .\n",
            "b_73\t1\t\tjohn is the kind of fool that i told you about .\n",
            "b_73\t0\t*\the 's fool .\n",
            "b_73\t0\t*\the 's a fool enough to try it .\n",
            "b_73\t0\t*\tshe 's just enough tall .\n",
            "b_73\t0\t*\tshe 's enough tall .\n",
            "b_73\t0\t*\tshe speaks enough clearly .\n",
            "b_73\t1\t\the 's that reliable a man .\n",
            "b_73\t0\t*\the 's a that reliable man .\n",
            "b_73\t1\t\the 's too reliable a man .\n",
            "b_73\t0\t*\the 's a too reliable man .\n",
            "b_73\t1\t\the 's as reliable a man .\n",
            "b_73\t0\t*\the 's an as reliable man .\n",
            "b_73\t1\t\the 's so reliable a man .\n",
            "b_73\t0\t*\the 's a so reliable man .\n",
            "b_73\t0\t*\the 's more reliable a man .\n",
            "b_73\t0\t*\the 's reliable enough a man .\n",
            "b_73\t1\t\the 's a reliable enough man .\n",
            "b_73\t1\t\ttom was not more reliable than a grasshopper .\n",
            "b_73\t1\t\ttom was no more reliable than a grasshopper .\n",
            "b_73\t0\t*\tnot more reliable a man could be found .\n",
            "b_73\t0\t*\tany more reliable a man could not be found .\n",
            "b_73\t1\t\ti do n't want trouble .\n",
            "b_73\t0\t*\tjohn is not more reliable a fellow than bill .\n",
            "b_73\t1\t\tjohn is not a more reliable fellow than bill .\n",
            "b_73\t1\t\tjohn is n't any more reliable a fellow than bill .\n",
            "b_73\t0\t*\tjohn is n't an any more reliable fellow than bill .\n",
            "b_73\t1\t\tjohn is no more reliable a fellow than bill .\n",
            "b_73\t0\t*\tjohn is a no more reliable fellow than bill .\n",
            "b_73\t1\t\ti have as many too many marbles as you .\n",
            "b_73\t1\t\ti have as many marbles too many as you .\n",
            "b_73\t1\t\ti have six too many marbles .\n",
            "b_73\t1\t\ti have six marbles too many .\n",
            "b_73\t1\t\ti have six more of them .\n",
            "b_73\t0\t*\ti have six of them more .\n",
            "b_73\t1\t\ti have half a dozen too many of these marbles .\n",
            "b_73\t0\t*\ti have half a dozen of these marbles too many .\n",
            "b_73\t1\t\tshe writes clearly enough .\n",
            "b_73\t1\t\tshe is as brilliant a woman as her mother .\n",
            "b_73\t0\t*\tshe is as brilliant the woman as her mother .\n",
            "b_73\t1\t\ti 've never known as strong a person as louise .\n",
            "b_73\t1\t\tfido is a smarter dog than spot .\n",
            "b_73\t0\t*\tfido is the smarter dog than spot .\n",
            "b_73\t1\t\twhat his father wants him to be is a better pool player .\n",
            "b_73\t1\t\ta taller man than bill is here .\n",
            "b_73\t1\t\ti 've never known a smarter dog than fido .\n",
            "b_73\t1\t\the 's so tall a man that doors are dangerous to him .\n",
            "b_73\t1\t\the 's such a tall man that doors are dangerous to him .\n",
            "b_73\t1\t\the 's such a tall man .\n",
            "b_73\t1\t\the 's such the tall man .\n",
            "b_73\t1\t\twhat her mother wants her to be is such a fine surgeon that everyone will respect her .\n",
            "b_73\t1\t\tit was as awful a picture as it first seemed .\n",
            "b_73\t0\t*\tit was so awful a picture as it first seemed .\n",
            "b_73\t1\t\tit was n't as awful a picture as it first seemed .\n",
            "b_73\t1\t\tit was n't such an awful picture as it first seemed .\n",
            "b_73\t1\t\tit was so awful a picture that i tore it up .\n",
            "b_73\t1\t\tit was such an awful picture that i tore it up .\n",
            "b_73\t1\t\tmary is such a wit that people are afraid of her .\n",
            "b_73\t1\t\tsally is n't such a fool as people think .\n",
            "b_73\t0\t*\tsally is such a fool as people think .\n",
            "b_73\t1\t\ti love her so much .\n",
            "b_73\t1\t\ti gave her so much .\n",
            "b_73\t0\t*\ti gave her so .\n",
            "b_73\t1\t\thilda is such a scholar .\n",
            "b_73\t1\t\thilda is such a scholar that all her work is impeccable .\n",
            "b_73\t1\t\thilda is such a scholar as you were speaking of just now .\n",
            "b_73\t1\t\tso eminent a scholar as dr. lucille hein was here .\n",
            "b_73\t1\t\tsuch an eminent scholar as dr. lucille hein was here .\n",
            "b_73\t1\t\tso elegant a solution as you have presented us with can elicit only admiration .\n",
            "b_73\t1\t\tyou have presented so elegant a solution that we can only admire it .\n",
            "b_73\t1\t\tsuch a scholar as you were speaking of just now is here .\n",
            "b_73\t0\t*\tso much of a scholar is here .\n",
            "b_73\t1\t\ther mother wants mary to be such an eminent woman that everyone will respect her .\n",
            "b_73\t1\t\tjohn a decidedly taller man than bill .\n",
            "b_73\t0\t*\tjohn is a decidedly too tall man .\n",
            "b_73\t1\t\tthat 's an obviously better solution .\n",
            "b_73\t0\t*\tthat 's an obviously so good solution .\n",
            "b_73\t1\t\tshe made so much better a reply .\n",
            "b_73\t0\t*\tshe made such a much better reply .\n",
            "b_73\t0\t*\tshe made such a better reply .\n",
            "b_73\t0\t*\tthat 's the most kind answer that i ever heard .\n",
            "b_73\t0\t*\tthat 's a most kind answer that i ever heard .\n",
            "b_73\t0\t*\tthat 's a kindest answer that i ever heard .\n",
            "b_73\t1\t\tthat 's the kindest answer that i ever heard .\n",
            "b_73\t1\t\tmost helpful advice is unwanted .\n",
            "b_73\t1\t\tsally will give me more helpful advice than the advice i got from you .\n",
            "b_73\t1\t\ti 've never seen a man who is taller than my mother .\n",
            "b_73\t1\t\ti 've never seen the one man taller than my father .\n",
            "b_73\t0\t*\ti 've never seen the taller man than my father .\n",
            "b_73\t0\t*\ti 've never seen the one taller man than my father .\n",
            "b_73\t1\t\tjohn wants to come up with as good a solution as christine did .\n",
            "b_73\t1\t\tjohn wants to come up with a solution as good as christine .\n",
            "b_73\t1\t\tjohn wants to find a solution better than christine 's .\n",
            "b_73\t1\t\tcaviar is eaten by jack more than mush .\n",
            "b_73\t1\t\tmore caviar than mush is eaten by jack .\n",
            "b_73\t1\t\tjack ate more of this than he ate of that .\n",
            "b_73\t1\t\tthe table is longer than the door is wide .\n",
            "b_73\t1\t\tmary 's happy about her work , and john 's happy about his children .\n",
            "b_73\t0\t*\tmary 's happy about her work , and john 's about his children .\n",
            "b_73\t1\t\tmary 's happy about her work , and john is about his children .\n",
            "b_73\t1\t\tmary is happy with her work , and john is with his children .\n",
            "b_73\t1\t\tmary 's happy with her work , and john 's with his children .\n",
            "b_73\t0\t*\tthe table is longer than the door 's wide .\n",
            "b_73\t1\t\tthe table is long , and the door 's wide .\n",
            "b_73\t0\t*\ti was happier there than i 'm here .\n",
            "b_73\t1\t\ti 'm sad , more than i 'm angry .\n",
            "b_73\t0\t*\ti 'm sadder than i 'm angry .\n",
            "b_73\t1\t\ti 'm more sad than angry .\n",
            "b_73\t1\t\ti 'm worrying , more than thinking .\n",
            "b_73\t0\t*\ti 'm more worrying than thinking .\n",
            "b_73\t0\t*\ti 'm sadder than angry .\n",
            "b_73\t1\t\ti 'm sad , as much as i 'm angry .\n",
            "b_73\t1\t\ti 'm as much sad as angry .\n",
            "b_73\t0\t*\ti 'm as sad as angry .\n",
            "b_73\t1\t\ti am angrier today that i was yesterday .\n",
            "b_73\t1\t\tjohn is taller than six feet .\n",
            "b_73\t1\t\tjohn is taller than bill .\n",
            "b_73\t1\t\tmary has more than two friends .\n",
            "b_73\t0\t*\tmary has more than just bill and pete friends .\n",
            "b_73\t1\t\tmary has more friends than two .\n",
            "b_73\t1\t\tmary has more friends than just bill and pete .\n",
            "b_73\t1\t\tthey may grow as much as six feet high .\n",
            "b_73\t0\t*\tthey may grow as much as bamboo high .\n",
            "b_73\t1\t\tthey may grow as high as six feet .\n",
            "b_73\t1\t\tsome of them made as many as 20 errors .\n",
            "b_73\t0\t*\tsome of them made as many as joan errors .\n",
            "b_73\t1\t\tsome of them made as many errors as joan .\n",
            "b_73\t0\t*\tjohn is taller than six feet is .\n",
            "b_73\t1\t\tjohn is taller than pete is .\n",
            "b_73\t0\t*\tmary has more friends that two .\n",
            "b_73\t0\t*\tmary has more friends than just bill and pete are .\n",
            "b_73\t0\t*\tjohn is more than five feet short .\n",
            "b_73\t1\t\tjohn is shorter than five feet .\n",
            "b_73\t1\t\tmary has more enemies than bill has friends .\n",
            "b_73\t0\t*\tmary has more than bill has friends enemies .\n",
            "b_73\t1\t\tmary does n't have as many too many too many as jane .\n",
            "b_73\t1\t\tjane has more nearly as many too many than mary .\n",
            "b_73\t1\t\tmary swam five more laps than joan swam .\n",
            "b_73\t1\t\tmary swam as many more laps than joan as linda .\n",
            "c_13\t1\t\tbill kissed himself .\n",
            "c_13\t0\t*\tbill kissed herself .\n",
            "c_13\t1\t\tsally kissed herself .\n",
            "c_13\t0\t*\tkiss himself .\n",
            "c_13\t1\t\tthe robot kissed itself .\n",
            "c_13\t1\t\tshe knocked herself on the head with a zucchini .\n",
            "c_13\t0\t*\tshe knocked himself on the head with a zucchini .\n",
            "c_13\t1\t\tthe snake flattened itself against the rock .\n",
            "c_13\t1\t\tthe joneses think themselves the best family on the block .\n",
            "c_13\t0\t*\tthe joneses think himself the most wealthy guy on the block .\n",
            "c_13\t1\t\tgary and kevin ran themselves into exhaustion .\n",
            "c_13\t0\t*\tgary and kevin ran himself into exhaustion .\n",
            "c_13\t1\t\tpeople from tucson think very highly of themselves .\n",
            "c_13\t1\t\ti gave myself the bucket of ice cream .\n",
            "c_13\t0\t*\tshe hit myself with a hammer .\n",
            "c_13\t1\t\tshe hit herself with a hammer .\n",
            "c_13\t1\t\tdoug blew the building up .\n",
            "c_13\t1\t\tdoug blew up the building .\n",
            "c_13\t1\t\tdoug blew it up .\n",
            "c_13\t1\t\tdoug blew up it .\n",
            "c_13\t0\t*\twho do you wonder what bought ?\n",
            "c_13\t1\t\ti wonder what fiona bought .\n",
            "c_13\t0\t*\ttoothbrush the is blue .\n",
            "c_13\t1\t\tcheese mice love stinks .\n",
            "c_13\t1\t\tthe dancing chorus line of elephants broke my television set .\n",
            "c_13\t1\t\trosie loves magazine ads .\n",
            "c_13\t1\t\ti think rosie loves magazine ads .\n",
            "c_13\t1\t\tdana doubts that drew believes i think rosie loves magazine ads .\n",
            "c_13\t1\t\tdave left .\n",
            "c_13\t1\t\tdave and alina left .\n",
            "c_13\t1\t\tdave , dan , erin , and alina left .\n",
            "c_13\t1\t\twho do you think that ciaran will question first ?\n",
            "c_13\t1\t\twho do you think ciaran will question first ?\n",
            "c_13\t1\t\twho do you think will question seamus first ?\n",
            "c_13\t0\t*\twho do you think that will question seamus first ?\n",
            "c_13\t1\t\ti expect soon to see the results .\n",
            "c_13\t1\t\ti expect to see the results soon .\n",
            "c_13\t1\t\ti expect to soon see the results .\n",
            "c_13\t0\t*\ti expect more than to double my profits .\n",
            "c_13\t0\t*\ti expect to double more than my profits .\n",
            "c_13\t0\t*\ti expect to double my profits more than .\n",
            "c_13\t1\t\ti expect to more than double my profits .\n",
            "c_13\t1\t\twho did you see in las vegas ?\n",
            "c_13\t1\t\tyou are taller than me .\n",
            "c_13\t0\t*\tmy red is refrigerator .\n",
            "c_13\t0\t*\twho do you think that saw bill ?\n",
            "c_13\t1\t\tmy friends wanted to quickly leave the party .\n",
            "c_13\t0\t*\tbunnies carrots eat .\n",
            "c_13\t1\t\tgeorge sang to himself .\n",
            "c_13\t0\t*\thimself sang to george .\n",
            "c_13\t1\t\tbetsy loves herself in blue leather .\n",
            "c_13\t1\t\teveryone should be able to defend himself .\n",
            "c_13\t1\t\teveryone should be able to defend herself .\n",
            "c_13\t1\t\ti hope nobody will hurt themselves .\n",
            "c_13\t1\t\ti hope nobody will hurt himself .\n",
            "c_13\t1\t\tdo n't hit yourself !\n",
            "c_13\t1\t\tshe is dancing .\n",
            "c_13\t1\t\tthey are dancing .\n",
            "c_13\t1\t\tthe man is dancing .\n",
            "c_13\t1\t\tthe men are dancing .\n",
            "c_13\t1\t\tthe students met to discuss the project .\n",
            "c_13\t1\t\tzeke cooked and ate the chili .\n",
            "c_13\t1\t\tzeke ate and cooked the chili .\n",
            "c_13\t1\t\the put the clothes .\n",
            "c_13\t1\t\the put in the washing machine .\n",
            "c_13\t1\t\ti gave my brother a birthday present .\n",
            "c_13\t1\t\ti gave a birthday present to my brother .\n",
            "c_13\t1\t\twhere do you guys live at ?\n",
            "c_13\t1\t\tit is obvious to everybody that tasha likes misha .\n",
            "c_13\t1\t\tthe man loved peanut butter cookies .\n",
            "c_13\t1\t\tthe puppy loved peanut butter cookies .\n",
            "c_13\t1\t\tthe king loved peanut butter cookies .\n",
            "c_13\t0\t*\tthe green loved peanut butter cookies .\n",
            "c_13\t0\t*\tthe in loved peanut butter cookies .\n",
            "c_13\t0\t*\tthe sing loved peanut butter cookies .\n",
            "c_13\t1\t\tjohn went to the store .\n",
            "c_13\t1\t\tthe man went to the store .\n",
            "c_13\t0\t*\tquickly walks went to the store .\n",
            "c_13\t0\t*\tto the washroom kissed the blarney stone .\n",
            "c_13\t1\t\tthe destruction of the city bothered the mongols .\n",
            "c_13\t1\t\tsincerity is an important quality .\n",
            "c_13\t1\t\tthe assassination of the president .\n",
            "c_13\t1\t\ttucson is a great place to live .\n",
            "c_13\t1\t\tgabrielle 's mother is an axe murderer .\n",
            "c_13\t1\t\thamsters mother attractive offspring .\n",
            "c_13\t1\t\twendy 's mother country is iceland .\n",
            "c_13\t1\t\tlouis said that parts of speech intrigued her .\n",
            "c_13\t0\t*\tcat ate the spider .\n",
            "c_13\t1\t\tthe cat ate the spider .\n",
            "c_13\t1\t\tcats ate the spider .\n",
            "c_13\t1\t\tthe cats ate the spider .\n",
            "c_13\t0\t*\ti ate apple .\n",
            "c_13\t1\t\ti ate the apple .\n",
            "c_13\t1\t\ti ate sugar .\n",
            "c_13\t1\t\ti ate the sugar .\n",
            "c_13\t1\t\the is filled with sincerity .\n",
            "c_13\t1\t\ti doubt his sincerity .\n",
            "c_13\t1\t\tthe dastardly surgeon stole the physician 's lunch .\n",
            "c_13\t1\t\ti asked the question .\n",
            "c_13\t1\t\ti asked if you knew the answer .\n",
            "c_13\t1\t\ti hit the ball .\n",
            "c_13\t1\t\ti spared him the trouble .\n",
            "c_13\t0\t*\ti put the box the book .\n",
            "c_13\t1\t\ti put the book in the box .\n",
            "c_13\t1\t\ti gave the box to leah .\n",
            "c_13\t1\t\ti gave leah the box .\n",
            "c_13\t1\t\ti told daniel the story .\n",
            "c_13\t1\t\ti told daniel that the exam was cancelled .\n",
            "c_13\t1\t\ti told the story to daniel .\n",
            "c_13\t1\t\tthe canadian government uses a parliamentary system of democracy .\n",
            "c_13\t1\t\tthe canadian bought himself a barbecue .\n",
            "c_13\t1\t\tthe prudish linguist did n't enjoy looking at the internet .\n",
            "c_13\t1\t\twe keep those censored copies of the book hidden to protect the sensibilities of the prudish .\n",
            "c_13\t1\t\tsusan bought some flowers for her mother .\n",
            "c_13\t1\t\tsusan bought some flowers for her birthday .\n",
            "c_13\t0\t*\tsusan bought her birthday some flowers .\n",
            "c_13\t1\t\ti gave blood .\n",
            "c_13\t1\t\ti do n't give a darn .\n",
            "c_13\t1\t\tandy gives freely of his time .\n",
            "c_13\t1\t\tdan gave his life .\n",
            "c_13\t1\t\tdan gives to charity .\n",
            "c_13\t1\t\tsorry , i gave last week .\n",
            "c_13\t1\t\tthe student loved his phonology readings .\n",
            "c_13\t1\t\ti saw these dancers and those musicians smoking something .\n",
            "c_13\t1\t\ti am drinking lemonade and eating a brownie .\n",
            "c_13\t1\t\twe went through the woods and over the bridge .\n",
            "c_13\t1\t\tthe man whose car i hit last week sued me .\n",
            "c_13\t1\t\tthe big man from ny has often said that he gave peanuts to elephants .\n",
            "c_13\t1\t\tthe man killed the king with the knife .\n",
            "c_13\t1\t\twe ate at a really fancy restaurant .\n",
            "c_13\t0\t*\twe ate at .\n",
            "c_13\t1\t\tbig bowls of beans are what i like .\n",
            "c_13\t1\t\tthe big boy was kissed by the drooling dog .\n",
            "c_13\t1\t\tthe drooling dog kissed the big boy .\n",
            "c_13\t1\t\tjohn and the man went to the store .\n",
            "c_13\t0\t*\tjohn and very blue went to the store .\n",
            "c_13\t1\t\tbruce loved and kelly hated phonology class .\n",
            "c_13\t0\t*\tthe with milk coffee is hot .\n",
            "c_13\t1\t\tthe kangaroo hopped over the truck .\n",
            "c_13\t1\t\ti have n't seen this sentence before .\n",
            "c_13\t1\t\tsusan will never sing at weddings .\n",
            "c_13\t1\t\tthe officer carefully inspected the license .\n",
            "c_13\t1\t\tevery cat always knows the location of her favorite catnip toy .\n",
            "c_13\t1\t\tthe cat put her catnip toy on the plastic mat .\n",
            "c_13\t1\t\tthe very young child walked from school to the store .\n",
            "c_13\t1\t\tjohn paid a dollar for a head of lettuce .\n",
            "c_13\t1\t\tteenagers drive rather quickly .\n",
            "c_13\t1\t\ta clever magician with the right equipment can fool the audience easily .\n",
            "c_13\t1\t\tthe police might plant the drugs in the apartment .\n",
            "c_13\t1\t\tthose olympic hopefuls should practice diligently daily .\n",
            "c_13\t1\t\tthe latest research on dieting always warns people about the dangers of too much cholesterol .\n",
            "c_13\t1\t\tthat annoying faucet was dripping constantly for months .\n",
            "c_13\t1\t\tmarian wonders if the package from boston will ever arrive .\n",
            "c_13\t1\t\ti said that bonny should do some dances from the middle east .\n",
            "c_13\t1\t\tthat dan smokes in the office really bothers alina .\n",
            "c_13\t1\t\tthe belief that syntactic theory reveals the inner structure of sentences emboldened the already much too cocky professor .\n",
            "c_13\t1\t\ti bought the parrot in the store .\n",
            "c_13\t1\t\ti put the milk in the fridge .\n",
            "c_13\t1\t\ti mailed the sweater to mary .\n",
            "c_13\t1\t\ti knew the man with the brown hair .\n",
            "c_13\t1\t\tjohn said mary went to the store quickly .\n",
            "c_13\t1\t\ti discovered an old english poem .\n",
            "c_13\t1\t\tsusanne gave the minivan to george .\n",
            "c_13\t1\t\tclyde got a passionate love letter from stacy .\n",
            "c_13\t1\t\the blew out the candle .\n",
            "c_13\t1\t\the turned off the light .\n",
            "c_13\t1\t\the blew up the building .\n",
            "c_13\t1\t\the rode out the storm .\n",
            "c_13\t0\t*\tshannon kissed quietly the kitten .\n",
            "c_13\t1\t\tshannon left quietly every day .\n",
            "c_13\t1\t\tjuliet says that romeo lies to his parents a lot .\n",
            "c_13\t1\t\tthe puppy licked the kitten 's face .\n",
            "c_13\t1\t\tit is raining .\n",
            "c_13\t1\t\tfred feels fine .\n",
            "c_13\t1\t\tthat bill 's breath smells of onions bothers erin .\n",
            "c_13\t1\t\tsusan kissed the clown 's nose .\n",
            "c_13\t1\t\tcedric danced a jolly jig .\n",
            "c_13\t1\t\tdale said that the lawn was overgrown .\n",
            "c_13\t1\t\tgilgamesh cut the steak with a knife .\n",
            "c_13\t1\t\twe drove all the way to buenos aires .\n",
            "c_13\t1\t\tjohn tagged lewis with a regulation baseball on tuesday .\n",
            "c_13\t1\t\tthe big man from new york loves bagels with cream cheese .\n",
            "c_13\t1\t\tsusan rode a bright blue train from new york .\n",
            "c_13\t1\t\tthe plucky platypus kicked a can of soup from new york to tucson .\n",
            "c_13\t1\t\tjohn said martha sang the aria with gusto .\n",
            "c_13\t1\t\tmartha said john sang the aria from la bohème .\n",
            "c_13\t1\t\tthe book of poems with the bright red cover stinks .\n",
            "c_13\t1\t\tlouis hinted mary stole the purse deftly .\n",
            "c_13\t1\t\tthe extremely tired students hated syntactic trees with a passion .\n",
            "c_13\t1\t\tmany soldiers have claimed bottled water satisfies thirst best .\n",
            "c_13\t1\t\tnetworking helps you grow your business .\n",
            "c_13\t1\t\ti did n't read a single book the whole time i was in the library .\n",
            "c_13\t1\t\ti did not have a red cent .\n",
            "c_13\t1\t\tfelicia wrote a fine paper on zapotec .\n",
            "c_13\t1\t\theidi hit herself on the head with a zucchini .\n",
            "c_13\t0\t*\therself hit heidi on the head with a zucchini .\n",
            "c_13\t1\t\theidi believes any description of herself .\n",
            "c_13\t1\t\tjohn knew that there would be a picture of himself hanging in the post .\n",
            "c_13\t1\t\talthough he loves marshmallows , john is not a big fan of chocolate .\n",
            "c_13\t1\t\this yearbook picture gives tom the creeps .\n",
            "c_13\t1\t\tmarilyn monroe is norma jeane baker .\n",
            "c_13\t1\t\tgene simmons was originally named haim goldberg .\n",
            "c_13\t1\t\tkevin ate spaghetti with a spoon and geordie did so too .\n",
            "c_13\t1\t\tthe chef eats beans and serves salads with forks .\n",
            "c_13\t1\t\ti am frightened of tigers .\n",
            "c_13\t1\t\ti am afraid of tigers .\n",
            "c_13\t1\t\ti am fond of circus performers .\n",
            "c_13\t1\t\ti fear tigers .\n",
            "c_13\t1\t\ti like circus performers .\n",
            "c_13\t1\t\ti am afraid of tigers and fond of clowns without exception .\n",
            "c_13\t1\t\ti am frightened of tigers and fond of clowns without exception .\n",
            "c_13\t1\t\tbob is very serious about mary , but less so than paul .\n",
            "c_13\t1\t\tthe book of poems with a red cover from blackwell by robert burns takes a very long time to read .\n",
            "c_13\t1\t\tthe book of poems with a red cover from blackwell by robert burns takes a very long time to read .\n",
            "c_13\t1\t\tthe book of poems from blackwell with a red cover by robert burns takes a very long time to read .\n",
            "c_13\t1\t\tthe book of poems from blackwell by robert burns with a red cover takes a very long time to read .\n",
            "c_13\t1\t\tthe book of poems by robert burns from blackwell with a red cover takes a very long time to read .\n",
            "c_13\t1\t\tthe book of poems by robert burns with a red cover from blackwell takes a very long time to read .\n",
            "c_13\t1\t\tthe book of poems with a red cover by robert burns from blackwell takes a very long time to read .\n",
            "c_13\t0\t*\tthe book with a red cover of poems from blackwell by robert burns takes a very long time to read .\n",
            "c_13\t0\t*\tthe book with a red cover from blackwell of poems by robert burns takes a very long time to read .\n",
            "c_13\t0\t*\tthe book with a red cover from blackwell by robert burns of poems takes a very long time to read .\n",
            "c_13\t1\t\tthe book of poems with a red cover and with a blue spine takes a very long time to read .\n",
            "c_13\t1\t\tthe book of poems and of fiction from blackwell takes a very long time to read .\n",
            "c_13\t0\t*\tthe one of poems with a red cover takes a very long time to read .\n",
            "c_13\t1\t\ti loved the policeman intensely with all my heart .\n",
            "c_13\t0\t*\ti loved intensely the policeman with all my heart .\n",
            "c_13\t0\t*\ti loved the policeman the baker intensely with all my heart .\n",
            "c_13\t1\t\tmika loved the policeman intensely and susan did so half heartedly .\n",
            "c_13\t0\t*\tsusan did so the baker .\n",
            "c_13\t1\t\tjohn fears dogs .\n",
            "c_13\t1\t\tjohn is afraid of dogs .\n",
            "c_13\t1\t\ttwo or three books take a very long time to read .\n",
            "c_13\t0\t*\ttwo or boring books take a very long time to read .\n",
            "c_13\t1\t\tthe red dress with the pink stripes looks good on sandy .\n",
            "c_13\t1\t\tthe ugly man from brazil found books of poems in the puddle .\n",
            "c_13\t1\t\terin never keeps her pencils in the correct drawer .\n",
            "c_13\t1\t\tdan walked to new mexico in the rain last year .\n",
            "c_13\t1\t\tgeorge wrote a volume of poems in latin for jane .\n",
            "c_13\t1\t\tpeople with boxes of old clothes lined up behind the door of the building with the leaky roof .\n",
            "c_13\t1\t\tthat automobile factories abound in michigan worries me greatly .\n",
            "c_13\t1\t\tno one understands that phrase structure rules explain the little understood phenomenon of the infinite length of sentences .\n",
            "c_13\t1\t\tmy favorite language is a language with simple morphology and complicated syntax .\n",
            "c_13\t1\t\tivan got a headache on wednesday from the disgruntled students of phonology from michigan .\n",
            "c_13\t1\t\tthe collection of syntax articles with the red cover bores students of syntax in tucson .\n",
            "c_13\t1\t\tthe red volume of obscene verse from italy shocked the puritan soul of the minister with the beard quite thoroughly yesterday .\n",
            "c_13\t1\t\tthe biggest man in the room said that john danced an irish jig from county kerry to county tipperary on thursday .\n",
            "c_13\t1\t\ta burlap sack of potatoes with mealy skins fell on the professor of linguistics with the terrible taste in t-shirts from the twelfth story .\n",
            "c_13\t1\t\tthe bright green filing cabinet was filled to the brim with the most boring articles from a prestigious journal of linguistics with a moderately large readership .\n",
            "c_13\t1\t\tthe coat of the panther is dark black .\n",
            "c_13\t1\t\tthe roof of the building is leaking .\n",
            "c_13\t1\t\tthe hat of the man standing over there impressed me greatly .\n",
            "c_13\t1\t\tthe panther 's coat is dark black .\n",
            "c_13\t1\t\tthe building 's roof is leaking .\n",
            "c_13\t1\t\tthe man standing over there 's hat impressed me greatly .\n",
            "c_13\t0\t*\tthe man 's standing over there hat impressed me greatly .\n",
            "c_13\t0\t*\tthe man standing over there 's the hat impressed me greatly .\n",
            "c_13\t1\t\tthe boy ran .\n",
            "c_13\t1\t\thoward is a linguistics student .\n",
            "c_13\t1\t\tpeter said that danny danced .\n",
            "c_13\t1\t\tbill wants susan to leave .\n",
            "c_13\t1\t\tpeter thinks that cathy loves him .\n",
            "c_13\t1\t\tpeople selling their stocks caused the crash of 1929 .\n",
            "c_13\t1\t\tfor mary to love that boor is a travesty .\n",
            "c_13\t1\t\ti said that mary signed my yearbook .\n",
            "c_13\t1\t\ti want mary to sign my yearbook .\n",
            "c_13\t1\t\ti 've never seen you eat asparagus .\n",
            "c_13\t1\t\ti know you ate asparagus .\n",
            "c_13\t0\t*\ti 've never seen you ate asparagus .\n",
            "c_13\t0\t*\ti 've never seen him eats asparagus .\n",
            "c_13\t1\t\ti 've never seen him eat asparagus .\n",
            "c_13\t1\t\ti think that he eats asparagus .\n",
            "c_13\t1\t\ti want to eat asparagus .\n",
            "c_13\t1\t\ti want him to eat asparagus .\n",
            "c_13\t1\t\ti wonder if he eats asparagus .\n",
            "c_13\t1\t\tfor him to eat asparagus is a travesty .\n",
            "c_13\t1\t\ti asked for him to eat the asparagus .\n",
            "c_13\t1\t\ti think he will eat asparagus .\n",
            "c_13\t1\t\tfabio asked if claus had run a marathon .\n",
            "c_13\t0\t*\tfabio asked if had claus run a marathon .\n",
            "c_13\t0\t*\tfabio asked had if claus run a marathon .\n",
            "c_13\t1\t\tyou can lead a horse to water but will it drink ?\n",
            "c_13\t1\t\the will go .\n",
            "c_13\t1\t\the goes .\n",
            "c_13\t1\t\tthe peanut butter has got moldy .\n",
            "c_13\t1\t\tthe swing blasted the golf ball across the green .\n",
            "c_13\t1\t\tthat harry loves dancing is evidenced by his shiny tap shoes .\n",
            "c_13\t1\t\tthe brazilians pumped the oil across the river .\n",
            "c_13\t1\t\tlenin believes the tsar to be a power hungry dictator .\n",
            "c_13\t1\t\tbrezhnev had said for andropov to leave .\n",
            "c_13\t1\t\tyeltsin saw stalin holding the bag .\n",
            "c_13\t1\t\trobert thinks that students should eat asparagus .\n",
            "c_13\t1\t\trobert thinks that student should eat asparagus .\n",
            "c_13\t1\t\tlinguistics students like phonetics tutorials .\n",
            "c_13\t1\t\tmartha said that bill loved his cheerios in the morning .\n",
            "c_13\t1\t\teloise wants you to study a new language . assume to = t .\n",
            "c_13\t1\t\tfor maurice to quarrel with joel frightened maggie .\n",
            "c_13\t1\t\tno man has ever beaten the centaur .\n",
            "c_13\t0\t*\tsome man has ever beaten the centaur .\n",
            "c_13\t0\t*\tevery man has ever beaten the centaur .\n",
            "c_13\t1\t\trosemary hates new york .\n",
            "c_13\t0\t*\trosemary hates .\n",
            "c_13\t1\t\tjennie smiled .\n",
            "c_13\t0\t*\tjennie smiled the microwave .\n",
            "c_13\t1\t\ttraci gave the whale a lollipop .\n",
            "c_13\t0\t*\ttraci gave the whale .\n",
            "c_13\t0\t*\ttraci gave a lollipop .\n",
            "c_13\t1\t\tryan hit andrew .\n",
            "c_13\t1\t\tmichael accidentally broke the glass .\n",
            "c_13\t1\t\tleah likes cookies .\n",
            "c_13\t1\t\tlorenzo saw the eclipse .\n",
            "c_13\t1\t\tsyntax frightens kenny .\n",
            "c_13\t1\t\talyssa kept her syntax book .\n",
            "c_13\t1\t\tthe arrow hit ben .\n",
            "c_13\t1\t\tthe psychologist hates phonology .\n",
            "c_13\t1\t\tdoug went to chicago .\n",
            "c_13\t1\t\tdave was given the margarita mix .\n",
            "c_13\t1\t\tgeorge gave jessica the book .\n",
            "c_13\t1\t\tdaniel received a scolding from hanna .\n",
            "c_13\t1\t\tbob gave steve the syntax assignment .\n",
            "c_13\t1\t\tstacy came directly from linguistics class .\n",
            "c_13\t1\t\tandrew is in tucson 's finest apartment .\n",
            "c_13\t1\t\tchris hacked the computer apart with an axe .\n",
            "c_13\t1\t\tthis key will open the door to the linguistics building .\n",
            "c_13\t1\t\the bought these flowers for aaron .\n",
            "c_13\t1\t\tshe cooked matt dinner .\n",
            "c_13\t0\t*\tjohn placed the flute .\n",
            "c_13\t1\t\tjohn put the book on the table .\n",
            "c_13\t1\t\tjohn put the book on the table with a pair of tongs .\n",
            "c_13\t1\t\tmegan loves kevin .\n",
            "c_13\t0\t*\tmegan loves .\n",
            "c_13\t0\t*\tmegan loves jason .\n",
            "c_13\t1\t\tit rained .\n",
            "c_13\t1\t\tit snowed .\n",
            "c_13\t1\t\tit hailed .\n",
            "c_13\t1\t\tthat bill loves chocolate is likely .\n",
            "c_13\t1\t\tit is likely that bill likes chocolate .\n",
            "c_13\t1\t\ti put a book on it .\n",
            "c_13\t1\t\tit bit me on the leg .\n",
            "c_13\t1\t\tshannon sent dan an email .\n",
            "c_13\t1\t\tstacy hit a baseball to julia .\n",
            "c_13\t1\t\tjaime danced a jig .\n",
            "c_13\t1\t\tyuko rubbed the pizza with a garlic clove .\n",
            "c_13\t1\t\tit is raining in san francisco .\n",
            "c_13\t1\t\tthe stodgy professor left with his teaching assistant .\n",
            "c_13\t1\t\ti played a tune on my ipod .\n",
            "c_13\t1\t\tmolly gave calvin a kiss .\n",
            "c_13\t1\t\tmercedes gave a test to the students in the lecture hall .\n",
            "c_13\t1\t\tspot ate a cat treat .\n",
            "c_13\t1\t\tsusan ate yesterday at the restaurant .\n",
            "c_13\t1\t\tgwen looked at a fire truck .\n",
            "c_13\t1\t\tmichael asked a question .\n",
            "c_13\t1\t\tadam asked if hyacinth likes pineapples .\n",
            "c_13\t1\t\ti feel it is unfortunate that television is so vulgar these days .\n",
            "c_13\t1\t\tthat angus hates sushi is mysterious .\n",
            "c_13\t0\t*\tjennie smiled the sandwich .\n",
            "c_13\t0\t*\tplaced the flute on the table .\n",
            "c_13\t0\t*\tjohn placed on the table .\n",
            "c_13\t0\t*\tjohn placed the flute the violin on the table .\n",
            "c_13\t0\t*\tthe rock placed the sky with the fork .\n",
            "c_13\t0\t*\tjohn placed the flute the table .\n",
            "c_13\t1\t\tjohn bit the apple .\n",
            "c_13\t1\t\tsusan forgave louis .\n",
            "c_13\t1\t\tthe jockey rides the horse .\n",
            "c_13\t1\t\tphillip gave the soldier the medal .\n",
            "c_13\t1\t\tthe apple was bitten .\n",
            "c_13\t1\t\tlouis was forgiven .\n",
            "c_13\t1\t\tthe horse was ridden .\n",
            "c_13\t1\t\tthe medal was given to the soldier .\n",
            "c_13\t1\t\tthe soldier was given the medal .\n",
            "c_13\t1\t\tthe apple was bitten by john .\n",
            "c_13\t1\t\tlouis was forgiven by susan .\n",
            "c_13\t1\t\tthe horse was ridden by the jockey .\n",
            "c_13\t1\t\tthe medal was given to the soldier by phillip .\n",
            "c_13\t1\t\tthe soldier was given the medal by phillip .\n",
            "c_13\t1\t\ti ate a basket of apples .\n",
            "c_13\t1\t\ti ate .\n",
            "c_13\t1\t\ti think that john likes his beer .\n",
            "c_13\t1\t\ti think john likes his beer .\n",
            "c_13\t0\t*\ti think for john to like his beer .\n",
            "c_13\t0\t*\ti think if john likes his beer .\n",
            "c_13\t1\t\ti ordered that john drink his beer .\n",
            "c_13\t1\t\ti ordered john drink his beer .\n",
            "c_13\t0\t*\ti ordered for john to drink his beer .\n",
            "c_13\t1\t\ti ordered john to drink his beer .\n",
            "c_13\t0\t*\ti ordered if john drink his beer .\n",
            "c_13\t0\t*\ti inquired that john like his beer .\n",
            "c_13\t0\t*\ti inquired john likes his beer .\n",
            "c_13\t0\t*\ti inquired for john to like his beer .\n",
            "c_13\t0\t*\ti inquired john to like his beer .\n",
            "c_13\t1\t\ti inquired if john likes his beer .\n",
            "c_13\t1\t\theidi thinks that andy is eating salmon flavored candy bars .\n",
            "c_13\t1\t\theidi thinks that andy has eaten salmon flavored candy bars .\n",
            "c_13\t1\t\theidi thinks that andy will eat salmon flavored candy bars .\n",
            "c_13\t1\t\theidi thinks that andy eats salmon flavored candy bars .\n",
            "c_13\t1\t\theidi thinks that the salmon flavored candy bars were eaten .\n",
            "c_13\t1\t\the has danced .\n",
            "c_13\t1\t\ti had eaten the deep fried muffins .\n",
            "c_13\t1\t\ti have eaten the beef waffles .\n",
            "c_13\t1\t\ti will have eaten the beef waffles .\n",
            "c_13\t1\t\tjeff was dancing with sylvia while amy sat angrily at their table .\n",
            "c_13\t1\t\tthe soup had been being eaten when it got spilled .\n",
            "c_13\t1\t\tjeff must have eaten the deep fried muffin .\n",
            "c_13\t0\t*\tjeff has must eaten the deep fried muffin .\n",
            "c_13\t1\t\tjeff must not have eaten the deep fried muffin .\n",
            "c_13\t0\t*\tjeff not must have eaten the deep fried muffin .\n",
            "c_13\t1\t\tcalvin has a peanut .\n",
            "c_13\t1\t\tsusan has a cold .\n",
            "c_13\t1\t\tbill had an accident .\n",
            "c_13\t1\t\tcalvin has eaten a peanut .\n",
            "c_13\t1\t\tfrank has drunk too much .\n",
            "c_13\t1\t\tbill has been dancing .\n",
            "c_13\t1\t\tdane is a doctor .\n",
            "c_13\t1\t\tjorge was the one .\n",
            "c_13\t1\t\talex was eating the popsicle .\n",
            "c_13\t1\t\tmegan was sat on by her brother .\n",
            "c_13\t1\t\tcatherine did her homework .\n",
            "c_13\t1\t\tcatherine did not eat .\n",
            "c_13\t1\t\tcalvin did not do a back flip .\n",
            "c_13\t1\t\thas bill eaten his tuna ?\n",
            "c_13\t1\t\tis bill eating his tuna ?\n",
            "c_13\t1\t\tdid bill eat his dinner ?\n",
            "c_13\t0\t*\tate bill his dinner ?\n",
            "c_13\t0\t*\thas calvin a bowl ?\n",
            "c_13\t1\t\tangus is not leaving .\n",
            "c_13\t1\t\tcalvin has not eaten his dinner .\n",
            "c_13\t1\t\tspot did not play with his mouse .\n",
            "c_13\t0\t*\tcalvin ate not his dinner .\n",
            "c_13\t0\t*\tcalvin has not any catnip .\n",
            "c_13\t0\t*\tangus did not his homework .\n",
            "c_13\t1\t\ti should not eat plums .\n",
            "c_13\t0\t*\ti have not should eat plums .\n",
            "c_13\t0\t*\ti must can eat plums .\n",
            "c_13\t0\t*\ti have should eat plums .\n",
            "c_13\t0\t*\ti want to should eat plums .\n",
            "c_13\t1\t\tcalvin will not eat the beef waffles .\n",
            "c_13\t0\t*\tcalvin not will eat the beef waffles .\n",
            "c_13\t0\t*\tcalvin could will eat the beef waffles .\n",
            "c_13\t0\t*\tcalvin will could eat the beef waffles .\n",
            "c_13\t1\t\ti ate deep fried muffins .\n",
            "c_13\t1\t\the always eats deep fried muffins .\n",
            "c_13\t0\t*\ti might ate deep fried muffins .\n",
            "c_13\t0\t*\the always might eats deep fried muffins .\n",
            "c_13\t1\t\the might eat deep fried muffins .\n",
            "c_13\t1\t\ti might eat deep fried muffins .\n",
            "c_13\t1\t\the will eat deep fried muffins .\n",
            "c_13\t0\t*\the will eats deep fried muffins .\n",
            "c_13\t1\t\tsylvia will be slapping jeff upside the head in martial arts class .\n",
            "c_13\t1\t\tsylvia could be slapping jeff upside the head in martial arts class .\n",
            "c_13\t1\t\tsylvia is slapping jeff upside the head in martial arts class .\n",
            "c_13\t1\t\tthe cat had eaten .\n",
            "c_13\t1\t\tthe cat had been eating .\n",
            "c_13\t1\t\tthe tuna had been eaten .\n",
            "c_13\t0\t*\tthe cat had haven eaten .\n",
            "c_13\t1\t\tthe cat was leaving .\n",
            "c_13\t1\t\tthe tuna was being eaten .\n",
            "c_13\t0\t*\tthe cat was being eating .\n",
            "c_13\t0\t*\tthe cat was having eaten .\n",
            "c_13\t1\t\tthe cake was eaten .\n",
            "c_13\t0\t*\tthe cake was been eating .\n",
            "c_13\t0\t*\tthe cake was have eaten .\n",
            "c_13\t1\t\treggie did not chase the ball .\n",
            "c_13\t1\t\tdid calvin eat the beef waffles ?\n",
            "c_13\t1\t\twhat did calvin eat ?\n",
            "c_13\t0\t*\tjohn must not do have eaten .\n",
            "c_13\t0\t*\tjohn must do not have eaten .\n",
            "c_13\t1\t\tthe prisoner must have been being interrogated when the supervisor walked into the room and saw what was going on and put a stop to it .\n",
            "c_13\t1\t\tfiona must not eat the sauteed candy canes .\n",
            "c_13\t1\t\tfiona has not eaten the sauteed candy canes .\n",
            "c_13\t1\t\tcan fiona eat sauteed candy canes ?\n",
            "c_13\t0\t*\ti wanted that he should leave .\n",
            "c_13\t0\t*\ti wanted he should leave .\n",
            "c_13\t0\t*\ti wanted if he should leave .\n",
            "c_13\t1\t\ti wanted him to leave .\n",
            "c_13\t1\t\ti wanted to leave .\n",
            "c_13\t0\t*\theidi investigated that john ate the cauliflower .\n",
            "c_13\t0\t*\theidi investigated john ate the cauliflower .\n",
            "c_13\t1\t\theidi investigated whether john ate the cauliflower .\n",
            "c_13\t1\t\theidi investigated if john ate the cauliflower .\n",
            "c_13\t0\t*\theidi investigated john to eat the cauliflower .\n",
            "c_13\t0\t*\theidi investigated to eat the cauliflower .\n",
            "c_13\t1\t\tjohn said heidi was obsessed with broccoli .\n",
            "c_13\t0\t*\tjohn said if heidi was obsessed with broccoli .\n",
            "c_13\t0\t*\tjohn said heidi to eat the broccoli .\n",
            "c_13\t1\t\tandy promised that we would go .\n",
            "c_13\t1\t\tandy promised we would go .\n",
            "c_13\t0\t*\tandy promised if we would go .\n",
            "c_13\t0\t*\tandy promised us to go .\n",
            "c_13\t1\t\tandy promised to go .\n",
            "c_13\t1\t\tif i were a rich man , i 'd buy a diamond ring .\n",
            "c_13\t0\t*\tif he is a rich man , he 'd buy a diamond ring .\n",
            "c_13\t1\t\trory eats .\n",
            "c_13\t1\t\trory ate muffins .\n",
            "c_13\t1\t\tthe muffins were eaten .\n",
            "c_13\t1\t\trory had eaten the muffins .\n",
            "c_13\t1\t\trory has eaten the muffins .\n",
            "c_13\t1\t\trory must have eaten the muffins .\n",
            "c_13\t1\t\trory may be eating the muffins .\n",
            "c_13\t1\t\trory will eat the muffins .\n",
            "c_13\t1\t\trory eats muffins .\n",
            "c_13\t1\t\trory is eating muffins .\n",
            "c_13\t1\t\trory might have been eating the muffins .\n",
            "c_13\t1\t\tthe muffins might have been being eaten .\n",
            "c_13\t1\t\tthe tuna had been being eaten .\n",
            "c_13\t1\t\tcalvin will eat .\n",
            "c_13\t1\t\tthe tuna will be eaten .\n",
            "c_13\t1\t\tcalvin will be eating .\n",
            "c_13\t1\t\tcalvin will have eaten .\n",
            "c_13\t1\t\tthe tuna will be being eaten .\n",
            "c_13\t1\t\tthe tuna will have been eaten .\n",
            "c_13\t1\t\tcalvin will have been eating .\n",
            "c_13\t1\t\tcalvin was eating .\n",
            "c_13\t1\t\tcalvin had eaten .\n",
            "c_13\t1\t\tcalvin had been eating .\n",
            "c_13\t1\t\tthe tuna must have been eaten .\n",
            "c_13\t1\t\tthe tuna will have been being eaten .\n",
            "c_13\t1\t\the has not eaten yet today .\n",
            "c_13\t1\t\ti have never seen this movie .\n",
            "c_13\t1\t\ti never have a pen when i need it .\n",
            "c_13\t1\t\ti have always loved peanut butter .\n",
            "c_13\t1\t\ti do not love peanut butter .\n",
            "c_13\t1\t\tmartha often thinks kim hates phonology .\n",
            "c_13\t1\t\tdo you like peanut butter ?\n",
            "c_13\t1\t\thave you always hated peanut butter ?\n",
            "c_13\t1\t\tare you always thinking dirty thoughts ?\n",
            "c_13\t1\t\tbradley left .\n",
            "c_13\t1\t\tstacy left tucson .\n",
            "c_13\t1\t\tjohn left his wife .\n",
            "c_13\t0\t*\ti want bradley that left .\n",
            "c_13\t0\t*\tjohn thinks that left .\n",
            "c_13\t1\t\tthat john will leave is likely .\n",
            "c_13\t1\t\tit is likely that john will leave .\n",
            "c_13\t1\t\tthe policeman kissed the puppy .\n",
            "c_13\t1\t\tthe puppy was kissed by the policeman .\n",
            "c_13\t1\t\tthe puppy was kissed .\n",
            "c_13\t1\t\tjohn laughed .\n",
            "c_13\t1\t\tthe audience laughed .\n",
            "c_13\t0\t*\tbill is likely john to hit .\n",
            "c_13\t0\t*\tit was kissed the puppy .\n",
            "c_13\t1\t\tjennifer swatted steve .\n",
            "c_13\t1\t\tsteve swatted jennifer .\n",
            "c_13\t1\t\tshe swatted him .\n",
            "c_13\t1\t\the swatted her .\n",
            "c_13\t1\t\ti walk .\n",
            "c_13\t1\t\tyou walk .\n",
            "c_13\t1\t\tit is likely that patrick left .\n",
            "c_13\t1\t\tthat patrick left is likely .\n",
            "c_13\t0\t*\tpatrick is likely that left .\n",
            "c_13\t0\t*\tit is likely patrick to leave .\n",
            "c_13\t0\t*\tpatrick to leave is likely .\n",
            "c_13\t1\t\tpatrick is likely to leave .\n",
            "c_13\t1\t\the kissed her .\n",
            "c_13\t1\t\tshe was kissed .\n",
            "c_13\t0\t*\tshe was kissed him .\n",
            "c_13\t0\t*\tit was kissed her .\n",
            "c_13\t1\t\tstacy danced at the palace .\n",
            "c_13\t1\t\tstacy arrived at the palace .\n",
            "c_13\t0\t*\tthere danced three men at the palace .\n",
            "c_13\t0\t*\tthere arrived three men at the palace .\n",
            "c_13\t0\t*\tit seems sonny to love cher .\n",
            "c_13\t0\t*\tbill was bitten the dog .\n",
            "c_13\t0\t*\tdonny is likely that left .\n",
            "c_13\t1\t\tthe shah slept in a bed .\n",
            "c_13\t1\t\tthe bed was slept in by the shah .\n",
            "c_13\t1\t\tdust fell on the bed .\n",
            "c_13\t0\t*\tthe bed was fallen on by the dust .\n",
            "c_13\t1\t\tbill was hit by the baseball .\n",
            "c_13\t0\t*\twas been hit by bill by the baseball .\n",
            "c_13\t1\t\tbill gave sue the book .\n",
            "c_13\t1\t\tsue was given the book by bill .\n",
            "c_13\t0\t*\tthe book was been given by bill by sue .\n",
            "c_13\t1\t\ti cut the soft bread .\n",
            "c_13\t1\t\tthe soft bread cuts easily .\n",
            "c_13\t1\t\tthe boat sank .\n",
            "c_13\t1\t\tthe torpedo sank the boat .\n",
            "c_13\t1\t\tthe captain sank the boat .\n",
            "c_13\t1\t\tthe captain sank the boat with a torpedo .\n",
            "c_13\t0\t*\twas sunk by the boat .\n",
            "c_13\t1\t\tthe boat was sunk by the captain with a torpedo .\n",
            "c_13\t1\t\ti sent a book to louis .\n",
            "c_13\t1\t\ti sent louis a book .\n",
            "c_13\t1\t\ta book was sent to louis .\n",
            "c_13\t0\t*\tlouis was sent a book to .\n",
            "c_13\t0\t*\tto louis was sent a book .\n",
            "c_13\t1\t\tlouis was sent a book .\n",
            "c_13\t0\t*\ta book was sent louis .\n",
            "c_13\t1\t\tjohn seems to have left .\n",
            "c_13\t1\t\tbill wants john to leave .\n",
            "c_13\t1\t\tjohn wants bill to leave .\n",
            "c_13\t1\t\tjohn wants him to leave .\n",
            "c_13\t1\t\tjohn believes him to have been at the game .\n",
            "c_13\t1\t\the is believed by john to have been at the game .\n",
            "c_13\t1\t\the is believed to have been at the game .\n",
            "c_13\t1\t\tbecky bought the syntax book .\n",
            "c_13\t1\t\twhat did becky buy ?\n",
            "c_13\t1\t\twhat did stacy say becky bought ?\n",
            "c_13\t1\t\tmatt kissed her .\n",
            "c_13\t1\t\twhom did matt kiss ?\n",
            "c_13\t1\t\ti wonder who jim kissed .\n",
            "c_13\t1\t\tthe fact that i like strawberry flavored milk shakes is none of your business .\n",
            "c_13\t1\t\tshe made the outrageous claim that tuna flavored milkshakes are good for you .\n",
            "c_13\t1\t\ti asked where you found it .\n",
            "c_13\t1\t\ti wo n't reveal the place .\n",
            "c_13\t1\t\ti asked who she kissed .\n",
            "c_13\t1\t\ti know several people who she kissed .\n",
            "c_13\t1\t\ti know several people she kissed .\n",
            "c_13\t1\t\ti know several people that she kissed .\n",
            "c_13\t1\t\ti know i bought the book you recommended .\n",
            "c_13\t1\t\ti know i bought the book that you recommended .\n",
            "c_13\t1\t\tthe guy who is wearing the red hat just hit me !\n",
            "c_13\t1\t\tthat guy , who i think might be drunk , just hit me !\n",
            "c_13\t0\t*\tthe man , who i think might be drunk , that is escaping hit me .\n",
            "c_13\t1\t\twhat did bill claim that he read ?\n",
            "c_13\t1\t\twhat do you think matt kissed ?\n",
            "c_13\t0\t*\twhat did bill make the claim that he read in the syntax book ?\n",
            "c_13\t0\t*\twhich cake did you see the man who baked ?\n",
            "c_13\t1\t\ti wonder what john bought .\n",
            "c_13\t1\t\thow do you think john bought the sweater ?\n",
            "c_13\t0\t*\thow do you wonder what john bought ?\n",
            "c_13\t1\t\thow do you think john bought what ?\n",
            "c_13\t1\t\ti wonder what john bought how .\n",
            "c_13\t1\t\ti wonder what john kissed .\n",
            "c_13\t0\t*\twho did you wonder what kissed ?\n",
            "c_13\t1\t\ti asked what john kissed .\n",
            "c_13\t1\t\tthat the police would arrest several rioters was a certainty .\n",
            "c_13\t1\t\ti liked mary and john .\n",
            "c_13\t0\t*\twho did you like and john ?\n",
            "c_13\t1\t\ti ate some popcorn and drank some soda .\n",
            "c_13\t0\t*\twhat did you eat some popcorn and drink ?\n",
            "c_13\t1\t\twho loves who ?\n",
            "c_13\t1\t\twho loves whom ?\n",
            "c_13\t1\t\tshelly loves who ?\n",
            "c_13\t1\t\tfred saw a spaceship in the linguistics lounge ?\n",
            "c_13\t1\t\twhat is bothering you ?\n",
            "c_13\t1\t\twho has seen my snorkel ?\n",
            "c_13\t1\t\thow was the plot discovered by the authorities ?\n",
            "c_13\t1\t\twhich animals appear to have lost their collars ?\n",
            "c_13\t1\t\twhat did jean think was likely to have been stolen ?\n",
            "c_13\t1\t\tcar sales have surprised the stockbrokers .\n",
            "c_13\t1\t\tcan you find the light bulb store ?\n",
            "c_13\t1\t\tjohn was bitten by an advertising executive .\n",
            "c_13\t1\t\tit is likely that tami will leave new york .\n",
            "c_13\t1\t\ttami is likely to leave new york .\n",
            "c_13\t1\t\tlucy seems to have been mugged .\n",
            "c_13\t1\t\twhat did you buy at the supermarket ?\n",
            "c_13\t1\t\twhat is it likely for beth to have bought at the supermarket ?\n",
            "c_13\t1\t\twhat is likely to have been bought at the supermarket ?\n",
            "c_13\t1\t\tthe trail we walked today was built by slave labor .\n",
            "c_13\t1\t\tbill is always complaining about the guys who work near him .\n",
            "c_13\t1\t\tthe cost of bagels that are imported from iceland surprised the teacher who mike hired last week .\n",
            "c_13\t0\t*\tjosh gave clay carefully a book .\n",
            "c_13\t1\t\tjosh gave clay a book carefully .\n",
            "c_13\t1\t\tbriana showed justin himself .\n",
            "c_13\t0\t*\tbriana showed himself justin .\n",
            "c_13\t1\t\ti blew up the building .\n",
            "c_13\t1\t\ti blew the building up .\n",
            "c_13\t0\t*\ti blew up it .\n",
            "c_13\t1\t\ti blew it up .\n",
            "c_13\t1\t\tsusan sent the package to heidi .\n",
            "c_13\t1\t\ti asked mike if he had seen the yeti .\n",
            "c_13\t1\t\ti bought some flowers for manuel .\n",
            "c_13\t1\t\ti bought manuel some flowers .\n",
            "c_13\t1\t\tjean is likely to leave .\n",
            "c_13\t1\t\tjean is reluctant to leave .\n",
            "c_13\t0\t*\tjean is likely .\n",
            "c_13\t1\t\tjean wants brian to leave .\n",
            "c_13\t1\t\tjean persuaded brian to leave .\n",
            "c_13\t1\t\tthat jean left is likely .\n",
            "c_13\t1\t\tit is likely that jean left .\n",
            "c_13\t0\t*\tis likely jean to leave .\n",
            "c_13\t0\t*\tit is reluctant that jean left .\n",
            "c_13\t0\t*\tthat jean left is reluctant .\n",
            "c_13\t1\t\tjean is likely to dance .\n",
            "c_13\t1\t\tthe cat is out of the bag .\n",
            "c_13\t1\t\tthe cat thinks that he is out of the bag .\n",
            "c_13\t0\t*\tis likely to jean dance .\n",
            "c_13\t1\t\tit is likely that jean will dance .\n",
            "c_13\t1\t\tjean wants robert .\n",
            "c_13\t1\t\tjean wants him .\n",
            "c_13\t0\t*\ti want she to dance .\n",
            "c_13\t1\t\ti want jean .\n",
            "c_13\t1\t\ti want jean to dance .\n",
            "c_13\t1\t\tjean wants herself to dance .\n",
            "c_13\t1\t\tjean is reluctant .\n",
            "c_13\t1\t\tto find a new mate , go to a dating service .\n",
            "c_13\t1\t\tjean tried to behave .\n",
            "c_13\t1\t\trobert knows that it is essential to be well behaved .\n",
            "c_13\t1\t\trobert knows that it is essential .\n",
            "c_13\t1\t\trobert knows it is essential that he is well behaved .\n",
            "c_13\t1\t\tlouis begged kate to leave .\n",
            "c_13\t0\t*\tlouis begged kate that she leave her job .\n",
            "c_13\t0\t*\tlouis begged kate to shave himself .\n",
            "c_13\t1\t\tlouis begged kate that he be allowed to shave himself .\n",
            "c_13\t1\t\tto behave oneself in public is expected .\n",
            "c_13\t1\t\trobert knew that it was necessary to behave himself .\n",
            "c_13\t1\t\tmike expected greg incorrectly to take out the trash .\n",
            "c_13\t1\t\tthe boys do n't all want to leave .\n",
            "c_13\t1\t\trobert is eager to do his homework .\n",
            "c_13\t1\t\tjean seems to be in a good mood .\n",
            "c_13\t1\t\trosemary tried to get a new car .\n",
            "c_13\t1\t\tsusan begged bill to let her sing in the concert .\n",
            "c_13\t1\t\tsusan begged to be allowed to sing in the concert .\n",
            "c_13\t1\t\tchristina is ready to leave .\n",
            "c_13\t1\t\tfred was believed to have wanted to try to dance .\n",
            "c_13\t1\t\tsusan consented to try to seem to have been kissed .\n",
            "c_13\t1\t\talan told me who wanted to seem to be invincible .\n",
            "c_13\t1\t\twhat did john want to eat ?\n",
            "c_13\t1\t\tthis book is easy to read .\n",
            "c_13\t1\t\tjohn is easy to please .\n",
            "c_13\t1\t\tto improve myself is a goal for next year .\n",
            "c_13\t1\t\tto improve yourself would be a good idea .\n",
            "c_13\t1\t\tto improve himself , bruce should consider therapy .\n",
            "c_13\t1\t\tto improve herself , jane went to a health spa .\n",
            "c_13\t1\t\tkathleen really hates her job .\n",
            "c_13\t1\t\tmy brother likes collecting jazz records .\n",
            "c_13\t1\t\tmartina is deathly afraid of spiders .\n",
            "c_13\t1\t\tthat kind of behavior annoys me .\n",
            "c_13\t1\t\tthe news pleased the students .\n",
            "c_13\t1\t\thorror films disturb milo .\n",
            "c_13\t1\t\tthe exhibition really impressed the critics .\n",
            "c_13\t1\t\tkathleen hates those pictures of herself .\n",
            "c_13\t1\t\tthe children admired photos of each other .\n",
            "c_13\t1\t\tsandra hates reading about herself in the tabloids .\n",
            "c_13\t1\t\tpictures of himself always disturb milo .\n",
            "c_13\t1\t\tto be able to buy myself a ticket to france would be a dream .\n",
            "c_13\t1\t\treading about herself in the tabloids always annoys sandra .\n",
            "c_13\t1\t\tbrandon has been reading more novels than he has short stories .\n",
            "c_13\t1\t\trobin will eat cabbage but she wo n't ice cream .\n",
            "c_13\t1\t\tjohn could bake something , but i 'm not sure what .\n",
            "c_13\t1\t\tfrank will eat an apple and morgan will too .\n",
            "c_13\t1\t\tfrank will eat an apple and morgan will eat an apple too .\n",
            "c_13\t1\t\tcalvin will strike himself .\n",
            "c_13\t1\t\tcalvin will strike himself and otto will too .\n",
            "c_13\t1\t\tcalvin will strike himself and otto will strike himself too .\n",
            "c_13\t1\t\tcalvin has dated every girl who jeff has .\n",
            "c_13\t1\t\tcalvin has dated every girl who jeff has dated .\n",
            "c_13\t1\t\ti know which guys you 've dated , but i do n't know which guys you have n't .\n",
            "c_13\t0\t*\twhich language do you want to hire someone who speaks ?\n",
            "c_13\t0\t*\tthey want to hire someone who speaks a balkan language , but i do n't know which language .\n",
            "c_13\t1\t\tcalvin will fire someone today , but i do n't know who .\n",
            "c_13\t1\t\tpeter was talking with someone but i do n't know who .\n",
            "c_13\t1\t\tbrandon read every book that megan did .\n",
            "c_13\t1\t\tevery book that megan did brandon read too .\n",
            "c_13\t1\t\tdarin has eaten more squid than john has octopus .\n",
            "c_13\t1\t\twhat does calvin like .\n",
            "c_13\t1\t\talexandra wants to catch a fish and sylvia does too .\n",
            "c_13\t1\t\tcalvin admired himself in the mirror .\n",
            "c_13\t0\t*\tchris said that himself was sad .\n",
            "c_13\t1\t\tchris wants himself to win .\n",
            "c_13\t1\t\twhich pictures of himself did chris see in the gallery ?\n",
            "c_13\t1\t\tchris liked which pictures of himself ?\n",
            "c_13\t1\t\twhich pictures of himself did chris like ?\n",
            "c_13\t0\t*\theidi believes bill 's description of herself .\n",
            "c_13\t1\t\theidi thinks that she has won .\n",
            "c_13\t1\t\theidi thinks that pictures of herself are beautiful .\n",
            "c_13\t1\t\theidi gave a present to herself .\n",
            "c_13\t1\t\tthe army 's destruction of the palace was a tragedy .\n",
            "c_13\t1\t\tthe army destroyed the palace .\n",
            "c_13\t1\t\theidi wants to kiss herself .\n",
            "c_13\t0\t*\theidi believes john 's description of herself .\n",
            "c_13\t0\t*\theidi dislikes the tv 's depiction of herself .\n",
            "c_13\t1\t\theidi said that pictures of herself were embarrassing .\n",
            "c_13\t0\t*\theidi said that bill 's pictures of herself were embarrassing .\n",
            "c_13\t0\t*\tchris said that himself was angry .\n",
            "c_13\t1\t\theidi saw peter 's picture of her .\n",
            "c_13\t1\t\theidi saw drawings of her .\n",
            "c_13\t1\t\tjohn loves himself .\n",
            "c_13\t1\t\tjohn loves pictures of himself .\n",
            "c_13\t0\t*\tjohn loves mary 's pictures of himself .\n",
            "c_13\t0\t*\tjohn thinks that mary 's depiction of himself is wrong .\n",
            "c_13\t1\t\tjohn thinks that most depictions of himself are wrong .\n",
            "c_13\t1\t\tjohn seems to like pictures of himself .\n",
            "c_13\t1\t\tjohn believes himself to be the best at baseball .\n",
            "c_13\t1\t\tjohn wants to congratulate himself .\n",
            "c_13\t0\t*\tjohn loves him .\n",
            "c_13\t1\t\tjohn loves his puppy .\n",
            "c_13\t1\t\tjohn asked if the unflattering description of his work would be published in the paper .\n",
            "c_13\t1\t\tjohn asked if his essay would be published in the paper .\n",
            "d_98\t1\t\tany owl can hunt mice .\n",
            "d_98\t0\t*\tjohn talked to any woman .\n",
            "d_98\t0\t*\tany woman contributed to the fund .\n",
            "d_98\t1\t\tjohn talked to any woman who came up to him .\n",
            "d_98\t1\t\tany woman who heard the news contributed to the fund .\n",
            "d_98\t1\t\tany man who saw the fly in the food did n't eat dinner .\n",
            "d_98\t1\t\tyou may pick any flower .\n",
            "d_98\t0\t*\tyou must pick any flower .\n",
            "d_98\t1\t\tany pilot could be flying this plane .\n",
            "d_98\t0\t*\tany pilot must be flying this plane .\n",
            "d_98\t1\t\tany student must work hard .\n",
            "d_98\t1\t\tany doctor will tell you that .\n",
            "d_98\t1\t\tany soldier should be prepared to die for her country .\n",
            "d_98\t1\t\tjohn talked to a woman .\n",
            "d_98\t1\t\tjohn did n't talk to a woman .\n",
            "d_98\t1\t\tjohn kissed even the ugliest woman .\n",
            "d_98\t1\t\tjohn kissed even the ugliest woman who came up to him .\n",
            "d_98\t1\t\ta lion is usually majestic .\n",
            "d_98\t0\t*\tany lion is usually majestic .\n",
            "d_98\t1\t\ta philosopher is sometimes wrong .\n",
            "d_98\t1\t\tany philosopher is sometimes wrong .\n",
            "d_98\t1\t\tyou must pick a flower .\n",
            "d_98\t1\t\ta pilot must be flying this plane .\n",
            "d_98\t1\t\ta student must work hard .\n",
            "d_98\t1\t\ta soldier should be prepared to die for her country .\n",
            "d_98\t1\t\trarely is any lion majestic .\n",
            "d_98\t1\t\tseldom is any lion majestic .\n",
            "d_98\t1\t\tnever is any lion majestic .\n",
            "d_98\t0\t*\tusually , any lion is majestic .\n",
            "d_98\t0\t*\toften , any lion is majestic .\n",
            "d_98\t0\t*\talways , any lion is majestic .\n",
            "d_98\t1\t\tyou may pick absolutely any flower .\n",
            "d_98\t1\t\tyou may pick almost any flower .\n",
            "d_98\t1\t\talmost any pilot could be flying this plane .\n",
            "d_98\t1\t\tabsolutely any pilot could be flying this plane .\n",
            "d_98\t1\t\tyou may pick any flower except the rose .\n",
            "d_98\t1\t\tany pilot except sue could be flying this plane .\n",
            "d_98\t1\t\tjohn talked to absolutely any woman who came up to him .\n",
            "d_98\t1\t\tjohn talked to almost any woman who came up to him .\n",
            "d_98\t1\t\tjohn talked to any woman who came up to him except sue .\n",
            "d_98\t1\t\tjohn put carrots from his garden in the salad .\n",
            "d_98\t1\t\tjohn put any carrot from his garden in the salad .\n",
            "d_98\t1\t\tjohn talked to a woman who came up to him .\n",
            "d_98\t1\t\ta woman who heard the news contributed to the fund .\n",
            "d_98\t1\t\ta man who saw the fly in the food did n't eat dinner .\n",
            "d_98\t1\t\tjohn talked to every woman who came up to him .\n",
            "d_98\t1\t\tevery woman who heard the news contributed to the fund .\n",
            "d_98\t1\t\tevery man who saw the fly in the food did n't eat dinner .\n",
            "d_98\t1\t\tjohn talked to every woman .\n",
            "d_98\t1\t\tmary regretted that she did anything to help him .\n",
            "d_98\t0\t*\tmary talked to any man or any woman .\n",
            "d_98\t1\t\tevery student who is in mary 's class is working on polarity items .\n",
            "d_98\t1\t\tit happens to be true of every student in mary 's class that he is working on polarity items .\n",
            "d_98\t1\t\tevery student in mary 's class , by virtue of being in her class , is working on polarity items .\n",
            "d_98\t1\t\tevery student in mary 's class happened to vote republican .\n",
            "d_98\t1\t\tevery woman standing under that tree is mary 's friend .\n",
            "d_98\t1\t\tthe president thanked every soldier who had fought in the .\n",
            "d_98\t1\t\teverybody who attended last week 's huge rally signed the petition .\n",
            "d_98\t1\t\twe did n't keep a list of the names , but the president thanked every soldier who had fought in the gulf war .\n",
            "d_98\t0\t*\tevery student in mary 's class , whoever they were , happened to vote republican .\n",
            "d_98\t0\t*\tevery woman standing under that tree , whoever she may be , is mary 's friend .\n",
            "d_98\t0\t*\tany student in mary 's class happened to vote .\n",
            "d_98\t0\t*\tany woman standing under that tree is mary 's friend .\n",
            "d_98\t1\t\tthe president thanked any soldier who had fought in the gulf .\n",
            "d_98\t1\t\tevery restaurant that advertises in any of these papers happens to have four stars in the handbook .\n",
            "d_98\t1\t\teverybody who is in mary 's semantics seminar is writing a paper on polarity items .\n",
            "d_98\t1\t\tjohn talked to any woman at the party .\n",
            "d_98\t1\t\tjohn talked to any politician who is powerful .\n",
            "d_98\t0\t*\tjohn talked to any powerful politician .\n",
            "d_98\t1\t\tmary confidently answered any objections .\n",
            "d_98\t1\t\tafter the dinner , we threw away any leftovers .\n",
            "d_98\t0\t*\tjohn bought any picture of queen elizabeth .\n",
            "d_98\t1\t\tjohn bought any picture of queen elizabeth that was on sale .\n",
            "d_98\t1\t\tevery philosopher is sometimes wrong , but he usually does n't admit it .\n",
            "d_98\t0\t*\tany lion is generally majestic .\n",
            "d_98\t0\t*\tany lion is rare .\n",
            "d_98\t1\t\tany female tiger has orange fur , marked with black stripes .\n",
            "d_98\t1\t\tbirds fly .\n",
            "d_98\t1\t\tany bird flies .\n",
            "d_98\t1\t\tall fugitives are in jail now .\n",
            "d_98\t1\t\tall lizards will die .\n",
            "d_98\t0\t*\tyesterday john talked to any woman .\n",
            "d_98\t1\t\tyesterday john talked to any woman he saw .\n",
            "d_98\t1\t\tsnow is white and snow is not white .\n",
            "d_98\t1\t\tany man did n't eat dinner .\n",
            "d_98\t1\t\tmary talks to any student .\n",
            "d_98\t0\t*\tmary talked to any angry student .\n",
            "d_98\t1\t\tmary talked to any student who was angry .\n",
            "d_98\t0\t*\tmary talked to any actual student .\n",
            "d_98\t0\t*\tany pilot on duty today must be flying this plane .\n",
            "d_98\t1\t\tany pilot must be out flying planes today .\n",
            "d_98\t1\t\tevery student read any book on giraffes he found .\n",
            "d_98\t0\t*\tyou must pick any flower in this bed .\n",
            "d_98\t1\t\tyou may pick any of the flowers .\n",
            "d_98\t0\t*\tyou must pick any of the flowers .\n",
            "d_98\t0\t*\tmary picked any of the flowers .\n",
            "d_98\t1\t\tyou may pick every flower .\n",
            "d_98\t1\t\tyou may pick any flower , but leave a few for mary .\n",
            "d_98\t1\t\tyou may pick any five flowers .\n",
            "d_98\t1\t\tmary did n't pick any of the flowers .\n",
            "d_98\t1\t\tpick any flower .\n",
            "d_98\t1\t\tconfiscate any liquor .\n",
            "d_98\t1\t\tpick any of these flowers .\n",
            "d_98\t0\t*\tconfiscate any of this liquor .\n",
            "d_98\t0\t*\tmary did n't see almost every flower .\n",
            "d_98\t0\t*\tmary did n't see almost any flower .\n",
            "d_98\t1\t\tevery student in mary 's class is working on negative polarity .\n",
            "d_98\t1\t\tthere were twenty students at the lecture and every student who was there said it was inspiring .\n",
            "d_98\t0\t*\tthere were twenty students at the lecture and any student who was there said it was inspiring .\n",
            "d_98\t1\t\twe have many graduate students but this year the graduate director met with every student in the graduate program individually to discuss their progress .\n",
            "d_98\t0\t*\twe have many graduate students but this year the graduate director met with any student in the graduate program individually to discuss their progress .\n",
            "d_98\t1\t\tsusan found every book she had been looking for at borders .\n",
            "d_98\t0\t*\tsusan found any book she had been looking for at borders .\n",
            "d_98\t1\t\tpaul has interviewed every student who was at the scene of the crime and kate has interviewed them too .\n",
            "d_98\t0\t*\tpaul has interviewed any student who was at the scene of the crime and kate has interviewed them too .\n",
            "d_98\t0\t*\tprofessor smith would support sue and prof jones bill .\n",
            "d_98\t1\t\tthere is every book by chomsky in this library .\n",
            "d_98\t0\t*\tthere is any book by chomsky in this library .\n",
            "d_98\t1\t\tthere 's everything mary had asked for in this store .\n",
            "d_98\t0\t*\tthere 's anything mary had asked for in this store .\n",
            "d_98\t1\t\tthere is any book you could imagine in this library .\n",
            "d_98\t1\t\tthere 's anything mary could desire in this store .\n",
            "d_98\t1\t\tthat evening john laughed with everybody he talked to .\n",
            "d_98\t1\t\tthat evening john laughed with anybody he talked to .\n",
            "d_98\t1\t\tjohn talked to everybody who came up to him at the party .\n",
            "d_98\t1\t\tjohn talked to anybody who came up to him at the party .\n",
            "d_98\t1\t\tbill offered mary everything he had cooked for dinner .\n",
            "d_98\t0\t*\tbill offered mary anything he had cooked for dinner .\n",
            "d_98\t1\t\tthose days bill offered mary everything he cooked .\n",
            "d_98\t1\t\tthose days bill offered mary anything he cooked .\n",
            "d_98\t1\t\tjohn made a fool of himself in front of everyone who was there .\n",
            "d_98\t1\t\tjohn made a fool of himself in front of anyone who was there .\n",
            "d_98\t1\t\tmary sang for everyone who wanted to hear her .\n",
            "d_98\t1\t\tmary sang for anyone who wanted to hear her .\n",
            "d_98\t1\t\tjohn slipped in front of everyone who was there .\n",
            "d_98\t0\t*\tjohn slipped in front of anyone who was there .\n",
            "d_98\t1\t\tat 4 p.m . i saw john lecturing to everyone who was near him .\n",
            "d_98\t0\t*\tat 4 p.m . i saw john lecturing to anyone who was near him .\n",
            "d_98\t1\t\tjohn knew every language that we encountered on our trip .\n",
            "d_98\t1\t\tjohn knew any language that we encountered on our trip .\n",
            "d_98\t1\t\tjohn liked everything that was placed before him .\n",
            "d_98\t1\t\tjohn liked anything that was placed before him .\n",
            "d_98\t1\t\tat the end of his speech , the president thanked any soldier who had fought in the gulf war .\n",
            "d_98\t1\t\tbob does not think that there is anyone from greece in his basement .\n",
            "d_98\t1\t\tcan anyone pledge $ 1000 ?\n",
            "d_98\t1\t\tis it possible for everyone to to pledge $ 1000 ?\n",
            "d_98\t1\t\tis there someone who can pledge $ 1000 ?\n",
            "d_98\t1\t\tif anybody comes , he rings the doorbell .\n",
            "d_98\t1\t\tevery student who wins any trophy displays it in a prominent place .\n",
            "d_98\t0\t*\tjohn saw anything .\n",
            "d_98\t1\t\tjohn did n't see anything .\n",
            "d_98\t0\t*\tsome who read anything passed .\n",
            "d_98\t1\t\tevery who read anything passed .\n",
            "d_98\t1\t\tno student who read anything passed .\n",
            "d_98\t0\t*\tsome answered any question .\n",
            "d_98\t0\t*\tevery student answered any question .\n",
            "d_98\t1\t\tany cat does n't like mice .\n",
            "d_98\t1\t\tevery cat does n't like mice .\n",
            "d_98\t1\t\tevery cat does n't like mice , for example felix does n't .\n",
            "d_98\t1\t\talmost every cat likes mice , but felix does n't .\n",
            "d_98\t0\t*\tevery cat does n't like mice , but felix does n't .\n",
            "d_98\t1\t\talmost every cat likes mice , for example felix does n't .\n",
            "g_81\t1\t\tthe dodgers beat the red sox and the dodgers were beaten by the giants .\n",
            "g_81\t1\t\tthe dodgers beat the red sox and the giants beat the dodgers .\n",
            "g_81\t1\t\tdifferent teams beat the red sox and were beaten by the giants .\n",
            "g_81\t1\t\tjohn gave the books to mary and the records to sue .\n",
            "g_81\t1\t\thow many did you buy of those pies at the fair ?\n",
            "g_81\t1\t\thow many have you given of these books to these people .\n",
            "g_81\t0\t*\tthe man chased fido returned .\n",
            "g_81\t1\t\tthe man that chased fido returned .\n",
            "g_81\t1\t\tthe man i think chased fido returned .\n",
            "g_81\t0\t*\tthe man i think that chased fido returned .\n",
            "g_81\t1\t\tthe man who i think chased fido returned .\n",
            "g_81\t0\t*\tthe man who i think that chased fido returned .\n",
            "g_81\t1\t\twho did you think mary saw ?\n",
            "g_81\t1\t\thow slowly would you say he was driving ?\n",
            "g_81\t1\t\thow suspicious was mary ?\n",
            "g_81\t1\t\twho saw the man ?\n",
            "g_81\t1\t\twho do you think that you saw ?\n",
            "g_81\t0\t*\twho do you think that saw you ?\n",
            "g_81\t1\t\twho do you regret that you saw ?\n",
            "g_81\t0\t*\twho do you regret that saw you ?\n",
            "g_81\t1\t\twho do you think you saw ?\n",
            "g_81\t1\t\twho do you think saw you ?\n",
            "g_81\t0\t*\twho do you regret you saw ?\n",
            "g_81\t0\t*\twho do you regret saw you ?\n",
            "g_81\t0\t*\twho did you believe that came ?\n",
            "g_81\t0\t*\twho did you wonder whether came ?\n",
            "g_81\t0\t*\twho did you wonder if came ?\n",
            "g_81\t0\t*\twho did you arrange for to come ?\n",
            "g_81\t0\t*\twhich table did you wonder on kim put the book ?\n",
            "g_81\t0\t*\twhich did you buy the table on kim put the book ?\n",
            "g_81\t0\t*\twhat do you believe that iron is to be a fact well known to virtually everybody ?\n",
            "g_81\t0\t*\twho did you wonder saw kim ?\n",
            "g_81\t0\t*\twhich did you buy the table supported the book ?\n",
            "g_81\t0\t*\tthe fact , i put it down to that kim came .\n",
            "g_81\t0\t*\tthe table , i put kim on which supported the book .\n",
            "g_81\t1\t\twho is it that mary likes ?\n",
            "g_81\t1\t\the was talkative .\n",
            "g_81\t1\t\the was a bully .\n",
            "g_81\t1\t\the was talkative and a bully .\n",
            "g_81\t0\t*\tthe talkative and a bully man entered .\n",
            "g_81\t0\t*\ttalkative and a bully entered .\n",
            "g_81\t0\t*\tjohn is easy to please and to love mary .\n",
            "g_81\t0\t*\tthe man who mary loves and sally hates george computed my tax .\n",
            "g_81\t1\t\tjohn is easy to please and to love .\n",
            "g_81\t1\t\tthe kennel which mary made and fido sleeps in has been stolen .\n",
            "g_81\t1\t\tthe kennel in which mary keeps drugs and fido sleeps has been stolen .\n",
            "g_81\t0\t*\tthe kennel in which mary made and fido sleeps has been stolen .\n",
            "g_81\t1\t\tjohn saw more horses than bill saw or pete talked to .\n",
            "g_81\t1\t\tjohn saw more horses than bill saw cows or pete talked to cats .\n",
            "g_81\t0\t*\tjohn saw more horses than bill saw cows or pete talked to .\n",
            "g_81\t1\t\ti know a man who bill saw and mary liked .\n",
            "g_81\t1\t\ti know a man who saw bill and liked mary .\n",
            "g_81\t0\t*\ti know a man who bill saw and liked mary .\n",
            "g_81\t1\t\ti wonder who bill saw and mary liked .\n",
            "g_81\t0\t*\ti wonder who bill saw and liked mary .\n",
            "g_81\t1\t\ti wonder who mary likes and hopes will win .\n",
            "g_81\t0\t*\tjohn asked who and where bill had seen .\n",
            "g_81\t1\t\twhich book and which pencil did john buy ?\n",
            "g_81\t0\t*\twhere and when did bill put the book ?\n",
            "g_81\t1\t\ton which table and under which flower pot did john put the keys ?\n",
            "g_81\t1\t\tto which city and to which conference did bill go ?\n",
            "g_81\t1\t\tto which city and which conference did bill go ?\n",
            "g_81\t1\t\twhich city and which conference did bill go to ?\n",
            "g_81\t0\t*\twhich city and which conference did bill go to to ?\n",
            "g_81\t0\t*\twhich city and to which conference did bill go to ?\n",
            "g_81\t0\t*\tto which city and which conference did bill go to ?\n",
            "g_81\t0\t*\tjohn , who and whose friends you saw , is a fool .\n",
            "g_81\t1\t\tjohn , to who and to whose friends that letter was addressed , is a fool .\n",
            "g_81\t1\t\ti wonder when and how often she went that day .\n",
            "g_81\t1\t\ti wonder who and whose friends he handed over to the fbi .\n",
            "g_81\t1\t\ti have wanted to know exactly what happened to rosa luxemburg for many years .\n",
            "g_81\t1\t\ti have wanted to know for many years exactly what happened to rosa .\n",
            "g_81\t1\t\ti had hoped that it was true that rosa luxemburg had actually defected to iceland for many years .\n",
            "g_81\t1\t\ti had hoped that it was true for many years that rosa luxemburg had actually defected to iceland .\n",
            "g_81\t1\t\ti have wanted to meet the man who spent so much money planning the assassination of kennedy for many years .\n",
            "g_81\t1\t\ti have wanted to meet for many years the man who spent so much money planning the assassination of kennedy .\n",
            "g_81\t1\t\tthe woman believed that the man was ill who was here .\n",
            "g_81\t1\t\tthe woman believed that the man who was here was ill .\n",
            "g_81\t1\t\tthe woman who was here believed that the man was ill .\n",
            "g_81\t1\t\ta woman hit a girl who was pregnant .\n",
            "g_81\t1\t\tpeople are said to do crazier things at higher speeds there by dorothy than they are by other people .\n",
            "g_81\t1\t\tpeople are said to do such crazy things at such high speeds there by dorothy that i am getting skeptical .\n",
            "g_81\t1\t\ta woman hit a pregnant girl .\n",
            "g_81\t1\t\ta pregnant woman hit a girl .\n",
            "g_81\t1\t\ta man just came in and a woman went out who were similar in all kinds of ways .\n",
            "g_81\t1\t\ta man just came in and a woman went out who hate each other like poison and always have .\n",
            "g_81\t0\t*\ti find it easy to believe - but joan finds it hard to believe - tom to be dishonest .\n",
            "g_81\t0\t*\tjohn offered , and harry gave , sally a cadillac .\n",
            "g_81\t0\t*\tjohn told , and harry showed , seymour that sally was a virgin .\n",
            "g_81\t1\t\tjack may be and tony certainly is a werewolf .\n",
            "g_81\t1\t\tharry has claimed but i do not believe that melvin is a communist .\n",
            "g_81\t1\t\ti like but tom does n't like to visit new places .\n",
            "g_81\t1\t\ti can tell you when , but i ca n't tell you why , he left me .\n",
            "g_81\t1\t\ti 've been wondering whether , but would n't positively want to state that .\n",
            "g_81\t1\t\tjohn hummed , and mary sang , the same tune .\n",
            "g_81\t1\t\tjohn hummed , and mary sang , at equal volumes .\n",
            "g_81\t1\t\tjohn gave mary , and joan presented to fred , books which looked .\n",
            "g_81\t1\t\tthe red sox beat , and the giants were beaten by , different teams .\n",
            "g_81\t1\t\tsmith loaned , and his widow later donated , a valuable collection of manuscripts to the library .\n",
            "m_02\t1\t\twhich club did you hit the winning putt with ?\n",
            "m_02\t1\t\twith which club did you hit the winning putt ?\n",
            "m_02\t1\t\tethel was sitting at her desk .\n",
            "m_02\t0\t*\tthe ethel was sitting at her desk .\n",
            "m_02\t0\t*\taccountant was sitting at her desk .\n",
            "m_02\t1\t\tthe accountant was sitting at her desk .\n",
            "m_02\t1\t\taccountants audit our finances every year .\n",
            "m_02\t0\t*\ti would like an accountants to sort out my tax return .\n",
            "m_02\t1\t\tsome accountants were quietly counting in the back office .\n",
            "m_02\t1\t\twould more accountants make any difference to my tax bill ?\n",
            "m_02\t1\t\tthe truck spread salt .\n",
            "m_02\t1\t\tthe truck spread the salt .\n",
            "m_02\t1\t\tthe truck spread salts .\n",
            "m_02\t1\t\tthis truck spread less salt than that one .\n",
            "m_02\t0\t*\tthis truck spread fewer salt than that one .\n",
            "m_02\t1\t\tthere are fewer trucks on the motorway this winter .\n",
            "m_02\t1\t\tthere are less trucks on the motorway this winter .\n",
            "m_02\t0\t*\tthe white rabbit vanished his watch .\n",
            "m_02\t1\t\tdogs chase cats .\n",
            "m_02\t0\t*\tdogs chase .\n",
            "m_02\t1\t\tflora cooks .\n",
            "m_02\t1\t\tflora cooks gourmet meals .\n",
            "m_02\t1\t\tthe cat shot into the kitchen on sunday morning carrying a dead mouse .\n",
            "m_02\t1\t\tthe cat sauntered into the kitchen carrying a dead mouse .\n",
            "m_02\t1\t\tmaisie drove her car from morningside to leith on wednesday .\n",
            "m_02\t1\t\ton wednesday maisie drove her car from morningside to leith .\n",
            "m_02\t1\t\tmaisie drove her car on wednesday from morningside to leith .\n",
            "m_02\t1\t\tjeeves sauntered into the room .\n",
            "m_02\t0\t*\tinto jeeves sauntered the room .\n",
            "m_02\t1\t\tinto the room sauntered jeeves .\n",
            "m_02\t1\t\twhich room did jeeves sauntered into ?\n",
            "m_02\t1\t\tinto which room did jeeves sauntered ?\n",
            "m_02\t1\t\tbarbara handed the results to alan on tuesday .\n",
            "m_02\t1\t\tthe pupils in this maths class gave cakes to margaret every .\n",
            "m_02\t1\t\tcakes were given to margaret every friday by the pupils in this maths class .\n",
            "m_02\t1\t\tthis parcel is very heavy .\n",
            "m_02\t1\t\tthis very heavy parcel was delivered yesterday .\n",
            "m_02\t1\t\tvery heavy , this parcel !\n",
            "m_02\t1\t\twhat this parcel is is very heavy .\n",
            "m_02\t1\t\twe felled the murder with this chainsaw .\n",
            "m_02\t1\t\twith this chainsaw we felled the murder .\n",
            "m_02\t1\t\tbarbara handed the intriguing results of the latest examination to alan on tuesday .\n",
            "m_02\t1\t\tbarbara handed them to alan on tuesday .\n",
            "m_02\t1\t\tthis large parcel is very heavy .\n",
            "m_02\t1\t\tthis large parcel is very heavy and so is this small packet .\n",
            "m_02\t1\t\tvera is knitting in the lounge .\n",
            "m_02\t1\t\tvera is knitting there .\n",
            "m_02\t1\t\tgrandma is coming to mr chalky 's school tomorrow .\n",
            "m_02\t1\t\tgrandma is coming here tomorrow .\n",
            "m_02\t1\t\tthe cat was sleeping in the kitchen .\n",
            "m_02\t1\t\tthe cat trotted into the kitchen .\n",
            "m_02\t1\t\tthe mouse jumped out of the cheese box .\n",
            "m_02\t1\t\tthe mouse was out the cheese box .\n",
            "m_02\t1\t\tthe cat trotted in the kitchen .\n",
            "m_02\t1\t\tthe cat trotted in .\n",
            "m_02\t1\t\tthe mouse jumped out .\n",
            "m_02\t1\t\tthe terrier attacked the burglar .\n",
            "m_02\t1\t\tthe terrier savaged the burglar 's ankles .\n",
            "m_02\t1\t\tthe terrier attacked the burglar and the terrier savaged the burglar 's ankles .\n",
            "m_02\t1\t\tthe terrier attacked the burglar and savaged the burglar 's ankles .\n",
            "m_02\t1\t\tdid the wealthy young man buy that piano for his secret fiancée ?\n",
            "m_02\t1\t\twho bought that piano for his secret fiancée ?\n",
            "m_02\t1\t\twhat did the wealthy young man buy for his secret fiancée ?\n",
            "m_02\t1\t\twho did the wealthy young man buy that piano for ?\n",
            "m_02\t1\t\tthe wealthy young man bought his secret fiancée that piano .\n",
            "m_02\t1\t\tthat piano was bought for his secret fiancée by the wealthy young man .\n",
            "m_02\t1\t\ti do n't like the plum brandy , but the port i just love .\n",
            "m_02\t1\t\tfrank bought the piano for jane .\n",
            "m_02\t1\t\tfrank bought jane the piano .\n",
            "m_02\t1\t\tthe piano was bought for jane by frank .\n",
            "m_02\t1\t\tthe piano frank bought for jane .\n",
            "m_02\t1\t\tdid frank buy the piano for jane ?\n",
            "m_02\t1\t\tdid frank buy jane the piano ?\n",
            "m_02\t1\t\twas the piano bought for jane by frank ?\n",
            "m_02\t1\t\twhat did frank buy for jane ?\n",
            "m_02\t1\t\tfrank bought something for jane .\n",
            "m_02\t1\t\tdid frank buy something for jane .\n",
            "m_02\t1\t\twhat did frank buy for jane .\n",
            "m_02\t1\t\tthe children chased the dog .\n",
            "m_02\t1\t\tthe cook saved no scraps for the dog .\n",
            "m_02\t1\t\tsarah devoured the cakes in the kitchen last night .\n",
            "m_02\t1\t\tmr knightley despaired .\n",
            "m_02\t1\t\temma slighted miss bates .\n",
            "m_02\t1\t\tjane fairfax seemed upset .\n",
            "m_02\t1\t\tmr woodhouse sat in an armchair .\n",
            "m_02\t1\t\tmr knightley walked into the drawing room .\n",
            "m_02\t1\t\tmr elton handed his wife into the carriage .\n",
            "m_02\t1\t\temma gave bad advice to harriet .\n",
            "m_02\t1\t\tmr knightley suggested that thieves would break into hartfield .\n",
            "m_02\t1\t\teleanor blamed willoughby for marianne 's unhappiness .\n",
            "m_02\t1\t\teleanor blamed marianne 's unhappiness on willoughby .\n",
            "m_02\t1\t\tthe romans built this aqueduct .\n",
            "m_02\t1\t\tthe computer will calculate the value of the variable .\n",
            "m_02\t1\t\tthese objections killed the proposal .\n",
            "m_02\t0\t*\tlecturer was sitting at her desk .\n",
            "m_02\t1\t\ttoo much salt damages vehicles .\n",
            "m_02\t0\t*\ttoo much vehicles are damaged by salt .\n",
            "m_02\t0\t*\ttoo many salt damages vehicles .\n",
            "m_02\t1\t\ttoo many vehicles are damaged by salt .\n",
            "m_02\t1\t\tfrank churchill gave a piano to jane fairfax .\n",
            "m_02\t1\t\ta piano was given to jane fairfax by frank churchill .\n",
            "m_02\t1\t\twickham eloped with lydia .\n",
            "m_02\t1\t\tmiss bates can chatter on for hours .\n",
            "m_02\t1\t\thenry crawford loved fanny but fanny loved edmund .\n",
            "m_02\t1\t\tmr bingley became tired of jane or mr d'arcy persuaded mr .\n",
            "m_02\t1\t\telizabeth regretted that she had met wickham .\n",
            "m_02\t1\t\tcatherine feared that the abbey was haunted .\n",
            "m_02\t1\t\tthat anne was in conversation with mr elliott dismayed captain .\n",
            "m_02\t1\t\tfanny was delighted by the idea that she could subscribe to a library .\n",
            "m_02\t1\t\twho thought up the proposal that the committee be abolished ?\n",
            "m_02\t1\t\tthe cottage which mrs dashwood accepted was rather small .\n",
            "m_02\t1\t\tthe gentleman who saved marianne was willoughby .\n",
            "m_02\t1\t\tthe building that we liked is in thornton lacey .\n",
            "m_02\t1\t\tit was anne elliott who loved captain wentworth but who rejected his first proposal .\n",
            "m_02\t1\t\ta motorist has reported that the road is blocked by snow at bunker hill .\n",
            "m_02\t1\t\tthe labrador ate all the food which we left on the kitchen table .\n",
            "m_02\t1\t\tshow me the folder in which you stored the documents .\n",
            "m_02\t1\t\ti like the book that you gave me .\n",
            "m_02\t1\t\ti love the food they cook in the halls of residence .\n",
            "m_02\t1\t\ta motorist has reported the road is blocked at bunker hill .\n",
            "m_02\t1\t\ti am delighted at the idea they might demolish the appleton tower .\n",
            "m_02\t1\t\tthe cottage which mrs dashwood accepted was very small .\n",
            "m_02\t1\t\tanne musgrave has just seen mr elliott in bath street .\n",
            "m_02\t1\t\tnurse rooke has discovered where anne elliott stayed .\n",
            "m_02\t1\t\tnurse rooke suspected that mrs clay planned to run away with .\n",
            "m_02\t1\t\tanne astonished her father .\n",
            "m_02\t1\t\tthat captain wentworth married anne astonished her father .\n",
            "m_02\t1\t\tsir walter elliott imagined the scene .\n",
            "m_02\t1\t\tsir walter elliott imagined that he was still handsome .\n",
            "m_02\t1\t\tyesterday lydia eloped with wickham .\n",
            "m_02\t1\t\tlydia eloped with wickham yesterday .\n",
            "m_02\t1\t\twhen lydia went to brighton , she eloped with wickham .\n",
            "m_02\t1\t\tlydia eloped with wickham when she went to brighton .\n",
            "m_02\t1\t\tbecause of the strike the commuters travelled by army lorry .\n",
            "m_02\t1\t\tthe commuters travelled by army lorry because of the strike .\n",
            "m_02\t1\t\tbecause the bus drivers were on strike , the commuters travelled by army lorry .\n",
            "m_02\t1\t\tthe commuters travelled by army lorry because the bus drivers were on strike .\n",
            "m_02\t1\t\talthough mr d'arcy disliked mrs bennet he married elizabeth .\n",
            "m_02\t1\t\tin spite of his dislike of mrs bennet , mr d'arcy married elizabeth .\n",
            "m_02\t1\t\tif emma had left hartfield , mr woodhouse would have been unhappy .\n",
            "m_02\t1\t\tdid captain wentworth write a letter to anne elliott ?\n",
            "m_02\t1\t\twrite a letter to anne elliott .\n",
            "m_02\t0\t*\tbecause did marianne love willoughby , she refused to .\n",
            "m_02\t0\t*\tif did emma leave hartfield , mr woodhouse would be unhappy .\n",
            "m_02\t0\t*\twhen did fanny return , she found tom bertram very ill .\n",
            "m_02\t0\t*\tthe cottage which did mrs dashwood accept was rather small .\n",
            "m_02\t0\t*\tcatherine feared that was the abbey haunted .\n",
            "m_02\t1\t\tthe girls wondered who mr bennet had received in his library .\n",
            "m_02\t1\t\twe were wondering who did you meet at the conference .\n",
            "m_02\t1\t\tshe said that in came aunt norris .\n",
            "m_02\t1\t\tshe said that into the room came aunt norris .\n",
            "m_02\t0\t*\tthe person who in came at that moment was aunt norris .\n",
            "m_02\t0\t*\tbecause in came aunt norris , fanny stopped talking .\n",
            "m_02\t0\t*\twhen in came aunt norris , fanny stopped talking .\n",
            "m_02\t0\t*\tbecause into the room came aunt norris , fanny stopped talking .\n",
            "m_02\t0\t*\twhen into the room came aunt norris , fanny stopped talking .\n",
            "m_02\t1\t\tnever had sir thomas been so offended .\n",
            "m_02\t0\t*\tthe person who never had he been so offended was sir thomas .\n",
            "m_02\t0\t*\tbecause never had sir thomas been so offended , even mr yates left .\n",
            "m_02\t0\t*\twhen never had sir thomas been so offended , mr yates left .\n",
            "m_02\t1\t\tdr jones habitually ate too much rich food , did n't he ?\n",
            "m_02\t0\t*\twe realised that dr jones died because he ate too much rich food , did n't he ?\n",
            "m_02\t0\t*\tthe person who ate too much rich food did n't he was dr .\n",
            "m_02\t0\t*\tbecause dr jones ate too much rich food did n't he , he died of apoplexy .\n",
            "m_02\t0\t*\twhen dr jones died of apoplexy did n't he , mary crawford went to live with his wife .\n",
            "m_02\t1\t\tfanny stopped talking because in came aunt norris .\n",
            "m_02\t0\t*\tbecause in came aunt norris fanny stopped talking .\n",
            "m_02\t1\t\tmr yates left because never had sir thomas been so offended .\n",
            "m_02\t0\t*\tbecause never had sir thomas been so offended , mr yates left .\n",
            "m_02\t0\t*\tfanny stopped talking when in came aunt norris .\n",
            "m_02\t0\t*\twhen in came aunt norris fanny stopped talking .\n",
            "m_02\t0\t*\tfanny continued talking although in came aunt norris .\n",
            "m_02\t0\t*\talthough in came aunt norris , fanny continued talking .\n",
            "m_02\t1\t\tfanny had just stopped talking when in came aunt norris .\n",
            "m_02\t1\t\tfanny regretted talking to mary .\n",
            "m_02\t1\t\thenry wanted to marry fanny .\n",
            "m_02\t1\t\tmrs bennet having taken the others upstairs , mr bingley proposed to .\n",
            "m_02\t1\t\tall mr collins does is praise lady de bourg .\n",
            "m_02\t1\t\tlady de bourg tried to persuade elizabeth to renounce mr d'arcy .\n",
            "m_02\t1\t\thenry wanted to have married fanny before edmund returned .\n",
            "m_02\t1\t\tfanny regretted having talked to mary .\n",
            "m_02\t1\t\twhat mr collins is doing is praising lady de bourg .\n",
            "m_02\t0\t*\tfanny regretted being talking to mary .\n",
            "m_02\t0\t*\tall mr collins has done is have praised lady de bourg .\n",
            "m_02\t1\t\tjulia and maria wanted to be allowed to perform a play .\n",
            "m_02\t1\t\tedmund wanted fanny to be able to ride a horse .\n",
            "m_02\t0\t*\thenry wanted to possibly marry fanny .\n",
            "m_02\t1\t\tfanny loved talking to mary .\n",
            "m_02\t1\t\tslamming the door , he ran down the steps .\n",
            "m_02\t0\t*\the was knowing the country well .\n",
            "m_02\t1\t\twhen ripe , these apples will be delicious .\n",
            "m_02\t1\t\tthe tigers hunt prey at night .\n",
            "m_02\t1\t\tfiona hoped to meet the prime minister .\n",
            "m_02\t1\t\tarthur tried to bake a cake .\n",
            "m_02\t1\t\tfiona persuaded arthur to bake a cake .\n",
            "m_02\t1\t\tsusan wanted jane to study german .\n",
            "m_02\t1\t\tayala went to the ball and chatted to jonathan stubbs .\n",
            "m_02\t0\t*\tayala went to the ball and jonathan stubbs chatted to .\n",
            "m_02\t1\t\tayala went to the ball and was chatted to by jonathan stubbs .\n",
            "m_02\t1\t\tall the beatles came to merle park .\n",
            "m_02\t1\t\tthe beatles all came to merle park .\n",
            "m_02\t1\t\tboth jane and elizabeth were at home .\n",
            "m_02\t1\t\tjane and elizabeth were both at home .\n",
            "m_02\t1\t\tlarry hunted all the foxes .\n",
            "m_02\t0\t*\tlarry all hunted the foxes .\n",
            "m_02\t0\t*\tlarry hunted the foxes all .\n",
            "m_02\t1\t\tgeorge built both the houses .\n",
            "m_02\t0\t*\tgeorge both built the houses .\n",
            "m_02\t0\t*\tgeorge built the houses both .\n",
            "m_02\t1\t\tall the foxes were hunted by larry .\n",
            "m_02\t1\t\taugusta blamed herself for what happened .\n",
            "m_02\t1\t\tthese documents elizabeth is checking at this very moment .\n",
            "m_02\t1\t\tlouise broke the cup .\n",
            "m_02\t1\t\talison drove the car .\n",
            "m_02\t1\t\tmartha chewed the bread .\n",
            "m_02\t1\t\tthe cup was broken by louise .\n",
            "m_02\t1\t\tthe car was driven by alison .\n",
            "m_02\t1\t\tthe bread was chewed by martha .\n",
            "m_02\t1\t\tthese fields were marched over by all the armies of europe .\n",
            "m_02\t1\t\thow is someone to chat to a girl if she does not go out ?\n",
            "m_02\t1\t\tall the armies of europe marched over these fields .\n",
            "m_02\t1\t\tayala sent back the diamond necklace .\n",
            "m_02\t1\t\tayala sent the diamond necklace back .\n",
            "m_02\t1\t\tayala sent her cousin the diamond necklace .\n",
            "m_02\t0\t*\tayala sent back her cousin the diamond necklace .\n",
            "m_02\t1\t\ttatiana wrote to onegin .\n",
            "m_02\t1\t\tfrank bought a piano for jane .\n",
            "m_02\t1\t\tlucy sent a letter to jane .\n",
            "m_02\t1\t\tlucy sent jane a letter .\n",
            "m_02\t1\t\tthe company sent china its senior mining engineers to help plan the new mines .\n",
            "m_02\t0\t*\tthe experts attributed raphael this picture .\n",
            "m_02\t0\t*\ti forwarded winifred the letter .\n",
            "m_02\t0\t*\tthe manager presented the foreman a gold watch .\n",
            "m_02\t0\t*\tkick john the ball .\n",
            "m_02\t0\t*\tthe critics ascribe shakespeare this play .\n",
            "m_02\t1\t\twho did john send a book to ?\n",
            "m_02\t1\t\tto whom did john send a book ?\n",
            "m_02\t1\t\twhat place did you travel to ?\n",
            "m_02\t1\t\tto what place did you travel ?\n",
            "m_02\t1\t\twhat place did john send the book ?\n",
            "m_02\t0\t*\twho was the book sent by john .\n",
            "m_02\t0\t*\twhat place was the book sent by john ?\n",
            "m_02\t1\t\tonly to the best students would he give this book .\n",
            "m_02\t0\t*\tonly the best students would he give this book .\n",
            "m_02\t1\t\tonly to glasgow would he go by train .\n",
            "m_02\t0\t*\tonly glasgow would he travel by train .\n",
            "m_02\t1\t\tit is to the best students that he gives this book .\n",
            "m_02\t0\t*\tit is the best students he gives this book .\n",
            "m_02\t1\t\tit is to ireland that he is going .\n",
            "m_02\t0\t*\tit is ireland that he is going .\n",
            "m_02\t1\t\the told her the whole story .\n",
            "m_02\t1\t\tshe told him the whole story .\n",
            "m_02\t1\t\tthe other plan she rejected out of hand .\n",
            "m_02\t1\t\tthe vase got broken that sheila had brought all the way from .\n",
            "m_02\t1\t\tthe plan was rejected out of hand that traffic should be banned .\n",
            "m_02\t1\t\tnorman lemming jumped off the cliff and william lemming did so too .\n",
            "m_02\t1\t\tnorman lemming jumped off the cliff and so did william lemming .\n",
            "m_02\t1\t\tharriet could n't marry mr knightley but emma could .\n",
            "m_02\t1\t\twhat harriet did was marry mr martin .\n",
            "m_02\t1\t\tmarry mr martin was what harriet did .\n",
            "m_02\t1\t\temma insulted miss bates and annoyed mr knightley .\n",
            "m_02\t1\t\tharriet swooned .\n",
            "m_02\t1\t\tthe book is astonishingly boring .\n",
            "m_02\t1\t\tthe ethel we all know and love wishes to ask you some awkward questions .\n",
            "m_02\t1\t\tgolfers can be good company .\n",
            "m_02\t1\t\tenthusiastic golfers with large handicaps can be good company .\n",
            "m_02\t1\t\tthese enthusiastic golfers that i met at the nineteenth hole can be good company .\n",
            "m_02\t0\t*\tgolfer who is in training has a pretty powerful swing .\n",
            "m_02\t1\t\tmemo ate the spaghetti .\n",
            "m_02\t1\t\tmemo liked lasagna .\n",
            "m_02\t1\t\temma made harriet her friend .\n",
            "m_02\t1\t\tthe quiche and i were cooking .\n",
            "m_02\t1\t\terika made her mother an omelet and the kitchen a mess .\n",
            "m_02\t1\t\tbill went to london on monday .\n",
            "m_02\t1\t\tbill went on monday to london .\n",
            "m_02\t1\t\tmy brother lives near strasbourg .\n",
            "m_02\t1\t\tnear strasbourg my brother lives .\n",
            "m_02\t1\t\the planted the garden with roses last november .\n",
            "m_02\t1\t\the planted the garden last november with roses .\n",
            "m_02\t1\t\tthe baby chewed the biscuit .\n",
            "m_02\t1\t\tthe baby is heavy .\n",
            "m_02\t1\t\twhat the baby did was chew the biscuit .\n",
            "m_02\t1\t\tthe baby was chewing the biscuit .\n",
            "m_02\t1\t\tchew the biscuit !\n",
            "m_02\t1\t\thartfield house is in surrey .\n",
            "m_02\t1\t\tmr knightley rode to kingston .\n",
            "m_02\t1\t\teleanor and marianne travelled from shropshire .\n",
            "m_02\t1\t\tfrank gave a piano to jane fairfax .\n",
            "m_02\t1\t\tjane fairfax received a piano from frank .\n",
            "m_02\t1\t\tthe thief smashed the window with a hammer .\n",
            "m_02\t1\t\tcaptain wentworth recovered the property for mrs smith .\n",
            "m_02\t1\t\tthe window was broken by a hammer .\n",
            "m_02\t1\t\twren built st paul 's cathedral .\n",
            "m_02\t1\t\tsiobhan burnt a pattern on the piece of wood .\n",
            "m_02\t1\t\tthe dog dug a hole in the lawn .\n",
            "m_02\t1\t\tthe vase stood on the table in the hall .\n",
            "m_02\t1\t\timogen took the vase to her mother 's .\n",
            "m_02\t1\t\timogen broke the vase .\n",
            "m_02\t1\t\tsue knows the answer .\n",
            "m_02\t1\t\tthe answer is known to sue .\n",
            "m_02\t1\t\tjim was happily chopping logs .\n",
            "m_02\t1\t\tjim was chopping logs when margaret left and was still at it when she got back .\n",
            "m_02\t1\t\tjim was enthusiastically chopping logs .\n",
            "m_02\t1\t\tcaptain oates died in order to save his comrades .\n",
            "m_02\t1\t\tthis arch supports the weight of the tower .\n",
            "m_02\t1\t\twhat this arch does is support the weight of the tower .\n",
            "m_02\t1\t\tthis arch is supporting the weight of the tower .\n",
            "m_02\t1\t\tthe computer is playing six simultaneous games of three dimensional chess .\n",
            "m_02\t1\t\tthe intense cold killed the climbers .\n",
            "m_02\t1\t\tthe climbers were killed by the intense cold .\n",
            "m_02\t1\t\tthe climbers were killed with the intense cold .\n",
            "m_02\t1\t\tcatriona opened the door with this key .\n",
            "m_02\t1\t\tthe visas are with the passports .\n",
            "m_02\t1\t\tsally went to the party with andrew .\n",
            "m_02\t1\t\talan made the loaf with strong white flour .\n",
            "m_02\t1\t\tthe builders made the wall with concrete blocks .\n",
            "m_02\t1\t\tthe gardener planted roses in the garden .\n",
            "m_02\t1\t\tit was roses that the gardener planted in the garden .\n",
            "m_02\t1\t\tit is the garden that the gardener planted with roses .\n",
            "m_02\t1\t\troses are certain to be planted in the garden by the gardener .\n",
            "m_02\t1\t\tthe garden is certain to be planted with roses by the gardener .\n",
            "m_02\t1\t\thelen sent a scarf to jim for margaret .\n",
            "m_02\t1\t\twhat happened was they went home .\n",
            "m_02\t0\t*\twhat happened was they knew his parents .\n",
            "m_02\t0\t*\twe are knowing this theory .\n",
            "m_02\t1\t\tthey 're believing everything you say .\n",
            "m_02\t1\t\tyou 'll soon be owning all the land round here .\n",
            "m_02\t1\t\twhat she did was e-mail all her friends .\n",
            "m_02\t0\t*\twhat she did was know this theory .\n",
            "m_02\t0\t*\twhat she did was be very cold .\n",
            "m_02\t0\t*\twhat she did was own all the land round here .\n",
            "m_02\t1\t\tharriet talked to emma for hours .\n",
            "m_02\t1\t\tthe dog chased the cat for days .\n",
            "m_02\t1\t\tharriet told emma the whole story .\n",
            "m_02\t1\t\tthe dog caught the cat .\n",
            "m_02\t1\t\tthe beaver built a dam .\n",
            "m_02\t1\t\tanne played the tune on the piano .\n",
            "m_02\t1\t\tjane was playing the piano .\n",
            "m_02\t1\t\tjane played the piano .\n",
            "m_02\t1\t\ttess was knocking at the door .\n",
            "m_02\t1\t\ttess knocked at the door .\n",
            "m_02\t1\t\tfrank churchill was crossing the street .\n",
            "m_02\t1\t\tjane is visiting emma .\n",
            "m_02\t1\t\tjane visits emma .\n",
            "m_02\t1\t\ttess is knocking at the door .\n",
            "m_02\t1\t\ttess knocks at the door .\n",
            "m_02\t1\t\tfrank churchill is crossing the street .\n",
            "m_02\t1\t\tfrank churchill crosses the street .\n",
            "m_02\t1\t\treal play valencia next sunday .\n",
            "m_02\t1\t\ti leave for paris next week .\n",
            "m_02\t0\t*\tthe volcano erupts on tuesday .\n",
            "m_02\t1\t\tthe minister has arrived .\n",
            "m_02\t1\t\ti 've been at work for six hours .\n",
            "m_02\t1\t\thave you ever visited doubtful sound ?\n",
            "m_02\t1\t\tthere was an attack yesterday .\n",
            "m_02\t1\t\temma and harriet were attacked by those bandits .\n",
            "m_02\t1\t\tthose bandits attacked emma and harriet yesterday .\n",
            "m_02\t1\t\tthe vase was smashed deliberately .\n",
            "m_02\t1\t\tthe sheep got infected with scrapie .\n",
            "m_02\t1\t\tthe fans were deliberately provoked by a rival group .\n",
            "m_02\t1\t\tthe fans got deliberately provoked by a rival group .\n",
            "m_02\t1\t\tsix students got shot accidentally .\n",
            "m_02\t1\t\tsome gifts get used a dozen or so times a year .\n",
            "m_02\t1\t\tca n't you see i 'm reading ?\n",
            "m_02\t1\t\tpeople go hunting in the autumn .\n",
            "m_02\t1\t\twe spent yesterday cooking .\n",
            "m_02\t1\t\tshe buys for harrods .\n",
            "m_02\t1\t\ti saw and he chops .\n",
            "m_02\t1\t\tthis sweater washes well .\n",
            "m_02\t1\t\tthis book reads well .\n",
            "m_02\t1\t\tthese cars sold very quickly last week .\n",
            "m_02\t1\t\tit will take years for the mersey to clean .\n",
            "m_02\t1\t\tthe course is jumping well .\n",
            "m_02\t1\t\tone bomb did n't guide and crashed .\n",
            "m_02\t1\t\tfiona may be here by 5 o'clock .\n",
            "m_02\t1\t\tif fiona is here by 5 o'clock , we can go to the party .\n",
            "m_02\t1\t\tit 's high time fiona got a job .\n",
            "m_02\t0\t*\tit 's high time fiona gets a job .\n",
            "sgww85\t1\t\tpat is either stupid or a liar .\n",
            "sgww85\t1\t\tpat is a republican and proud of it .\n",
            "sgww85\t1\t\tpat is healthy and of sound mind .\n",
            "sgww85\t1\t\tpat is either asleep or at the office .\n",
            "sgww85\t1\t\tthat was a rude remark and in very bad taste .\n",
            "sgww85\t1\t\tsandy is either a lunatic or under the influence of drugs .\n",
            "sgww85\t1\t\ti am hoping to get an invitation and optimistic about my chances .\n",
            "sgww85\t1\t\ti am neither an authority on this subject nor trying to portray myself as one .\n",
            "sgww85\t1\t\tpat was neither recommended for promotion nor under any illusions about what that meant .\n",
            "sgww85\t1\t\tpat has become a banker and very conservative .\n",
            "sgww85\t1\t\ti consider that a rude remark and in very [ np and pp ] bad taste .\n",
            "sgww85\t1\t\tthe scene of the movie was in chicago .\n",
            "sgww85\t0\t*\tthe scene of the movie and that i wrote was in chicago .\n",
            "sgww85\t1\t\tjohn sang beautifully .\n",
            "sgww85\t1\t\tjohn sang a carol .\n",
            "sgww85\t0\t*\tjohn sang beautifully and a carol .\n",
            "sgww85\t1\t\tkim sang and sandy danced .\n",
            "sgww85\t1\t\tkim and sandy met .\n",
            "sgww85\t1\t\tkim sang and was accompanied by sandy .\n",
            "sgww85\t0\t*\tthe irritating and a bully man was my brother .\n",
            "sgww85\t0\t*\tsoon irritating and a bully started shouting again .\n",
            "sgww85\t1\t\tkim was a banker .\n",
            "sgww85\t1\t\tdana was quite competent .\n",
            "sgww85\t1\t\tleslie was in the flood zone .\n",
            "sgww85\t1\t\tronnie was talking to lou .\n",
            "sgww85\t1\t\tjean was given a prize .\n",
            "sgww85\t1\t\tpat has become a republican .\n",
            "sgww85\t1\t\tgerry became quite conservative .\n",
            "sgww85\t0\t*\tconnie has become of the opinion that we should get out .\n",
            "sgww85\t0\t*\ttracy became awarded a prize .\n",
            "sgww85\t0\t*\tchris will become talking to colleagues .\n",
            "sgww85\t1\t\tpat became a republican and quite conservative .\n",
            "sgww85\t0\t*\ttracy has become a republican and of the opinion that we must place nuclear weapons in europe .\n",
            "sgww85\t0\t*\tchris became quite conservative and trying to change their minds .\n",
            "sgww85\t0\t*\tgerry became a republican and awarded a prize .\n",
            "sgww85\t1\t\twe walked slowly and with great care .\n",
            "sgww85\t1\t\tthey wanted to leave tomorrow or on tuesday .\n",
            "sgww85\t1\t\twe are open saturdays , any national holiday , and on alternate .\n",
            "sgww85\t1\t\tkim alienates cats and beats his dog .\n",
            "sgww85\t1\t\tkim alienates cats and beat his dog .\n",
            "sgww85\t1\t\tkim alienated cats and beats his dog .\n",
            "sgww85\t1\t\tkim alienated cats and beat his dog .\n",
            "sgww85\t0\t*\tkim alienated cats and beaten his dog .\n",
            "sgww85\t0\t*\tkim beating his dog and alienates cats .\n",
            "sgww85\t0\t*\tkim to beat his dog and alienated cats .\n",
            "sgww85\t0\t*\tkim beaten his dog and alienates cats .\n",
            "sgww85\t1\t\twhich student 's grades went unreported ?\n",
            "sgww85\t1\t\tthey found pictures of themselves .\n",
            "sgww85\t0\t*\twho did you say my talking to would bother hilary ?\n",
            "sgww85\t1\t\twho did you say my talking to would bother ?\n",
            "sgww85\t1\t\twhich article did terry file without reading ?\n",
            "sgww85\t1\t\twhich books did robin read and hate ?\n",
            "sgww85\t0\t*\twhich books did robin talk to chris and read ?\n",
            "sgww85\t0\t*\twhich books did robin read and talk to chris ?\n",
            "sgww85\t0\t*\twho did robin visit and ?\n",
            "sgww85\t1\t\tthey talked to kim and to each other .\n",
            "sgww85\t1\t\the hated himself and his friends .\n",
            "sgww85\t1\t\tthey were wary of themselves and of each other .\n",
            "sgww85\t1\t\tthey asked which students and which teachers would get along together .\n",
            "sgww85\t1\t\twe called up every man whose father and whose mother had played on the team .\n",
            "sgww85\t1\t\ti went to the store and bought some whiskey .\n",
            "sgww85\t1\t\tshe 's gone and ruined her dress now .\n",
            "sgww85\t1\t\ti 've got to try and find that screw .\n",
            "sgww85\t1\t\tshe goes and buys some whiskey .\n",
            "sgww85\t1\t\ti have gone and bought some whiskey .\n",
            "sgww85\t1\t\ti will go and buy some whiskey .\n",
            "sgww85\t1\t\ti will try and buy some whiskey .\n",
            "sgww85\t0\t*\ti have gone and buys some whiskey .\n",
            "sgww85\t0\t*\tto go and buying whiskey is not the solution to your problem .\n",
            "sgww85\t0\t*\ti will go and bought some whiskey .\n",
            "sgww85\t0\t*\ti tried and buy some whiskey .\n",
            "sgww85\t0\t*\ti was trying and buying some whiskey .\n",
            "sgww85\t0\t*\twhat did you say i went and get ?\n",
            "sgww85\t0\t*\twhat did you say i go and got ?\n",
            "sgww85\t1\t\ti went to the store and i bought some whiskey .\n",
            "sgww85\t1\t\ti 've got to try and i 've got to find that screw .\n",
            "sgww85\t1\t\ti both went to the store and bought some whiskey .\n",
            "sgww85\t1\t\ti 've got to both try and find that screw .\n",
            "sgww85\t1\t\there 's the whiskey which i went to the store and bought .\n",
            "sgww85\t1\t\twhich dress has she gone and ruined now ?\n",
            "sgww85\t1\t\tthe screw which i 've got to try and find holds the door to the frame .\n",
            "sgww85\t1\t\teither we americans or i myself will get ourselves in trouble .\n",
            "sgww85\t1\t\teither you or i will incriminate ourselves .\n",
            "sgww85\t1\t\tyou and i may incriminate ourselves .\n",
            "sgww85\t1\t\twe americans and the british pamper ourselves .\n",
            "sgww85\t1\t\tyou british and you americans pamper yourselves .\n",
            "sgww85\t1\t\tyou british or you americans will get yourselves in trouble .\n",
            "sgww85\t1\t\tyou and kerry have outdone yourselves .\n",
            "sgww85\t1\t\tyou or kerry have perjured yourselves .\n",
            "sgww85\t1\t\tthe boys and the girls seem happy .\n",
            "sgww85\t0\t*\tthe boys and the girls seems happy .\n",
            "sgww85\t1\t\teither the boys or the girls are going to be there .\n",
            "sgww85\t1\t\tthe students and professor swansong are meeting in the park .\n",
            "sgww85\t1\t\teither professor swansong or the graduate students are going to proctor the exam .\n",
            "sgww85\t1\t\teither dana or lee is going to lead the parade .\n",
            "sgww85\t1\t\tkim and terry are happy .\n",
            "sgww85\t0\t*\teither the boys or the girls is going to be there .\n",
            "sgww85\t0\t*\tthe students and professor swansong is meeting in the park .\n",
            "sgww85\t0\t*\teither professor swansong or the graduate students is going to proctor the exam .\n",
            "sgww85\t1\t\teither dana or lee are going to lead the parade .\n",
            "sgww85\t1\t\tkim likes sandy , and lee leslie . to try to go to rome .\n",
            "sgww85\t1\t\tpat wanted to try to go to berne , and chris to go to rome . to rome .\n",
            "sgww85\t1\t\tkim went to the store , and then lou .\n",
            "sgww85\t1\t\tsome people go by car , but others by bike .\n",
            "sgww85\t1\t\tsome people like bagels , but others cream cheese .\n",
            "sgww85\t1\t\ton weekdays , terry eats meat and vegetables , but on weekends , only vegetables .\n",
            "sgww85\t0\t*\tjohn drinks coffee at 11 , and mary , tea at 10:30 .\n",
            "sgww85\t1\t\tjohn gave the books to mary at christmas , and the records to sue for her birthday .\n",
            "sgww85\t1\t\tjohn talked to his supervisor about his thesis , and erich to the dean about department politics .\n",
            "sgww85\t1\t\ta businessman will drink a martini to relax , and a health nut , a glass of wine , just to remain healthy .\n",
            "sgww85\t0\t*\tjohn left at 11 and at 12 , bill .\n",
            "sgww85\t1\t\tjohn left his office at 11 and at 12 , the library .\n",
            "sgww85\t1\t\ta policeman walked in at 11 , and at 12 , a fireman .\n",
            "sgww85\t1\t\ttwo days ago , we went out to dinner , and this afternoon , to the movies .\n",
            "sgww85\t1\t\ton this table , they put a lamp , and on that table , a radio .\n",
            "sgww85\t0\t*\tjohn did n't see mary and bill sue .\n",
            "sgww85\t1\t\tjohn did n't give the books to mary and the papers to sue .\n",
            "sgww85\t0\t*\tkim likes sandy , and lee to leslie .\n",
            "sgww85\t0\t*\tpat wanted to go to berne , and chris going to rome .\n",
            "sgww85\t0\t*\tkim gave a dollar to bobbie and a dime into his pocket .\n",
            "sgww85\t0\t*\tkim likes lee , and to ronnie .\n",
            "sgww85\t0\t*\tkim likes sandy and lee likes to leslie .\n",
            "sgww85\t1\t\tleslie is rather foolish , and lou a complete idiot .\n",
            "sgww85\t1\t\tkim seems to be just surviving , and terry in dire need of our help .\n",
            "sgww85\t1\t\twe consider leslie rather foolish , and lou a complete idiot .\n",
            "sgww85\t1\t\tpat has become crazy , and chris an incredible bore .\n",
            "sgww85\t0\t*\tpat has become crazy , and chris in good spirits .\n",
            "sgww85\t0\t*\ti gave a book to john 's mother and a magazine to him .\n",
            "sgww85\t1\t\tpat remembered the appointment and that it was important to be on time .\n",
            "sgww85\t1\t\tthat goldstein appointed heydrich and the implications thereof frightened many observers .\n",
            "sgww85\t1\t\twe talked about mr. colson and that he had worked at the .\n",
            "sgww85\t1\t\tyou can depend on my assistant and that he will be on time .\n",
            "sgww85\t1\t\tpat was annoyed by the children 's noise and that their parents did nothing to stop it .\n",
            "sgww85\t0\t*\twe talked about that he had worked at the white house .\n",
            "sgww85\t0\t*\tyou can depend on that he will be on time .\n",
            "sgww85\t0\t*\tpat was annoyed by that their parents did nothing to stop it .\n",
            "sgww85\t1\t\twe talked about the issues we had worked on as students and that our perspectives had changed over the years .\n",
            "sgww85\t0\t*\twe talked about that our perspectives had changed over the years and the issues we had worked on as students .\n",
            "sgww85\t1\t\tthat our perspectives had changed over the years and the issues we had worked on as students were the topics of discussion .\n",
            "sks13\t1\t\tthe clever snake disappeared into a hole in the ground .\n",
            "sks13\t0\t*\thole into disappeared ground the the in clever a little .\n",
            "sks13\t0\t*\tthe snake clever disappeared into a hole in the ground .\n",
            "sks13\t0\t*\tthis girl in the red coat will put \ba picture of bill it on your desk before tomorrow .\n",
            "sks13\t0\t*\tthis girl in the red coat will put a picture of bill \bon your desk there before tomorrow .\n",
            "sks13\t0\t*\tthis \bgirl in the red coat one will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t0\t*\tthis girl in the red coat will put a picture of bill on your desk it before tomorrow .\n",
            "sks13\t1\t\tthis girl in the red coat will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t1\t\tbill will put a picture of this girl in the red coat on your desk before tomorrow .\n",
            "sks13\t1\t\tshe will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t0\t*\tbill will put a picture of she on your desk before tomorrow .\n",
            "sks13\t1\t\tbill will put a picture of her on your desk before tomorrow .\n",
            "sks13\t0\t*\ther will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t0\t*\tshe her will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t0\t*\tbill will put a picture of she her on your desk before tomorrow .\n",
            "sks13\t1\t\tclean your desk before tomorrow .\n",
            "sks13\t1\t\tthis girl will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t1\t\tthis boy must not go to school , and his father must not go to school either .\n",
            "sks13\t1\t\tthis boy must not go to france , but his father must go to france .\n",
            "sks13\t1\t\tthis actress must play in this movie and she will play in this movie .\n",
            "sks13\t1\t\tcan mary win the race and will sue win the race too ?\n",
            "sks13\t1\t\tthis girl will buy bread and so will that one buy bread .\n",
            "sks13\t1\t\tthe tourists will go to the park .\n",
            "sks13\t1\t\twill the tourists go to the park ?\n",
            "sks13\t1\t\tsome student from australia speaks chinese .\n",
            "sks13\t1\t\tdoes some student from australia speak chinese ?\n",
            "sks13\t1\t\tthey would have been walking for hours .\n",
            "sks13\t1\t\twould they have been walking for hours ?\n",
            "sks13\t1\t\tthis girl will not buy bread , will she buy bread ?\n",
            "sks13\t1\t\tsean penn can act well in many kinds of movies , ca n't he act well in many kinds of movies ?\n",
            "sks13\t1\t\tyou will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t1\t\tthis girl in the red coat or you will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t1\t\tno boys will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t1\t\tthis girl in the red coat but no boys will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t1\t\tthis girl in the red coat will put it and a picture of bill on your desk before tomorrow .\n",
            "sks13\t1\t\tthis girl in the red coat will put a picture of bill in the mailbox before tomorrow .\n",
            "sks13\t1\t\tthis girl in the red coat will put a picture of bill on your desk after the dinner .\n",
            "sks13\t1\t\tthis girl in the red coat will put a picture of bill on your desk after the dinner and before tomorrow .\n",
            "sks13\t1\t\tthis girl in the red coat will eat her breakfast before tomorrow .\n",
            "sks13\t1\t\tthis girl in the red coat will eat her breakfast before tomorrow and put a picture of bill on your desk before tomorrow .\n",
            "sks13\t1\t\tthis girl in the red coat will eat her breakfast and will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t1\t\tthis girl in the red coat will put a picture of bill on your desk .\n",
            "sks13\t1\t\tthis girl in the red dress must put a picture of bill on your desk .\n",
            "sks13\t0\t*\tthis girl in the red coat will and dress must put a picture of bill on your desk .\n",
            "sks13\t0\t*\tthis girl in the or on the red coat will put a picture of bill on your desk .\n",
            "sks13\t1\t\tjohn and mary will play with henry and with sue .\n",
            "sks13\t1\t\tthey play unusual music , and i listen to unusual music .\n",
            "sks13\t1\t\tthey play and i listen to unusual music .\n",
            "sks13\t1\t\ti love ice milk tea but you hate ice milk tea .\n",
            "sks13\t1\t\ti love but you hate ice milk tea .\n",
            "sks13\t1\t\tshe may have thawed the roast and should have thawed the roast .\n",
            "sks13\t1\t\tshe may have and should have thawed the roast .\n",
            "sks13\t1\t\tsmith loaned a valuable collection of manuscripts to the library , and his widow later donated a valuable collection of manuscripts to the library .\n",
            "sks13\t1\t\tsmith loaned and his widow later donated a valuable collection of manuscripts to the library .\n",
            "sks13\t1\t\ti borrowed large sums of money from the bank , and my sister stole large sums of money from the bank .\n",
            "sks13\t1\t\ti borrowed and my sister stole large sums of money from the bank .\n",
            "sks13\t0\t*\tput a picture of bill on your desk , this girl in the red coat will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t1\t\tmary should know that you must go to the station .\n",
            "sks13\t1\t\tthat you must go to the station , mary should know that you must go to the station .\n",
            "sks13\t0\t*\tthis your , this girl in the red coat will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t0\t*\twill bill , this girl in the red coat will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t0\t*\tred picture desk , this girl in the red coat will put a picture of .\n",
            "sks13\t0\t*\tbefore your , this girl in the red coat will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t0\t*\tgirl in the red coat , this girl in the red coat will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t0\t*\twill put a picture of bill on your desk before tomorrow , this girl in the red coat will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t0\t*\tthe red , this girl in the red coat will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t0\t*\tof bill on , this girl in the red coat will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t0\t*\twill put , this girl in the red coat will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t0\t*\tyour desk before , this girl in the red coat will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t1\t\tit is your notes that john wants to look at after class .\n",
            "sks13\t1\t\tit is after class that john wants to look at your notes .\n",
            "sks13\t1\t\tit is john who wants to look at your notes after class .\n",
            "sks13\t1\t\tit was ann who bought a first edition of richard iii for $ 1000 .\n",
            "sks13\t1\t\tit was a first edition of richard iii that ann bought for $ 1000 .\n",
            "sks13\t1\t\tit was for $ 1000 that ann bought a first edition of richard iii .\n",
            "sks13\t0\t*\tit is before tomorrow that this girl in the red coat will put a picture of bill on your desk before tomorrow .\n",
            "sks13\t1\t\tmary saw the tall man coming from england .\n",
            "sks13\t0\t*\tit is the tall man coming from england that mary saw the tall man coming from england .\n",
            "sks13\t1\t\tmary saw the tall man come from the back .\n",
            "sks13\t0\t*\tit is the tall man come from the back that mary saw the tall man come from the back .\n",
            "sks13\t1\t\tit is a picture of bill that this girl in the red coat will put on your desk before tomorrow .\n",
            "sks13\t0\t*\tit is put a picture of bill on your desk before tomorrow that this girl in the red coat will .\n",
            "sks13\t1\t\twhat john wants to look at now is your notes .\n",
            "sks13\t0\t*\twhat mary gave was a book to john .\n",
            "sks13\t0\t*\twhat mary donated was a lot of money to npr .\n",
            "sks13\t1\t\tit is to cleveland that john drove the truck .\n",
            "sks13\t1\t\twhat john became was deadly afraid of flying .\n",
            "sks13\t0\t*\tit is deadly afraid of flying that john became .\n",
            "sks13\t1\t\tjohn told us that he wants to quit school .\n",
            "sks13\t0\t*\tit is that he wants to quit school that john told us .\n",
            "sks13\t1\t\twhat john told us is that he wants to quit school .\n",
            "sks13\t1\t\tjohn promised us to be gentle .\n",
            "sks13\t0\t*\tit is to be gentle that john promised .\n",
            "sks13\t1\t\tmary will arrive tomorrow .\n",
            "sks13\t0\t*\tit is arrive tomorrow that mary will .\n",
            "sks13\t1\t\thenri wants the book which is on the top shelf .\n",
            "sks13\t1\t\twhat henri wants is the book which is on the top shelf .\n",
            "sks13\t1\t\tthe spy became too friendly with his new contacts .\n",
            "sks13\t1\t\twhat the spy became was too friendly with his new contacts .\n",
            "sks13\t1\t\twhat this girl in the red coat will do is put a picture of bill on your desk before tomorrow .\n",
            "sks13\t1\t\thenri wants to buy these books about cooking .\n",
            "sks13\t1\t\twhich books about cooking does henri want to buy ?\n",
            "sks13\t1\t\ti sent it to you .\n",
            "sks13\t0\t*\ti sent to you it .\n",
            "sks13\t0\t*\ti sent to you recipes .\n",
            "sks13\t1\t\tbill 's mother 's friends are waiting at the restaurant .\n",
            "sks13\t1\t\tbill 's mother 's friends and john are waiting at the restaurant .\n",
            "sks13\t1\t\tit was john that was waiting at the restaurant .\n",
            "sks13\t0\t*\tit was john bill that were waiting at the restaurant .\n",
            "sks13\t1\t\tit was john and bill that were waiting at the restaurant .\n",
            "sks13\t1\t\ti will eat spaghetti on sunday with marco .\n",
            "sks13\t1\t\ti will speak to hector about this .\n",
            "sks13\t1\t\ti doubt that mary reads mysteries .\n",
            "sks13\t1\t\the muttered that the visitors will leave .\n",
            "sks13\t1\t\tthe fact that john is snoring is informative .\n",
            "sks13\t1\t\tthe man that mary saw knew me .\n",
            "sks13\t1\t\tthat the visiting team won the race could surprise them .\n",
            "sks13\t1\t\tthat is what you should see .\n",
            "sks13\t1\t\tjohn knows that she left .\n",
            "sks13\t1\t\tjohn knows whether she will come back .\n",
            "sks13\t1\t\tjohn knows that she left and whether she will come back .\n",
            "sks13\t1\t\tjohn knows that she left and john knows whether she will come back .\n",
            "sks13\t1\t\tjohn asked whether she left .\n",
            "sks13\t1\t\ti doubt if she kicks perfect goals every time .\n",
            "sks13\t1\t\tthey think that she can do it .\n",
            "sks13\t1\t\twhether she left is most unclear .\n",
            "sks13\t1\t\tthat the girl put a picture there proves her guilt .\n",
            "sks13\t1\t\ti prefer for the girl to put a picture there .\n",
            "sks13\t1\t\tfor the girl to put a picture there is what i prefer .\n",
            "sks13\t1\t\tfor the girl to put a picture there would surprise you .\n",
            "sks13\t1\t\ti prefer for the girl to win .\n",
            "sks13\t0\t*\ti prefer for the girl to will win .\n",
            "sks13\t0\t*\ti prefer for the girl to wins .\n",
            "sks13\t1\t\tlet 's walk .\n",
            "sks13\t1\t\ti run on the beach .\n",
            "sks13\t1\t\tthe three sunbathers went swimming .\n",
            "sks13\t1\t\ti hope that mary wins .\n",
            "sks13\t1\t\tthey know if mary won .\n",
            "sks13\t1\t\ti wonder whether mary will win .\n",
            "sks13\t1\t\tthey prefer for mary to leave .\n",
            "sks13\t1\t\tjohn wonders whether mary will win .\n",
            "sks13\t1\t\tjohn wonders whether to win .\n",
            "sks13\t1\t\twhether she will win is a question mary never considered .\n",
            "sks13\t1\t\twhether to win is a question mary never considered .\n",
            "sks13\t1\t\ti think that you will see that the girl will put a picture on your desk .\n",
            "sks13\t1\t\tthey understand that you will prefer for the girl to put a picture on your desk .\n",
            "sks13\t1\t\tmary cuts the paper easily .\n",
            "sks13\t1\t\tthe paper cuts easily .\n",
            "sks13\t1\t\tthat he won the race could surprise them .\n",
            "sks13\t0\t*\tthat him won the race could surprise them .\n",
            "sks13\t1\t\tfor him to win the race would surprise them .\n",
            "sks13\t0\t*\tfor he to win the race would surprise them .\n",
            "sks13\t1\t\tjohn saw mary .\n",
            "sks13\t1\t\tharry likes movies .\n",
            "sks13\t1\t\tfor mary to leave on time is important .\n",
            "sks13\t0\t*\tthink about linguistics all night , she does think about linguistics all night .\n",
            "sks13\t0\t*\tclimb to the top , they do climb to the top .\n",
            "sks13\t1\t\tjohn can go to the market on his bike .\n",
            "sks13\t1\t\tmary should buy some flowers on sunday .\n",
            "sks13\t1\t\tmy niece could write me letters before her third birthday .\n",
            "sks13\t1\t\tmy nephew could write letters to his parents with a fountain pen .\n",
            "sks13\t1\t\tjohn can go to the market quickly .\n",
            "sks13\t1\t\tmary should buy some flowers for her mother to arrange .\n",
            "sks13\t1\t\tmy niece could write me letters more faithfully .\n",
            "sks13\t1\t\tjohn can quickly go to the market .\n",
            "sks13\t1\t\tmy niece could more faithfully write me letters .\n",
            "sks13\t0\t*\tjohn can go to the market to india .\n",
            "sks13\t0\t*\tmary should buy some flowers some bread .\n",
            "sks13\t0\t*\tmy niece could write me you letters .\n",
            "sks13\t0\t*\tmy nephew could write letters the postcards to his parents .\n",
            "sks13\t1\t\tjohn can go to the market on his bike on a truck .\n",
            "sks13\t1\t\tmary should buy some flowers on sunday at 5 o'clock .\n",
            "sks13\t1\t\tmy nephew could write letters to his parents with a fountain pen with your help .\n",
            "sks13\t1\t\tpelé visited his uncle .\n",
            "sks13\t1\t\tshe sold the car to sam for five dollars .\n",
            "sks13\t1\t\tshe ran the car on propane from reno to vegas .\n",
            "sks13\t1\t\tthe process changed the substance from solid to liquid to gas to energy .\n",
            "sks13\t1\t\twe associated their subsidiaries with our corporate office .\n",
            "sks13\t1\t\ti cycled around france .\n",
            "sks13\t1\t\tmary drank some beer in the barn from 6 to nine .\n",
            "sks13\t1\t\tit was in the barn or it took place in the barn .\n",
            "sks13\t0\t*\tit was some beer or it took place some beer .\n",
            "sks13\t1\t\tthey wonder whether mary will run .\n",
            "sks13\t1\t\tthey wonder about this .\n",
            "sks13\t1\t\tthey wonder .\n",
            "sks13\t1\t\ti know that she runs .\n",
            "sks13\t1\t\ti know .\n",
            "sks13\t1\t\ti said that she runs .\n",
            "sks13\t1\t\ti said that .\n",
            "sks13\t0\t*\ti said .\n",
            "sks13\t1\t\ti prefer for mary to run .\n",
            "sks13\t1\t\ti prefer this .\n",
            "sks13\t0\t*\ti prefer .\n",
            "sks13\t1\t\ti said for mary to run .\n",
            "sks13\t1\t\ti said this .\n",
            "sks13\t1\t\ti put the book on the shelf .\n",
            "sks13\t0\t*\ti put the book .\n",
            "sks13\t0\t*\ti put .\n",
            "sks13\t1\t\ttwo ships appeared , arrived , remained , emerged .\n",
            "sks13\t1\t\tsuddenly , there appeared two ships on the horizon .\n",
            "sks13\t1\t\ttwo inspectors from the ins appeared , arrived , remained , emerged .\n",
            "sks13\t1\t\tthe ice melts , breaks .\n",
            "sks13\t1\t\tthe door opens , closes .\n",
            "sks13\t1\t\tthey melted , broke the ice .\n",
            "sks13\t1\t\tthey opened , closed the door .\n",
            "sks13\t1\t\tthey cooked , thickened the soup .\n",
            "sks13\t1\t\ti go , run , swim , jump , fly , crawl , dance , walk .\n",
            "sks13\t0\t*\tthey went me , ran me , swam me , jumped me , flew me , crawled me , danced me , walked me .\n",
            "sks13\t1\t\ti danced a dance .\n",
            "sks13\t1\t\the walked the walk .\n",
            "sks13\t1\t\tthe time elapsed slowly .\n",
            "sks13\t0\t*\tthe time elapsed the day .\n",
            "sks13\t1\t\ti see stars .\n",
            "sks13\t1\t\ti see .\n",
            "sks13\t1\t\ti liked mary .\n",
            "sks13\t0\t*\ti liked .\n",
            "sks13\t1\t\tthey surrounded the fort .\n",
            "sks13\t0\t*\tthey surrounded .\n",
            "sks13\t1\t\ti gave the charity .\n",
            "sks13\t1\t\ti gave money .\n",
            "sks13\t1\t\ti gave .\n",
            "sks13\t1\t\ti handed the ball to reg .\n",
            "sks13\t0\t*\ti handed the ball .\n",
            "sks13\t0\t*\ti handed to reg .\n",
            "sks13\t0\t*\ti handed .\n",
            "sks13\t1\t\tjohn ate .\n",
            "sks13\t1\t\tjohn knows .\n",
            "sks13\t0\t*\tjohn needed .\n",
            "sks13\t0\t*\tjohn criticized .\n",
            "sks13\t1\t\tjohn saw .\n",
            "sks13\t1\t\tjohn told .\n",
            "sks13\t1\t\tthe agency classified the documents .\n",
            "sks13\t0\t*\tthe agency classified .\n",
            "sks13\t1\t\tthe war intensified the poverty .\n",
            "sks13\t1\t\tthis project is manageable .\n",
            "sks13\t1\t\tit mattered on sunday .\n",
            "sks13\t1\t\ti saw john on sunday .\n",
            "sks13\t1\t\ti put the book on the desk on sunday .\n",
            "sks13\t1\t\ti saw john with a telescope .\n",
            "sks13\t0\t*\tit mattered with a telescope .\n",
            "sks13\t1\t\ti covered the bread with butter .\n",
            "sks13\t0\t*\ti emptied it with butter .\n",
            "sks13\t1\t\tmary will complete her exam within an hour .\n",
            "sks13\t0\t*\tmary will complete her exam for an hour .\n",
            "sks13\t1\t\tthe hiker will reach the top of the mountain within an hour .\n",
            "sks13\t0\t*\tthe hiker will reach the top of the mountain for an hour .\n",
            "sks13\t1\t\thenri will paint the floor for an hour .\n",
            "sks13\t1\t\ti will read linguistics for an hour .\n",
            "sks13\t1\t\tthe student left .\n",
            "sks13\t1\t\tonly the student left .\n",
            "sks13\t1\t\teven the student left .\n",
            "sks13\t1\t\tall the students left .\n",
            "sks13\t1\t\ti saw the student .\n",
            "sks13\t1\t\ti saw only the student .\n",
            "sks13\t1\t\ti saw all the students .\n",
            "sks13\t1\t\tjohn , who i saw yesterday , will visit us .\n",
            "sks13\t1\t\ti saw the brilliant student .\n",
            "sks13\t1\t\ti saw the brilliant one .\n",
            "sks13\t1\t\ti saw the brilliant student with long hair .\n",
            "sks13\t1\t\ti saw the brilliant one with long hair .\n",
            "sks13\t1\t\ti saw the one with long hair .\n",
            "sks13\t1\t\ti saw the physics student .\n",
            "sks13\t0\t*\ti saw the physics one .\n",
            "sks13\t1\t\ti saw the student of physics .\n",
            "sks13\t0\t*\ti saw the one of physics .\n",
            "sks13\t1\t\ti saw the student of physics with long hair .\n",
            "sks13\t1\t\tthe big student of physics with long hair in the library .\n",
            "sks13\t1\t\tit is big .\n",
            "sks13\t1\t\tit is with long hair .\n",
            "sks13\t0\t*\tit is of physics .\n",
            "sks13\t1\t\tit is in the library .\n",
            "sks13\t1\t\tthey are intense .\n",
            "sks13\t0\t*\tthey are intense of bill .\n",
            "sks13\t1\t\tthey intensified .\n",
            "sks13\t1\t\tthey are special .\n",
            "sks13\t0\t*\tthey are special of bill .\n",
            "sks13\t1\t\tthey specialized .\n",
            "sks13\t1\t\tshe is proud .\n",
            "sks13\t1\t\tshe is the mother .\n",
            "sks13\t1\t\tshe is the mother of john .\n",
            "sks13\t1\t\tthey read the paper .\n",
            "sks13\t1\t\tthe paper is readable .\n",
            "sks13\t0\t*\tit is readable of the paper .\n",
            "sks13\t0\t*\tthey are readable of the paper .\n",
            "sks13\t1\t\tthe driver of the car thinks that mary should leave dallas for boise tomorrow .\n",
            "sks13\t1\t\ther little sister will disagree with her .\n",
            "sks13\t1\t\tthe girl he met at the departmental party will very surely call him .\n",
            "sks13\t1\t\tbeavers build dams .\n",
            "sks13\t1\t\tjohn will see you .\n",
            "sks13\t1\t\tjohn thinks that mary left .\n",
            "sks13\t1\t\tjohn thinks mary left .\n",
            "sks13\t1\t\tjohn whispered that mary left .\n",
            "sks13\t1\t\tjohn will carefully study russian .\n",
            "sks13\t1\t\tjohn carefully studies russian .\n",
            "sks13\t0\t*\tjohn studies carefully russian .\n",
            "sks13\t1\t\ti wonder if she will use paints .\n",
            "sks13\t1\t\tyes , she will .\n",
            "sks13\t0\t*\tyes , she .\n",
            "sks13\t0\t*\tyes , she will use .\n",
            "sks13\t1\t\ti wonder if she used paints .\n",
            "sks13\t1\t\tyes , she did .\n",
            "sks13\t0\t*\tyes , she used .\n",
            "sks13\t1\t\tjohn will have been eating cake .\n",
            "sks13\t0\t*\tmary wo n't have been eating cake , but john .\n",
            "sks13\t1\t\tmary wo n't have been eating cake , but john will .\n",
            "sks13\t1\t\tmary wo n't have been eating cake , but john will have .\n",
            "sks13\t1\t\tmary wo n't have been eating cake , but john will have been .\n",
            "sks13\t1\t\tjohn will enthusiastically have been eating cake .\n",
            "sks13\t1\t\tjohn will have enthusiastically been eating cake .\n",
            "sks13\t0\t*\tjohn will have been eating enthusiastically cake .\n",
            "sks13\t1\t\tjohn will have been eating cake enthusiastically .\n",
            "sks13\t0\t*\tjohn studied carefully russian .\n",
            "sks13\t1\t\tjohn has carefully studied russian .\n",
            "sks13\t1\t\tjohn had carefully studied russian .\n",
            "sks13\t1\t\tjohn is carefully studying russian .\n",
            "sks13\t1\t\tjohn was carefully studying russian .\n",
            "sks13\t1\t\tjohn goes to school .\n",
            "sks13\t0\t*\tgoes john to school ?\n",
            "sks13\t1\t\tmary thinks that bill will come .\n",
            "sks13\t0\t*\tmary thinks whether bill will come .\n",
            "sks13\t0\t*\tmary thinks for bill to come .\n",
            "sks13\t1\t\tmary wonders whether bill will come .\n",
            "sks13\t0\t*\tmary wonders for bill to come .\n",
            "sks13\t0\t*\tmary prefers that bill will come .\n",
            "sks13\t0\t*\tmary prefers whether bill will come .\n",
            "sks13\t1\t\tmary prefers for bill to come .\n",
            "sks13\t1\t\ti wonder has mary worked for microsoft .\n",
            "sks13\t1\t\ti wonder whether mary has worked for microsoft .\n",
            "sks13\t0\t*\ti wonder whether has mary worked for microsoft .\n",
            "sks13\t0\t*\ti wonder has whether mary worked for microsoft .\n",
            "sks13\t1\t\twill john not go to school ?\n",
            "sks13\t1\t\thas henri not studied for his exam ?\n",
            "sks13\t1\t\tdid sue not pass her exam ?\n",
            "sks13\t1\t\two n't john go to school ?\n",
            "sks13\t1\t\tshould n't mary taste the soup ?\n",
            "sks13\t1\t\thas n't henri studied for his exam ?\n",
            "sks13\t1\t\tis n't bill sick ?\n",
            "sks13\t1\t\tdid n't sue pass her exam ?\n",
            "sks13\t0\t*\twill not john go to school ?\n",
            "sks13\t0\t*\tshould not mary taste the soup ?\n",
            "sks13\t0\t*\thas not henri studied for his exam ?\n",
            "sks13\t0\t*\tis not bill sick ?\n",
            "sks13\t0\t*\tdid not sue pass her exam ?\n",
            "sks13\t0\t*\tsue put .\n",
            "sks13\t0\t*\thenri arrived bill .\n",
            "sks13\t0\t*\tmary wonders that john said if bill left .\n",
            "sks13\t0\t*\thenri told sue in the drawer that bill put socks .\n",
            "sks13\t1\t\tshe will win the race .\n",
            "sks13\t0\t*\ther will the race .\n",
            "sks13\t1\t\telmer finished the cake and john did too , finish the cake .\n",
            "sks13\t1\t\twe need to provide two trees and .\n",
            "sks13\t1\t\twe also need to explain the relation between these trees .\n",
            "sks13\t0\t*\tjohn not liked mary .\n",
            "sks13\t0\t*\tjohn liked not mary .\n",
            "sks13\t1\t\tjohn did not like mary .\n",
            "sks13\t1\t\tjohn will endorse the treaty , but georges will not endorse the treaty .\n",
            "sks13\t1\t\twill george indeed not endorse the treaty ?\n",
            "sks13\t0\t*\the will indeed not endorse the treaty .\n",
            "sks13\t1\t\the will indeed endorse the treaty .\n",
            "sks13\t1\t\the will not endorse the treaty ; and indeed .\n",
            "sks13\t1\t\tjohn thinks that bill left .\n",
            "sks13\t1\t\tjohn asked whether bill left .\n",
            "sks13\t1\t\tjohn was wondering whether to leave or not .\n",
            "sks13\t1\t\tjohn was wondering whether to leave .\n",
            "sks13\t0\t*\ti read these big three books .\n",
            "sks13\t0\t*\tmary sent .\n",
            "sks13\t1\t\tmary sent a book to bill .\n",
            "sks13\t1\t\tmary send a book .\n",
            "sks13\t1\t\tmary sent bill a book , … .\n",
            "sks13\t0\t*\tbill examined a book .\n",
            "sks13\t0\t*\tsincerity examined a book .\n",
            "sks13\t0\t*\twe put .\n",
            "sks13\t1\t\twe put a book on the table .\n",
            "sks13\t1\t\twe think that bill left .\n",
            "sks13\t0\t*\twe think for bill left .\n",
            "sks13\t0\t*\twe think if bill left .\n",
            "sks13\t1\t\twe wonder whether bill left .\n",
            "sks13\t1\t\twe wonder if bill left .\n",
            "sks13\t0\t*\twe wonder that bill left .\n",
            "sks13\t1\t\tjohn came in .\n",
            "sks13\t1\t\tthen , john left .\n",
            "sks13\t1\t\the took his umbrella .\n",
            "sks13\t1\t\the hurt himself with it when he tried to open it .\n",
            "sks13\t1\t\tthe idiot ca n't even open an umbrella !\n",
            "sks13\t0\t*\tjohn hurt john with john 's umbrella when john tried to open it .\n",
            "sks13\t1\t\tjohn ca n't even open an umbrella !\n",
            "sks13\t1\t\tjohn said he was sick .\n",
            "sks13\t1\t\tthe ta who graded him says that john did really well .\n",
            "sks13\t0\t*\thimself should decide soon .\n",
            "sks13\t0\t*\tmary wrote a letter to himself last year .\n",
            "sks13\t1\t\the should decide soon .\n",
            "sks13\t1\t\tmary wrote a letter to him last year .\n",
            "sks13\t1\t\tour rabbit and the neighbor 's cat like each other .\n",
            "sks13\t1\t\tthe boys fought with each other .\n",
            "sks13\t1\t\teach of our rabbit and the neighbor 's cat likes the other .\n",
            "sks13\t1\t\teach of the boys fought with the other boys .\n",
            "sks13\t1\t\tthe boy likes himself .\n",
            "sks13\t0\t*\tthe boy likes herself .\n",
            "sks13\t0\t*\tthe boy likes themselves .\n",
            "sks13\t1\t\tthe girls likes themselves .\n",
            "sks13\t0\t*\tthe girls likes herself .\n",
            "sks13\t1\t\teach of the girls likes herself .\n",
            "sks13\t0\t*\tthe girls likes yourselves .\n",
            "sks13\t0\t*\thimself likes john .\n",
            "sks13\t1\t\tmary 's pictures of herself surprised bill .\n",
            "sks13\t1\t\ti noticed john 's excessive appreciation of himself .\n",
            "sks13\t1\t\tmary noticed john 's excessive appreciation of himself .\n",
            "sks13\t0\t*\tmary noticed john 's excessive appreciation of herself .\n",
            "sks13\t0\t*\tmary noticed that john excessively appreciates herself .\n",
            "sks13\t1\t\tjohn loved the new pictures of himself .\n",
            "sks13\t1\t\ti showed mary several portraits of herself .\n",
            "sks13\t0\t*\tjohn believes that mary saw himself .\n",
            "sks13\t1\t\tmary noticed that john excessively appreciates himself .\n",
            "sks13\t1\t\tmary appreciates only john and herself .\n",
            "sks13\t0\t*\tmary appreciates john and himself .\n",
            "sks13\t1\t\tmary really appreciates and constantly praises herself and sue knows it .\n",
            "sks13\t0\t*\tmary really appreciates and constantly praises himself and bill knows it .\n",
            "sks13\t1\t\tjohn heard their criticism of each other .\n",
            "sks13\t1\t\tjohn heard their criticism of themselves .\n",
            "sks13\t0\t*\tthey heard john 's criticism of each other .\n",
            "sks13\t0\t*\tthey heard john 's criticism of themselves .\n",
            "sks13\t0\t*\tjohn heard that they criticized each other .\n",
            "sks13\t0\t*\tthey heard that john criticized each other .\n",
            "sks13\t1\t\tjohn likes himself .\n",
            "sks13\t1\t\tthe students are proud of themselves .\n",
            "sks13\t1\t\teveryone likes himself .\n",
            "sks13\t1\t\tno spy betrayed himself .\n",
            "sks13\t1\t\ti heard john 's criticism of himself .\n",
            "sks13\t0\t*\ti heard john 's criticism of myself .\n",
            "sks13\t1\t\tjohn heard that i criticized myself .\n",
            "sks13\t0\t*\ti heard that john criticized myself .\n",
            "sks13\t1\t\tmary likes herself .\n",
            "sks13\t0\t*\tour rabbit and the neighbor 's cat like them .\n",
            "sks13\t0\t*\tbill likes herself .\n",
            "sks13\t0\t*\thimself laughs .\n",
            "sks13\t1\t\tthe girls likes them .\n",
            "sks13\t1\t\tjohn 's mother likes him .\n",
            "sks13\t1\t\tjohn believes that bill saw himself .\n",
            "sks13\t1\t\tjohn believes that bill saw him .\n",
            "sks13\t0\t*\tmary believes that bill saw herself .\n",
            "sks13\t1\t\tthey like their books .\n",
            "sks13\t1\t\teveryone thinks he is smart .\n",
            "sks13\t1\t\twho in this class thinks he is smart ?\n",
            "sks13\t1\t\tbill 's mother saw him .\n",
            "sks13\t0\t*\tno one 's mother saw himself .\n",
            "sks13\t1\t\tthe mayor of john 's hometown wrote to him .\n",
            "sks13\t1\t\tthe builder of his house visited peter .\n",
            "sks13\t1\t\tthat is a bird .\n",
            "sks13\t1\t\tthat 's the truth .\n",
            "sks13\t1\t\the is john .\n",
            "sks13\t1\t\tbob dylan is robert zimmerman .\n",
            "sks13\t1\t\ti like mary and she likes me .\n",
            "sks13\t0\t*\ti like mary and she does too .\n",
            "sks13\t0\t*\ti like mary and she does like mary too .\n",
            "sks13\t1\t\tshe considers john proud of his work .\n",
            "sks13\t1\t\tthey saw bill leave .\n",
            "sks13\t1\t\tmary prefers that her ice cream is in a cone .\n",
            "sks13\t1\t\thenry saw that bill left .\n",
            "sks13\t1\t\twhat mary prefers is her ice cream in a cone .\n",
            "sks13\t0\t*\twhat she considers is john proud of his work .\n",
            "sks13\t0\t*\twhat henry found is bill sad .\n",
            "sks13\t0\t*\twhat they saw is bill leave .\n",
            "sks13\t0\t*\twhat henry find was bill sad .\n",
            "sks13\t0\t*\tjohn heard mary describe himself .\n",
            "sks13\t1\t\tjohn heard mary describe herself .\n",
            "sks13\t0\t*\tmary considers john proud of herself .\n",
            "sks13\t1\t\tmary considers john proud of her .\n",
            "sks13\t1\t\tmary considers john proud of himself .\n",
            "sks13\t1\t\tjohn believes himself to be proud of mary .\n",
            "sks13\t1\t\tthe pictures of bill she put on your desk .\n",
            "sks13\t1\t\twhich pictures of bill did she put on your desk .\n",
            "sks13\t1\t\tsusan wanted to sleep .\n",
            "sks13\t1\t\tshe put the pictures of bill on your desk .\n",
            "sks13\t1\t\tthe pictures of bill , she put on your desk .\n",
            "sks13\t0\t*\tthe picture of bill she slept .\n",
            "sks13\t0\t*\tshe slept the picture of bill .\n",
            "sks13\t1\t\tyou put which picture of bill on his desk ?\n",
            "sks13\t1\t\twhich picture of bill did you put on his desk ?\n",
            "sks13\t1\t\thow many strings did you say she had to pull in order to do that ?\n",
            "sks13\t1\t\thow much care do you think he would be taking of his patients under those circumstances ?\n",
            "sks13\t1\t\thow much headway is he likely to make .\n",
            "sks13\t1\t\twho left bill .\n",
            "sks13\t0\t*\twhom left bill .\n",
            "sks13\t1\t\twho did bill leave .\n",
            "sks13\t1\t\twhom did bill leave .\n",
            "sks13\t1\t\tis there anything to do today ?\n",
            "sks13\t1\t\tthere are two main characters in the novel .\n",
            "sks13\t1\t\tthere are 3 firemen available .\n",
            "sks13\t0\t*\tthere stabbed an animal .\n",
            "sks13\t0\t*\tthere ran many people .\n",
            "sks13\t0\t*\tmary judged there .\n",
            "sks13\t0\t*\ti had a realization of there .\n",
            "sks13\t1\t\tthere were seven people .\n",
            "sks13\t1\t\tthere were several doctors available .\n",
            "sks13\t1\t\trodney was eating some squid , was n't he ?\n",
            "sks13\t1\t\tthere is a man ready to jump from the roof , is n't there ?\n",
            "sks13\t1\t\tsharks seem to swim slowly in the tropics .\n",
            "sks13\t1\t\tthe cat seems to be out of the bag .\n",
            "sks13\t1\t\tthe shit seems to have hit the fan .\n",
            "sks13\t0\t*\tthere run many people .\n",
            "sks13\t1\t\tthere seems to be a nurse available .\n",
            "sks13\t0\t*\tthere seems to stab an animal .\n",
            "sks13\t0\t*\tthere seems to run many people to the station .\n",
            "sks13\t1\t\tit seems that john left .\n",
            "sks13\t1\t\tseveral people seem sick .\n",
            "sks13\t1\t\tjohn considers several people sick .\n",
            "sks13\t1\t\tthere are several people sick .\n",
            "sks13\t1\t\tseveral people seem several people sick .\n",
            "sks13\t1\t\tseveral people are sick .\n",
            "sks13\t1\t\tbill is sick .\n",
            "sks13\t1\t\tsusan hopes to sleep .\n",
            "sks13\t1\t\tsusan hopes that she will sleep .\n",
            "sks13\t0\t*\tsusan hopes susan to sleep .\n",
            "sks13\t0\t*\teveryone hopes him to sleep .\n",
            "sks13\t1\t\teveryone hopes to sleep .\n",
            "sks13\t1\t\teveryone hopes that everyone will sleep .\n",
            "sks13\t0\t*\tsusan hopes her to sleep .\n",
            "sks13\t1\t\tonly churchill remembered giving the blood , sweat and tears speech .\n",
            "sks13\t1\t\tonly churchill remembered his giving the blood , sweat and tears speech .\n",
            "sks13\t1\t\tonly churchill remembered himself giving the blood , sweat and .\n",
            "sks13\t1\t\tsusan hopes herself to sleep .\n",
            "sks13\t1\t\tfor john to hurt his friends is stupid .\n",
            "sks13\t1\t\tto hurt his friends is stupid .\n",
            "sks13\t1\t\tfor john to hurt himself is stupid .\n",
            "sks13\t1\t\tto hurt oneself is stupid .\n",
            "sks13\t0\t*\tfor john to hurt oneself is stupid .\n",
            "sks13\t1\t\tjohn promised bill to leave .\n",
            "sks13\t1\t\tjohn promised mary that he would leave .\n",
            "sks13\t1\t\tjohn promised mary to cut the grass .\n",
            "sks13\t1\t\tjohn promise mary to control himself .\n",
            "sks13\t0\t*\tjohn promised mary to control herself .\n",
            "sks13\t0\t*\tjohn promised mary to shave herself .\n",
            "sks13\t1\t\tjohn seems to sleep all day .\n",
            "sks13\t1\t\tjohn hopes to sleep .\n",
            "sks13\t1\t\tjohn tried to sleep .\n",
            "sks13\t0\t*\tjohn believes to have slept .\n",
            "sks13\t1\t\tjohn believes bill to have slept .\n",
            "sks13\t0\t*\tjohn believes for bill to have slept .\n",
            "sks13\t1\t\tjohn believes that bill has slept .\n",
            "sks13\t0\t*\tjohn believes bill that mary has slept .\n",
            "sks13\t0\t*\tjohn convinced to sleep .\n",
            "sks13\t1\t\tjohn convinced bill to sleep .\n",
            "sks13\t0\t*\tjohn convinced bill for mary to sleep .\n",
            "sks13\t0\t*\tjohn convinced that bill has slept .\n",
            "sks13\t0\t*\tit convinced bill that mary should sleep .\n",
            "sks13\t1\t\tjohn believes it to be obvious that bill left .\n",
            "sks13\t1\t\tjohn believes it to be raining .\n",
            "sks13\t0\t*\tjohn convinced it to be obvious that bill left .\n",
            "sks13\t0\t*\tjohn convinced it to be raining .\n",
            "sks13\t0\t*\tjohn convinced there to be several firemen available .\n",
            "sks13\t1\t\tbill cooked the rice .\n",
            "sks13\t1\t\tthe rice was cooked by bill .\n",
            "sks13\t1\t\tbill visited mary .\n",
            "sks13\t1\t\tmary was visited by bill .\n",
            "sks13\t1\t\tjohn believes bill to have cooked the rice .\n",
            "sks13\t1\t\tjohn believes the rice to have been cooked by bill .\n",
            "sks13\t1\t\tjohn believes bill to have visited mary .\n",
            "sks13\t1\t\tjohn believes mary to have been visited by bill .\n",
            "sks13\t1\t\tjohn convinced bill to cook the rice .\n",
            "sks13\t0\t*\tjohn convinced the rice to be cooked by bill .\n",
            "sks13\t1\t\tjohn convinced bill to visit mary .\n",
            "sks13\t1\t\tjohn believes that bill slept .\n",
            "sks13\t1\t\ti sent money .\n",
            "sks13\t1\t\ti sent mary money .\n",
            "sks13\t1\t\ti sent money to mary .\n",
            "sks13\t0\t*\ti sent bill money to mary to sam .\n",
            "sks13\t1\t\ti worked on sunday in the city on that project without a break .\n",
            "sks13\t1\t\ti praised mary .\n",
            "sks13\t0\t*\ti praised .\n",
            "sks13\t1\t\tthe moon glows in the darkness .\n",
            "sks13\t1\t\tthe moon glows .\n",
            "sks13\t1\t\ti sang a song with mary while you did so with bill .\n",
            "sks13\t1\t\twhat mary did with bill was sing a song .\n",
            "ad03\t1\t\tshe tried to leave\n",
            "ad03\t1\t\twho said he would give the cloak to lee ?\n",
            "ad03\t0\t*\tgilgamesh does n't be in the dungeon\n",
            "ad03\t1\t\twhich book about herself did jenny say that anson had written .\n",
            "ad03\t1\t\tpaul had eighty eight billion sixty three million forty-four thousand nine hundred at\n",
            "ad03\t0\t*\twhat i said that was we would go .\n",
            "ad03\t1\t\tthe boy thought she was happy .\n",
            "ad03\t1\t\tthe landlord donated a helicopter\n",
            "ad03\t1\t\tmost dragons have been neutered .\n",
            "ad03\t1\t\twho did you meet all when you were in derry ?\n",
            "ad03\t1\t\tjason persuaded medea to desert her family .\n",
            "ad03\t1\t\tmichael abandoned an old friend at mardi gras\n",
            "ad03\t1\t\tyou friends of the king are all the same\n",
            "ad03\t1\t\the is that kind of actor\n",
            "ad03\t0\t*\tlucy 's gomez 's wallet\n",
            "ad03\t1\t\tmedea tended to appear to be evil .\n",
            "ad03\t1\t\the 's bound to could do it\n",
            "ad03\t1\t\tnathan received the cloak from benjamin\n",
            "ad03\t1\t\tthat the world is round is obvious .\n",
            "ad03\t1\t\tposeidon wept , after the executioner left .\n",
            "ad03\t1\t\ti asked who did medea poison .\n",
            "ad03\t1\t\ti never liked his analysis .\n",
            "ad03\t0\t*\tpeter is some happy pigs which can fly .\n",
            "ad03\t0\t*\tgilgamesh not left .\n",
            "ad03\t0\t*\tthere arrived by medea .\n",
            "ad03\t1\t\ti might have eaten some seaweed .\n",
            "ad03\t1\t\tthere appears to be a problem with this solution .\n",
            "ad03\t1\t\twhat julie became was fond of lloyd .\n",
            "ad03\t1\t\tbill did not defeat the gods but gilgamesh did .\n",
            "ad03\t1\t\taphrodite frees animals\n",
            "ad03\t0\t*\tthe hospital was donated the book to .\n",
            "ad03\t1\t\tmedea , jason poisoned .\n",
            "ad03\t0\t*\tthey kicked himself\n",
            "ad03\t1\t\temily showed benjamin himself in the mirror .\n",
            "ad03\t1\t\tjason was killed by medea .\n",
            "ad03\t1\t\thow did you eat the cake ?\n",
            "ad03\t1\t\ti asked who medea poisoned .\n",
            "ad03\t1\t\taphrodite wanted to live and ishtar tried to\n",
            "ad03\t1\t\ti was sitting not under the tree but under the bush\n",
            "ad03\t1\t\tthe child wails\n",
            "ad03\t1\t\tgilgamesh has n't left\n",
            "ad03\t0\t*\twhiskey do i drink .\n",
            "ad03\t1\t\tdracula thought that he was the prince of darkness .\n",
            "ad03\t1\t\the looked up the number .\n",
            "ad03\t1\t\tshe has kissed her .\n",
            "ad03\t1\t\tagamemnon stopped jason casting the spell\n",
            "ad03\t1\t\thumans love to eat some disgruntled old pigs in those ditches .\n",
            "ad03\t1\t\tjason whispered that the phoenix had escaped\n",
            "ad03\t1\t\tron definitely has bought a dog .\n",
            "ad03\t0\t*\the book\n",
            "ad03\t1\t\twho is it obvious that plato loves .\n",
            "ad03\t0\t*\twhich god the statue ?\n",
            "ad03\t0\t*\tkiss pigs is my happiest memory\n",
            "ad03\t0\t*\tdante accused\n",
            "ad03\t1\t\tthat picture of jenny in a rubber dress does n't flatter her .\n",
            "ad03\t1\t\the might could go\n",
            "ad03\t1\t\tbenjamin gave lee the cloak and nathan the chalice .\n",
            "ad03\t1\t\tthat monkey is ate the banana\n",
            "ad03\t1\t\ti bought a book about harry\n",
            "ad03\t0\t*\tthe children wails\n",
            "ad03\t1\t\twho was it obvious that plato loved ?\n",
            "ad03\t1\t\tit was for jenny that i intended to be present .\n",
            "ad03\t1\t\ti think she is pregnant\n",
            "ad03\t1\t\tit 's extremely windy today .\n",
            "ad03\t0\t*\twho did you believe that to kiss seemed wrong ?\n",
            "ad03\t1\t\tjason would prefer medea to have cursed agamemnon .\n",
            "ad03\t0\t*\tthe therapist 's analysis of lucy 's\n",
            "ad03\t1\t\twho did athena introduce to whom ?\n",
            "ad03\t1\t\tit appears that poseidon owns a dragon\n",
            "ad03\t1\t\ti have often eaten muffins .\n",
            "ad03\t1\t\tgilgamesh can seek ishtar\n",
            "ad03\t1\t\tyou kicked yourself\n",
            "ad03\t1\t\tagamemnon seems to have left .\n",
            "ad03\t1\t\tthe dragons had all eaten the pigs .\n",
            "ad03\t1\t\tanson shot the dinosaur with his rifle in the jungle\n",
            "ad03\t0\t*\tgenie intoned the mirror .\n",
            "ad03\t1\t\ti often have eaten muffins .\n",
            "ad03\t1\t\the kicked himself\n",
            "ad03\t1\t\tgilgamesh has not read the cuneiform tablets .\n",
            "ad03\t1\t\the might maybe do that , might n't he ?\n",
            "ad03\t1\t\ti intended for jenny to be present .\n",
            "ad03\t0\t*\twe believed to be omnipotent .\n",
            "ad03\t1\t\twhose poem about achilles did homer persuade jason that he should read ?\n",
            "ad03\t1\t\tjason would prefer for medea to have cursed agamemnon .\n",
            "ad03\t1\t\ti asked who medea gave what ?\n",
            "ad03\t1\t\ti have every hope that you will defeat him .\n",
            "ad03\t1\t\tparis is no more\n",
            "ad03\t1\t\the will can do it\n",
            "ad03\t1\t\twe believed him to be the headmaster\n",
            "ad03\t1\t\twho kissed who ?\n",
            "ad03\t1\t\twho did you say that john thought would leave early ?\n",
            "ad03\t0\t*\tany boy saw no one .\n",
            "ad03\t0\t*\twhat i arranged for jenny was to be present .\n",
            "ad03\t0\t*\the kicked herself\n",
            "ad03\t1\t\tcassandra has warned agamemnon again .\n",
            "ad03\t1\t\tgilgamesh has been fighting the dragon .\n",
            "ad03\t1\t\tlucy 's photograph of jane\n",
            "ad03\t1\t\twho did jason think medea had poisoned ?\n",
            "ad03\t1\t\tgilgamesh may have quickly cast the spell\n",
            "ad03\t0\t*\thaving read of shakespeare satisfied me\n",
            "ad03\t0\t*\tmedea tried her to leave .\n",
            "ad03\t1\t\tthe potion boiled over\n",
            "ad03\t1\t\tthere arrived a new actor .\n",
            "ad03\t1\t\ti ate fruit\n",
            "ad03\t1\t\ti hoped that you would defeat him .\n",
            "ad03\t1\t\twho seems to be certain to leave first ?\n",
            "ad03\t0\t*\tshe liked moya 's football .\n",
            "ad03\t1\t\this hen loves anson .\n",
            "ad03\t1\t\ti ate a mango and gillian did too .\n",
            "ad03\t1\t\twhy did you eat the cake ?\n",
            "ad03\t0\t*\the would can go\n",
            "ad03\t1\t\tperhaps gilgamesh should be leaving\n",
            "ad03\t1\t\ti want to can do it\n",
            "ad03\t1\t\tjason intended for him to learn magic .\n",
            "ad03\t1\t\ti went to the shop for to get bread .\n",
            "ad03\t1\t\ti asked which king invaded which city .\n",
            "ad03\t1\t\twe made the claim that perseus killed the gorgon .\n",
            "ad03\t1\t\tplato listened to dp demosthenes ' oration about philip .\n",
            "ad03\t1\t\tthe old house collapsed .\n",
            "ad03\t0\t*\ti believed she is pregnant\n",
            "ad03\t1\t\thow are you feeling ?\n",
            "ad03\t1\t\taphrodite misses gilgamesh .\n",
            "ad03\t1\t\tanson very happily demonized david .\n",
            "ad03\t1\t\tthat plato loved aster proved to be his undoing .\n",
            "ad03\t1\t\thas n't the potion worked ?\n",
            "ad03\t1\t\tbill 's reading shakespeare satisfied me\n",
            "ad03\t1\t\tevery vampire slept .\n",
            "ad03\t1\t\ti might be leaving soon .\n",
            "ad03\t0\t*\tit 's arrived first that julie and jenny\n",
            "ad03\t1\t\tthe man i saw left .\n",
            "ad03\t0\t*\the replied his answer .\n",
            "ad03\t1\t\tbecause they hated him , the druids forced jason to live in a cupboard\n",
            "ad03\t1\t\twe kicked ourselves\n",
            "ad03\t1\t\tdid medea poison jason ?\n",
            "ad03\t1\t\taphrodite freed animals\n",
            "ad03\t1\t\tthe book was donated to the hospital .\n",
            "ad03\t1\t\tmedea poisoned more children than jason did .\n",
            "ad03\t1\t\tnathan showed benjamin himself in the mirror .\n",
            "ad03\t1\t\tthat plato loved aster deeply was obvious .\n",
            "ad03\t1\t\the kicked him\n",
            "ad03\t1\t\tjason expected the doctor to treat medea\n",
            "ad03\t1\t\tthe therapist 's analysis of lucy\n",
            "ad03\t1\t\twhere are you living ?\n",
            "ad03\t1\t\twho showed what to who ?\n",
            "ad03\t1\t\tmedea thought that , after the executioner had left , poseidon would be relieved .\n",
            "ad03\t1\t\tthe consul 's gift of the gladiator to himself .\n",
            "ad03\t1\t\tall the dragons have been slain .\n",
            "ad03\t1\t\tgilgamesh should slowly be tickling the mandrake .\n",
            "ad03\t1\t\todysseus planned to hear the sirens .\n",
            "ad03\t1\t\tbill reading shakespeare and maureen singing schubert satisfies me\n",
            "ad03\t1\t\tthe shooting of the hunters was very loud .\n",
            "ad03\t0\t*\tthe librarians likes books .\n",
            "ad03\t0\t*\tcan he will do it ?\n",
            "ad03\t0\t*\ti ordered there to be three books on the subject .\n",
            "ad03\t1\t\ttruman punched johnson\n",
            "ad03\t1\t\the became fond of peanuts .\n",
            "ad03\t1\t\tthe therapist analysed lucy\n",
            "ad03\t1\t\tdracula thought himself to be the prince of darkness .\n",
            "ad03\t0\t*\twhich poem did you hear those recitals of last night ?\n",
            "ad03\t1\t\tathena introduced medea to jason\n",
            "ad03\t1\t\the 'll no can do it , will he ?\n",
            "ad03\t1\t\tanson is incredibly difficult to please .\n",
            "ad03\t1\t\tit was claimed by everyone that the poison was neutralised\n",
            "ad03\t1\t\tthe banana is being eaten by that monkey .\n",
            "ad03\t1\t\ti want to kiss pigs\n",
            "ad03\t1\t\tburn letters to him !\n",
            "ad03\t1\t\this analysis of her was flawed\n",
            "ad03\t1\t\tdid the potion boil over ?\n",
            "ad03\t1\t\ti did n't see him ever .\n",
            "ad03\t0\t*\tshe said moya liked football .\n",
            "ad03\t1\t\twe all thought him to be unhappy\n",
            "ad03\t1\t\twhich book are you reading ?\n",
            "ad03\t1\t\tthat monkey is eating the banana .\n",
            "ad03\t1\t\tthat bottle of water might have cracked open .\n",
            "ad03\t1\t\twho did gilgamesh believe to have kissed aphrodite ?\n",
            "ad03\t1\t\tpaul had three affairs . . .\n",
            "ad03\t1\t\tclose the door !\n",
            "ad03\t1\t\ti was eating not a peach but an apple\n",
            "ad03\t1\t\twhich poem did you go to hear a recital of last night ?\n",
            "ad03\t0\t*\twhen time will you be there .\n",
            "ad03\t1\t\ti have sent 0 letters to environmental heath .\n",
            "ad03\t1\t\twhy did you kill pegasus ?\n",
            "ad03\t1\t\taphrodite does free animals\n",
            "ad03\t1\t\tgilgamesh will seek ishtar\n",
            "ad03\t1\t\ti assumed him to be innocent\n",
            "ad03\t1\t\ti am being whipped\n",
            "ad03\t1\t\tnever will i do syntax again .\n",
            "ad03\t1\t\tthe children wail\n",
            "ad03\t1\t\tmary fell .\n",
            "ad03\t1\t\ti inquired if we could leave early .\n",
            "ad03\t1\t\tbenjamin gave the cloak and sent the book to lee\n",
            "ad03\t1\t\thera tried to appear to be happy .\n",
            "ad03\t0\t*\ti arranged for to see her .\n",
            "ad03\t0\t*\tbill 's reading shakespeare and maureen 's singing schubert satisfies me\n",
            "ad03\t0\t*\tmyself shaved me .\n",
            "ad03\t0\t*\tno reading shakespeare satisfied me\n",
            "ad03\t1\t\tthe emperor 's every wish was immediately carried out .\n",
            "ad03\t1\t\tjenny has eaten a cake .\n",
            "ad03\t0\t*\tmoya played football with her\n",
            "ad03\t0\t*\ti intoned fruit\n",
            "ad03\t1\t\tthe sheep cry\n",
            "ad03\t1\t\the ca n't possibly do that , can he\n",
            "ad03\t1\t\twe believed that aphrodite was omnipotent .\n",
            "ad03\t1\t\twhich book about ulysses did you say that you would read ?\n",
            "ad03\t0\t*\ti wanted any cake .\n",
            "ad03\t1\t\tgilgamesh is not reading the cuneiform tablets .\n",
            "ad03\t1\t\tjason persuaded medea to try to run away .\n",
            "ad03\t1\t\twe believed aphrodite to be omnipotent\n",
            "ad03\t1\t\tthat bottle of water might have .\n",
            "ad03\t1\t\ti do n't remember what all i said ?\n",
            "ad03\t0\t*\taphrodite said he freed the animals and freed the animals he\n",
            "ad03\t1\t\tthat aphrodite was so promiscuous astounded the other gods .\n",
            "ad03\t0\t*\tgilgamesh does n't ate the honey\n",
            "ad03\t1\t\ti claimed that she was pregnant\n",
            "ad03\t0\t*\taphrodite do freed animals .\n",
            "ad03\t1\t\tdavid wrote that you said that anson thought that julie had fainted\n",
            "ad03\t0\t*\tgilgamesh failed often biology\n",
            "ad03\t1\t\tit rained\n",
            "ad03\t1\t\tposeidon was asleep , when the executioner arrived .\n",
            "ad03\t1\t\tpeople are in the garden\n",
            "ad03\t1\t\tanson became happy\n",
            "ad03\t1\t\tit is tough to teach syntax .\n",
            "ad03\t1\t\tthere 's going to be a party , is n't there ?\n",
            "ad03\t0\t*\ti have might be flying helicopters .\n",
            "ad03\t1\t\tthey brought the hat to the teacher\n",
            "ad03\t1\t\twhat medea attempted was to poison her children .\n",
            "ad03\t1\t\tbenjamin gave the cloak 0 and sent the book to lee\n",
            "ad03\t1\t\tthe man chuckles\n",
            "ad03\t1\t\tmilena will make pasta .\n",
            "ad03\t1\t\taphrodite did free animals\n",
            "ad03\t1\t\tgilgamesh should seek ishtar\n",
            "ad03\t1\t\tthey depend on mary .\n",
            "ad03\t0\t*\tthe greeks arrived all .\n",
            "ad03\t0\t*\thas not the potion worked\n",
            "ad03\t0\t*\tgomez 's photograph of pugsley of lucy 's\n",
            "ad03\t0\t*\twill can he do it ?\n",
            "ad03\t1\t\thumans love to eat the old pigs .\n",
            "ad03\t0\t*\the could might go\n",
            "ad03\t1\t\tit 's under the bed that 's the best place to hide\n",
            "ad03\t1\t\the left .\n",
            "ad03\t1\t\tthat picture of her pleases jenny .\n",
            "ad03\t1\t\tconstantly reading shakespeare satisfied me\n",
            "ad03\t1\t\tthere was a dragon in the cave .\n",
            "ad03\t1\t\tpeople like lard .\n",
            "ad03\t1\t\tthese ones are to be smuggled from hungary .\n",
            "ad03\t0\t*\temily showed himself to benjamin in the mirror .\n",
            "ad03\t0\t*\ti said that that jason was jealous annoyed medea\n",
            "ad03\t1\t\twe donated a chopper to the new hospital\n",
            "ad03\t1\t\tthese expensive and illegal bottles of absinthe are to be smuggled from hungary .\n",
            "ad03\t1\t\ta programme about euripides is on a radio 4 tonight .\n",
            "ad03\t1\t\tlucy 's analysis was the most successful\n",
            "ad03\t0\t*\ti am having eaten seaweed .\n",
            "ad03\t1\t\tmedea tried to poison her children .\n",
            "ad03\t0\t*\tanson demonized old\n",
            "ad03\t1\t\twhat i said was that we would go .\n",
            "ad03\t1\t\tthere are many fish in the sea .\n",
            "ad03\t1\t\tjason gave the poisoned clothes to who ?\n",
            "ad03\t0\t*\tby is eaten monkey banana that the being\n",
            "ad03\t1\t\tbenjamin said he would run away and he did .\n",
            "ad03\t1\t\twho is sailing to ithaca ?\n",
            "ad03\t1\t\tthey sat on mary .\n",
            "ad03\t1\t\tjulie filed letters to herself .\n",
            "ad03\t1\t\the looked it up\n",
            "ad03\t1\t\twho 's there ?\n",
            "ad03\t0\t*\tthere was he in the garden .\n",
            "ad03\t1\t\the might no could have done it\n",
            "ad03\t0\t*\tgilgamesh did n't ate the honey\n",
            "ad03\t1\t\tthe sheep cries\n",
            "ad03\t1\t\tfor aphrodite to appear to be happy would be impossible .\n",
            "ad03\t0\t*\twho did plato listen to dp demosthenes ' oration about ?\n",
            "ad03\t0\t*\tme gave it to him .\n",
            "ad03\t1\t\tif one were to steal talismans from witches , then that would be dangerous .\n",
            "ad03\t1\t\tbill did not destroy the world .\n",
            "ad03\t1\t\tboth the twins might have been at the party .\n",
            "ad03\t1\t\tthat plato loved aster was obvious .\n",
            "ad03\t1\t\tthe pigs grunt\n",
            "ad03\t0\t*\twhere place are you living .\n",
            "ad03\t0\t*\twho was for medea to poison awful ?\n",
            "ad03\t0\t*\tjulie maintained if the barman was sober .\n",
            "ad03\t1\t\tthe analysis of lucy took longer than that of gomez .\n",
            "ad03\t0\t*\tjulie became a fond .\n",
            "ad03\t1\t\ti climbed up the tree .\n",
            "ad03\t1\t\ti inquired when we could leave .\n",
            "ad03\t1\t\twhere alison and david soaked their feet was in the kitchen\n",
            "ad03\t0\t*\twhat medea wondered if was the potion was ready\n",
            "ad03\t1\t\tthat photograph of jane of lucy 's\n",
            "ad03\t1\t\tthe constant reading of shakespeare satisfied me\n",
            "ad03\t0\t*\tmedea wondered if that the potion was ready\n",
            "ad03\t1\t\twhat she thought was that the poison was neutralised\n",
            "ad03\t1\t\tbecause she had got the highest marks , medea was happy\n",
            "ad03\t1\t\twhen did you arrive ?\n",
            "ad03\t1\t\twhich poisonous plant is it certain that we will find in amazonia ?\n",
            "ad03\t1\t\tthe microphone salesman 's 0 irritating patter was relentless .\n",
            "ad03\t1\t\tthe paris i used to know is no more\n",
            "ad03\t1\t\tsam gave the cloak to lee and gave the magic chalice to matthew .\n",
            "ad03\t1\t\tgilgamesh has eaten the honey\n",
            "ad03\t1\t\ti will eat a mango , and gillian will too .\n",
            "ad03\t1\t\tcomputer viruses increased in virulence last year .\n",
            "ad03\t1\t\tat trade , anson danced extremely frantically\n",
            "ad03\t1\t\trichard is going to chop some wood .\n",
            "ad03\t1\t\tthe poem that homer wrote .\n",
            "ad03\t1\t\twho did drink the poison ?\n",
            "ad03\t1\t\tevan 's every idea was completely insane .\n",
            "ad03\t1\t\tsally is making scones , and gillian is too .\n",
            "ad03\t1\t\teveryone claimed that the poison was neutralized .\n",
            "ad03\t0\t*\tjonathan persuaded kate to lick himself .\n",
            "ad03\t1\t\tthat jason arrived infuriated medea .\n",
            "ad03\t1\t\tmedea was happy , because she had got the highest marks\n",
            "ad03\t1\t\tkeep yourself clean !\n",
            "ad03\t1\t\tcassandra has foretold disaster again .\n",
            "ad03\t1\t\tbill 's reading of shakespeare satisfied me\n",
            "ad03\t1\t\twho poisoned who ?\n",
            "ad03\t1\t\tpigs love truffles .\n",
            "ad03\t1\t\towners of pigs love truffles\n",
            "ad03\t1\t\tso quickly did the vampire move , that we barely saw him .\n",
            "ad03\t1\t\thumans love to eat those pigs .\n",
            "ad03\t0\t*\tshe has kissed she .\n",
            "ad03\t0\t*\tjason intended for he to learn magic .\n",
            "ad03\t1\t\tjason persuaded medea to be treated by the doctor\n",
            "ad03\t1\t\tit is true that i might be doing something other than going to the party .\n",
            "ad03\t1\t\tjason expected medea to be treated by the doctor\n",
            "ad03\t0\t*\ti found there .\n",
            "ad03\t1\t\tmoya said she liked football .\n",
            "ad03\t1\t\tanson became the mayor\n",
            "ad03\t1\t\tkane ate dirt .\n",
            "ad03\t1\t\tbenjamin gave the cloak to nathan\n",
            "ad03\t0\t*\tthe fig chuckled\n",
            "ad03\t1\t\tposeidon had run away , because the executioner murdered hera .\n",
            "ad03\t1\t\ta description of aristotle is in the book .\n",
            "ad03\t1\t\tjulie and jenny did .\n",
            "ad03\t1\t\tit 's quarter past four .\n",
            "ad03\t1\t\towners of a pig love to eat truffles .\n",
            "ad03\t0\t*\tthat whether the world is round is unknown bothered athena .\n",
            "ad03\t1\t\tno one expected agamemnon to to win\n",
            "ad03\t1\t\teuclid was interested in plato 's description of geometry .\n",
            "ad03\t0\t*\tevery reading shakespeare satisfied me\n",
            "ad03\t0\t*\tcan will he do it ?\n",
            "ad03\t1\t\tmedea poisoned who ?\n",
            "ad03\t0\t*\the looked up it\n",
            "ad03\t0\t*\twho guy did you see .\n",
            "ad03\t0\t*\twe kicked myself\n",
            "ad03\t0\t*\twho would poseidon run away , if the executioner murdered ?\n",
            "ad03\t0\t*\tanson kissed him\n",
            "ad03\t1\t\twhich city the claim that philip would invade .\n",
            "ad03\t1\t\ti have n't left yet\n",
            "ad03\t0\t*\ti am eating a mango and gillian has too .\n",
            "ad03\t0\t*\tletter is on the table\n",
            "ad03\t1\t\twho ate the cake ?\n",
            "ad03\t1\t\twhy did you say that you were leaving ?\n",
            "ad03\t1\t\tmichael left meg\n",
            "ad03\t0\t*\taphrodite quickly may free the animals\n",
            "ad03\t1\t\tthe reading of shakespeare satisfied me\n",
            "ad03\t0\t*\tthe weather rained\n",
            "ad03\t0\t*\tgilgamesh seek may ishtar\n",
            "ad03\t1\t\tno one expected to win .\n",
            "ad03\t0\t*\twho did that plato loved seem to be known by everyone .\n",
            "ad03\t1\t\tthe bear sniffs\n",
            "ad03\t1\t\tit hung on the wall .\n",
            "ad03\t0\t*\tjason killed .\n",
            "ad03\t0\t*\tmany people were there playing on the beach\n",
            "ad03\t1\t\tknow yourself !\n",
            "ad03\t1\t\tagamemnon attempted to behave well .\n",
            "ad03\t1\t\tjulie felt he was there\n",
            "ad03\t1\t\the thought that dracula was the prince of darkness .\n",
            "ad03\t1\t\ti have eaten already\n",
            "ad03\t1\t\tit is not true that i have left yet .\n",
            "ad03\t0\t*\tthat monkeys is eating the banana .\n",
            "ad03\t1\t\ti could have been flying helicopters by now .\n",
            "ad03\t0\t*\tanson put a book\n",
            "ad03\t0\t*\tgilgamesh might have not been reading the cuneiform tablets .\n",
            "ad03\t1\t\ti asked if medea poisoned jason .\n",
            "ad03\t1\t\twho did you persuade to go ?\n",
            "ad03\t1\t\twhat did you get all for xmas ?\n",
            "ad03\t1\t\tsome disgruntled old pigs in those ditches love truffles\n",
            "ad03\t1\t\tjason was killed .\n",
            "ad03\t0\t*\ti would like to might do it\n",
            "ad03\t0\t*\tpeter is some disgruntled old pigs in those ditches .\n",
            "ad03\t0\t*\tthere was him in the garden .\n",
            "ad03\t1\t\tgilgamesh is in the dungeon .\n",
            "ad03\t1\t\tanson will come to the party .\n",
            "ad03\t1\t\tgilgamesh has never flown a dragon .\n",
            "ad03\t0\t*\tjulie maintained her own questions over the course of the argument .\n",
            "ad03\t1\t\this analysis , i never liked .\n",
            "ad03\t1\t\tthat bottle of water might .\n",
            "ad03\t0\t*\tdid medea poison who ?\n",
            "ad03\t1\t\tshe took a picture of the phoenix\n",
            "ad03\t0\t*\tlook after herself !\n",
            "ad03\t1\t\twho did medea poison ?\n",
            "ad03\t1\t\ti tried for to get them .\n",
            "ad03\t1\t\twho did you introduce athena to ?\n",
            "ad03\t1\t\tcan i keep the screwdriver just like a carpenter keep the screwdriver ?\n",
            "ad03\t1\t\tjason refrained from casting the spell\n",
            "ad03\t1\t\tandrew likes lard on his sandwiches\n",
            "ad03\t1\t\twho seemed to have left first ?\n",
            "ad03\t0\t*\tron asked that the potion was ready\n",
            "ad03\t1\t\thierarchy of projections :\n",
            "ad03\t1\t\twe decided to paint the bathroom a lurid lime green colour .\n",
            "ad03\t1\t\tshe kicked her\n",
            "ad03\t0\t*\the knows he .\n",
            "ad03\t1\t\ti believed there to be three books on the subject .\n",
            "ad03\t0\t*\tthe child wail\n",
            "ad03\t1\t\twhich girl ate the cake ?\n",
            "ad03\t1\t\tthat plato lived in the city of athens was well-known .\n",
            "ad03\t0\t*\tcollapsed harry .\n",
            "ad03\t1\t\tfor you to do that would be a mistake .\n",
            "ad03\t0\t*\tjason thinks who medea had poisoned .\n",
            "ad03\t1\t\ti believe she is pregnant\n",
            "ad03\t1\t\tno one expected him to to win .\n",
            "ad03\t1\t\the 'll no can do it , can he ?\n",
            "ad03\t0\t*\twhich poem did you hear homer 's recital of last night ?\n",
            "ad03\t1\t\traffi slept well , and gillian will too\n",
            "ad03\t1\t\the 's bound to should do it\n",
            "ad03\t1\t\tit might have cracked open\n",
            "ad03\t1\t\twhere did perseus see the gorgon ?\n",
            "ad03\t1\t\tthe scissors are lost\n",
            "ad03\t1\t\tgilgamesh should be slowly tickling the mandrake\n",
            "ad03\t0\t*\tagamemnon seems pro to be a maniac\n",
            "ad03\t0\t*\tmyself saw me\n",
            "ad03\t1\t\ti believed she was pregnant\n",
            "ad03\t1\t\tanson gave fluffy to jenny .\n",
            "ad03\t1\t\tthe very old and extremely wise owl .\n",
            "ad03\t0\t*\twho did that plato loved prove to be his undoing .\n",
            "ad03\t0\t*\twhat medea believed was jason to be a murderer .\n",
            "ad03\t1\t\tthe owl hated the evil bat and loved the wise eagle .\n",
            "ad03\t1\t\tno one could remove the blood on the wall\n",
            "ad03\t0\t*\the can can go\n",
            "ad03\t0\t*\tgillian has made pasta and david is too .\n",
            "ad03\t0\t*\tjason intended for pro to learn magic .\n",
            "ad03\t1\t\tthe boys should could all go\n",
            "ad03\t0\t*\ti assumed to be innocent\n",
            "ad03\t1\t\tanson danced extremely frantically at trade .\n",
            "ad03\t0\t*\tthe gorgon is easy to believe the claim that perseus slew .\n",
            "ad03\t0\t*\tshe kicked itself\n",
            "ad03\t1\t\tjulie became a fond of lloyd .\n",
            "ad03\t1\t\tlee 's youngest and dawn 's oldest son ran away .\n",
            "ad03\t1\t\tanson kicked the cat\n",
            "ad03\t1\t\tmerlin is extremely evil .\n",
            "ad03\t1\t\tsyntax is easy to pretend that you can teach .\n",
            "ad03\t1\t\ti want to eat macaroni\n",
            "ad03\t1\t\twhich ode did which poet write ?\n",
            "ad03\t0\t*\twhat she thought that was the poison was neutralised\n",
            "ad03\t1\t\twho drank the poison ?\n",
            "ad03\t1\t\twhat medea arranged was for her children to be poisoned .\n",
            "ad03\t1\t\tno one 's mother had baked anything .\n",
            "ad03\t1\t\twhat kind of actor is he ?\n",
            "ad03\t1\t\twhat did she eat ?\n",
            "ad03\t0\t*\tfrantically at , anson danced extremely trade\n",
            "ad03\t1\t\ti have often a cold .\n",
            "ad03\t1\t\twho did maria say that she 'd kiss and kick ?\n",
            "ad03\t1\t\twhere did they go all for their holidays ?\n",
            "ad03\t1\t\tthey came running over the hill and through the woods\n",
            "ad03\t0\t*\tthe airport yawned\n",
            "ad03\t1\t\thow quickly did you eat the cake ?\n",
            "ad03\t1\t\tmany fish are in the sea .\n",
            "ad03\t1\t\tthey arrived first\n",
            "ad03\t1\t\tpeople were playing on the beach .\n",
            "ad03\t0\t*\tbenjamin gave to lee it .\n",
            "ad03\t0\t*\the liked anson .\n",
            "ad03\t0\t*\tthe bear sniff\n",
            "ad03\t0\t*\ti inquired could we leave early .\n",
            "ad03\t1\t\tthe bears sniff\n",
            "ad03\t0\t*\ti persuaded there to be a problem .\n",
            "ad03\t1\t\this book\n",
            "ad03\t1\t\the looked the number up\n",
            "ad03\t1\t\thas jenny eaten a cake ?\n",
            "ad03\t1\t\twhich goddess helped us ?\n",
            "ad03\t1\t\tmedea killed jason .\n",
            "ad03\t1\t\tron certainly will buy a dog .\n",
            "ad03\t0\t*\tthey shaved david and anson .\n",
            "ad03\t0\t*\twe believed to be the headmaster\n",
            "ad03\t1\t\twhich king did you wonder invaded which city ?\n",
            "ad03\t1\t\tno one expected agamemnon to win .\n",
            "ad03\t0\t*\tthe day snowed\n",
            "ad03\t1\t\tgilgamesh never flies dragons .\n",
            "ad03\t0\t*\tkeep myself clean !\n",
            "ad03\t1\t\tthe dragons have all been slain .\n",
            "ad03\t0\t*\tdid that medea killed her children upset jason ?\n",
            "ad03\t1\t\tthe amoeba coughed and then it fainted .\n",
            "ad03\t1\t\ti want to sing\n",
            "ad03\t1\t\the will can go\n",
            "ad03\t0\t*\tmedea seemed that has poisoned jason .\n",
            "ad03\t1\t\thaving read shakespeare satisfied me\n",
            "ad03\t0\t*\tpeter is owners of pigs .\n",
            "ad03\t0\t*\todysseus attempted the helmsman to hear the sirens .\n",
            "ad03\t1\t\tgilgamesh may seek ishtar\n",
            "ad03\t1\t\tthe librarian likes books .\n",
            "ad03\t1\t\talison and david soaked their feet after dinner\n",
            "ad03\t1\t\tmary is faster than john is .\n",
            "ad03\t1\t\talison and david soaked their feet in the kitchen\n",
            "ad03\t0\t*\tyou kicked you\n",
            "ad03\t1\t\tdid you see mary ?\n",
            "ad03\t1\t\traffi has made pasta , and david has too .\n",
            "ad03\t1\t\tthere seemed to be three men in the garden .\n",
            "ad03\t1\t\tthat medea murdered jason did n't surprise anyone .\n",
            "ad03\t1\t\tmoya 's football team loved her\n",
            "ad03\t0\t*\ti sent she away .\n",
            "ad03\t1\t\tjason persuaded medea that she should desert her family\n",
            "ad03\t0\t*\taphrodite stinks to be omnipotent .\n",
            "ad03\t1\t\tevery reading of shakespeare satisfied me\n",
            "ad03\t0\t*\tbill reading shakespeare and maureen singing schubert satisfy me\n",
            "ad03\t1\t\twhen the executioner arrived , poseidon was asleep\n",
            "ad03\t1\t\tthey kicked themselves\n",
            "ad03\t1\t\tmany vampires have become vegetarian .\n",
            "ad03\t0\t*\tthat that the world is round is obvious upset hermes .\n",
            "ad03\t0\t*\tbill not destroyed the world .\n",
            "ad03\t1\t\tjohn saw stephan\n",
            "ad03\t0\t*\ti destroyed there .\n",
            "ad03\t0\t*\twhat was euclid interested in plato 's description of ?\n",
            "ad03\t1\t\ti like anson\n",
            "ad03\t1\t\tthe dragons simply all died out .\n",
            "ad03\t1\t\tgilgamesh did not fly the dragon .\n",
            "ad03\t1\t\twhich goddess might help us ?\n",
            "ad03\t1\t\thumans love to eat pigs .\n",
            "ad03\t1\t\twhich poem about achilles did homer recite ?\n",
            "ad03\t1\t\tthe boys should all could go\n",
            "ad03\t0\t*\tthe owl hated the evil and the wise eagle .\n",
            "ad03\t1\t\tthe shield that saved achilles life .\n",
            "ad03\t1\t\tevan 's every desire\n",
            "ad03\t1\t\ti wondered whether medea had fled .\n",
            "ad03\t1\t\ti have eaten my hat already\n",
            "ad03\t0\t*\the will could go\n",
            "ad03\t1\t\tjenny swallowed the fly\n",
            "ad03\t1\t\tthe flying car hit the tree in the air\n",
            "ad03\t1\t\ti have a book .\n",
            "ad03\t1\t\tjason thought of defending the dragon\n",
            "ad03\t1\t\tit seems that agamemnon is a maniac\n",
            "ad03\t0\t*\twhich city do you believe the claim that philip would invade ?\n",
            "ad03\t1\t\twe claimed that perseus had killed the gorgon\n",
            "ad03\t1\t\twe need some technician to help us .\n",
            "ad03\t0\t*\tthe scissors is lost\n",
            "ad03\t1\t\ti have been flying helicopters for years .\n",
            "ad03\t1\t\tsam gave the cloak to lee and the magic chalice to matthew .\n",
            "ad03\t0\t*\twe kicked us\n",
            "ad03\t1\t\tno reading of shakespeare satisfied me\n",
            "ad03\t1\t\twhat did he reply ?\n",
            "ad03\t0\t*\tit was claimed that by everyone the poison was neutralised\n",
            "ad03\t1\t\ti asked which city which king invaded .\n",
            "ad03\t1\t\traffi makes pesto pasta , and david does too\n",
            "ad03\t1\t\teat dirt !\n",
            "ad03\t1\t\tlook after yourself !\n",
            "ad03\t0\t*\tshe wanted to can leave\n",
            "ad03\t1\t\tarthur gave the tapestry to lancelot .\n",
            "ad03\t1\t\twe took the car to the town\n",
            "ad03\t1\t\tbenjamin gave the cloak to lee .\n",
            "ad03\t1\t\tnot reading shakespeare satisfied me\n",
            "ad03\t0\t*\tthere were killed three men .\n",
            "ad03\t1\t\tgilgamesh has not been reading the cuneiform tablets\n",
            "ad03\t0\t*\tthe imposition of the government of a fine .\n",
            "ad03\t1\t\twhen alison and david soaked their feet was after dinner\n",
            "ad03\t1\t\tthis problem 's analysis is made a lot easier when you understand differential equations .\n",
            "ad03\t0\t*\tdracula thought that himself was the prince of darkness .\n",
            "ad03\t0\t*\tgilgamesh might have been not reading the cuneiform tablets .\n",
            "ad03\t1\t\twho asked which statue which tourist had taken a photo of ?\n",
            "ad03\t1\t\twillow said that she 'd kiss tara and kick xander .\n",
            "ad03\t1\t\ti climbed right up the tree .\n",
            "ad03\t1\t\tall the dragons had escaped .\n",
            "ad03\t1\t\twho did you attempt to force jason to kill ?\n",
            "ad03\t1\t\ti thought of the moon\n",
            "ad03\t0\t*\tbenjamin thought he would give the cloak to lee and the cloak to lee he gave .\n",
            "ad03\t1\t\ti wondered had he left yet .\n",
            "ad03\t1\t\ti thought she was pregnant\n",
            "ad03\t1\t\ti arranged for him to see her .\n",
            "ad03\t1\t\tit was over the hill and through the woods that they came running\n",
            "ad03\t1\t\twho did you ask saw what ?\n",
            "ad03\t0\t*\tgilgamesh might can seek ishtar\n",
            "ad03\t1\t\tgilgamesh arrived\n",
            "ad03\t0\t*\tjason arrived by medea .\n",
            "ad03\t1\t\toil spread over the sea shore .\n",
            "ad03\t1\t\twhat jason asked was whether the potion was ready\n",
            "ad03\t0\t*\tjason asked whether that the potion was ready\n",
            "ad03\t1\t\thave you seen mary ? i have vp seen mary\n",
            "ad03\t1\t\tit seems that agamemnon left .\n",
            "ad03\t1\t\tthose monkeys are eating the banana .\n",
            "ad03\t0\t*\ti introduced her to he .\n",
            "ad03\t0\t*\tnathan showed to benjamin it .\n",
            "ad03\t0\t*\the kicked yourself\n",
            "ad03\t1\t\tanson tried to shave himself .\n",
            "ad03\t0\t*?\tgilgamesh never has flown a dragon .\n",
            "ad03\t0\t*\twhat julie did of lloyd was become fond .\n",
            "ad03\t1\t\tit is not allowed to incriminate oneself .\n",
            "ad03\t1\t\tthe analysis of the problem was flawed\n",
            "ad03\t1\t\twhich goddess did help us ?\n",
            "ad03\t1\t\tposeidon appears to have turned out to have left .\n",
            "ad03\t0\t*\tgilgamesh has been not reading the cuneiform tablets .\n",
            "ad03\t0\t*\tdanced extremely , anson frantically at trade\n",
            "ad03\t0\t*\taphrodite wanted to live and ishtar tried to do\n",
            "ad03\t0\t*\ti kicked yourself\n",
            "ad03\t1\t\thow fond of esther is agamemnon ?\n",
            "ad03\t1\t\tron heard a discussion in the foyer\n",
            "ad03\t0\t*\tmy mother hated myself\n",
            "ad03\t1\t\tthe students demonstrated the technique this morning\n",
            "ad03\t1\t\the walked up the hill .\n",
            "ad03\t0\t*\twe wanted to ate cake\n",
            "ad03\t0\t*\tjason knew those medea had cast the spell\n",
            "ad03\t0\t*\tgilgamesh must should seek ishtar\n",
            "ad03\t1\t\taphrodite said he freed the animals and free the animals he did\n",
            "ad03\t1\t\tdid you drink the poison ?\n",
            "ad03\t1\t\twhether agamemnon had triumphed was unknown .\n",
            "ad03\t0\t*\ther has kissed her .\n",
            "ad03\t1\t\ti often have a cold .\n",
            "ad03\t0\t*\tjason whispered the phoenix had escaped\n",
            "ad03\t0\t*\tbill reading of shakespeare satisfied me\n",
            "ad03\t1\t\tdid n't the magic work ?\n",
            "ad03\t1\t\tanson thought julie had fainted\n",
            "ad03\t1\t\tthe horse fell\n",
            "ad03\t0\t*\todysseus attempted odysseus to hear the sirens .\n",
            "ad03\t1\t\tburn letters to peter !\n",
            "ad03\t1\t\tgenie intoned the prayer\n",
            "ad03\t1\t\tgilgamesh did n't fly the broomstick .\n",
            "ad03\t1\t\tron 's likely to be on the web , is n't he ?\n",
            "ad03\t1\t\tbill 's reading shakespeare and maureen 's singing schubert satisfy me\n",
            "ad03\t0\t*\towners of a pig loves to eat truffles\n",
            "ad03\t0\t*\tgilgamesh might loved ishtar\n",
            "ad03\t1\t\tpaul had an affair\n",
            "ad03\t1\t\tposeidon appears to own a dragon\n",
            "ad03\t1\t\tthe twins might have both been at the party .\n",
            "ad03\t0\t*\tthat jason had arrived was obvious infuriated medea .\n",
            "ad03\t1\t\tthat i should evaporate is my fondest dream\n",
            "ad03\t1\t\twhat gilgamesh may do is seek ishtar\n",
            "ad03\t1\t\tyou said that anson thought that julie had fainted\n",
            "ad03\t1\t\tthe owl hated the evil bat and the wise eagle\n",
            "ad03\t1\t\twhat did john buy ?\n",
            "ad03\t1\t\tagamemnon forced aphrodite to leave the school .\n",
            "ad03\t1\t\tthere is a description of aristotle in the book .\n",
            "ad03\t0\t*\tmedea exclaimed if the potion was ready\n",
            "ad03\t1\t\thumans love to eat them .\n",
            "ad03\t0\t*\tsomeone did medea poison .\n",
            "ad03\t1\t\tperhaps iphigenia will have murdered oedipus by tomorrow .\n",
            "ad03\t1\t\tso that he could escape , jason became invisible\n",
            "ad03\t0\t*\ti wondered who had medea poisoned .\n",
            "ad03\t1\t\ti asked did medea poison jason .\n",
            "ad03\t1\t\tagamemnon stopped jason from casting the spell\n",
            "ad03\t1\t\tno one wanted any cake .\n",
            "ad03\t1\t\ti wanted jimmy for to come with me .\n",
            "ad03\t0\t*\the walked the hill up .\n",
            "ad03\t1\t\tthey should have all sent oedipus to thebes\n",
            "ad03\t0\t*\tthose monkey are eating the banana .\n",
            "ad03\t0\t*\twho had poseidon run away , before the executioner murdered ?\n",
            "ad03\t1\t\ti asked anson if he was happy\n",
            "ad03\t1\t\tdaniel became a blond .\n",
            "ad03\t0\t*\thas that we have arrived back at our starting point proved that the world is round ?\n",
            "ad03\t1\t\tit was the man i saw that you wanted to meet .\n",
            "ad03\t1\t\tthat photograph by gomez of pugsley of lucy 's\n",
            "ad03\t1\t\ti ate that .\n",
            "ad03\t1\t\tit snowed\n",
            "ad03\t1\t\taphrodite said he would free the animals and free the animals he will\n",
            "ad03\t1\t\tthat the golden thread would show jason his path through the labyrinth was\n",
            "ad03\t1\t\tjulie and jenny arrived first\n",
            "ad03\t1\t\twhat have you eaten ?\n",
            "ad03\t0\t*\tpeter is owners .\n",
            "ad03\t0\t*\ti said this he left\n",
            "ad03\t1\t\twho has drunk my whiskey ?\n",
            "ad03\t0\t*\tyou said she liked yourself\n",
            "ad03\t0\t*\tshe tried to left\n",
            "ad03\t1\t\ti 'd planned to have finished , and finished i have\n",
            "ad03\t1\t\tron expected the sack .\n",
            "ad03\t1\t\tthat i am here proves that i care .\n",
            "ad03\t0\t*\tshe tried to may leave\n",
            "ad03\t1\t\tgilgamesh misses aphrodite\n",
            "ad03\t0\t*\twho seemed had poisoned jason ?\n",
            "ad03\t1\t\tthat plato loved aster seemed to be known by everyone .\n",
            "ad03\t1\t\twhen dining with evil crocodiles , it is advisable to wear armour .\n",
            "ad03\t0\t*\tbenjamin said he would give the cloak to lee and give the cloak he did to lee .\n",
            "ad03\t1\t\tdid the magic work ?\n",
            "ad03\t1\t\twho has drunk the poison ?\n",
            "ad03\t1\t\tbenjamin said he would give the cloak to lee and give the cloak to lee he did .\n",
            "ad03\t1\t\tjason became invisible , so that he could escape\n",
            "ad03\t1\t\taphrodite may quickly free the animals .\n",
            "ad03\t1\t\tthe horse galloped\n",
            "ad03\t1\t\thow quickly did the greeks take troy ?\n",
            "ad03\t1\t\tsome happy pigs which can fly love truffles\n",
            "ad03\t1\t\tjulie felt a twinge in her arm\n",
            "ad03\t1\t\tthe wizard turned the beetle into beer with a wave of his wand\n",
            "ad03\t0\t*\twho seemed that had poisoned jason ?\n",
            "ad03\t1\t\tkick me !\n",
            "ad03\t1\t\twe wanted to eat cake\n",
            "ad03\t1\t\tgomez 's photograph of pugsley belonging to lucy .\n",
            "ad03\t1\t\tall the boys should could go\n",
            "ad03\t1\t\tjulie maintained her own ideas over the course of the argument .\n",
            "ad03\t1\t\tthe intrepid pirate and the fearful captain 's mate sunk the galleon .\n",
            "ad03\t1\t\tgilgamesh might not have been reading the cuneiform tablets .\n",
            "ad03\t1\t\tit was obvious that plato loved aster obvious .\n",
            "ad03\t1\t\the loves him\n",
            "ad03\t1\t\twe all thought he was unhappy\n",
            "ad03\t0\t*\temily showed himself benjamin in the mirror .\n",
            "ad03\t1\t\tanson believed the report .\n",
            "ad03\t1\t\ti looked the number up .\n",
            "ad03\t0\t*\tanson is incredibly difficult to be pleased .\n",
            "ad03\t1\t\tno vampire slept .\n",
            "ad03\t1\t\tafter the executioner left , poseidon wept .\n",
            "ad03\t1\t\tpeter was at the party\n",
            "ad03\t0\t*\twhales have i seen .\n",
            "ad03\t0\t*\ti thought she is pregnant\n",
            "ad03\t0\t*\thimself saw him\n",
            "ad03\t1\t\tthat he is coming is clear .\n",
            "ad03\t0\t*\tthere seem three men to be in the garden .\n",
            "ad03\t0\t*\the analysis her was flawed\n",
            "ad03\t1\t\twhere all did they go for their holidays ?\n",
            "ad03\t1\t\tgilgamesh decided not to kill ishtar\n",
            "ad03\t1\t\tbill reading shakespeare satisfied me\n",
            "ad03\t1\t\tperseus saw the gorgon in his shield .\n",
            "ad03\t1\t\tposeidon would run away , if the executioner murdered hera .\n",
            "ad03\t0\t*\twho did a statue of surprise medea ?\n",
            "ad03\t1\t\twhat did you say ( that ) the poet had written ?\n",
            "ad03\t1\t\ti saw people playing there on the beach .\n",
            "ad03\t0\t*\twho was that plato loved obvious ?\n",
            "ad03\t1\t\ti did n't want any cake .\n",
            "ad03\t1\t\tthat i should kiss pigs is my fondest dream\n",
            "ad03\t0\t*\tgilgamesh flew not the broomstick .\n",
            "ad03\t1\t\tron failed biology , unfortunately\n",
            "ad03\t1\t\tthe men chuckle\n",
            "ad03\t1\t\ti expected there to be a problem .\n",
            "ad03\t1\t\tgilgamesh wanted to seduce ishtar , and seduce ishtar he did .\n",
            "ad03\t1\t\tharry collapsed .\n",
            "ad03\t1\t\ti asked who saw what .\n",
            "ad03\t0\t*\tthe doctor arrived a new actor .\n",
            "ad03\t0\t*\thim loves him\n",
            "ad03\t0\t*\twho had poseidon run away , because the executioner murdered ?\n",
            "ad03\t1\t\the has been happy\n",
            "ad03\t1\t\tposeidon had run away , before the executioner murdered hera .\n",
            "ad03\t0\t*\twhich the poem did homer recite ?\n",
            "ad03\t0\t*\tnot reading of shakespeare satisfied me\n",
            "ad03\t0\t*\twho did athena introduce who to ?\n",
            "ad03\t1\t\tmerlin is a dangerous sorcerer .\n",
            "ad03\t1\t\tanson saw anson .\n",
            "ad03\t1\t\ti am to eat macaroni .\n",
            "ad03\t1\t\tposeidon had escaped , before the executioner arrived .\n",
            "ad03\t1\t\towners love truffles\n",
            "ad03\t0\t*\tthe dragons were slain all .\n",
            "ad03\t0\t*\ti saw him ever .\n",
            "ad03\t1\t\thumans love to eat owners of pigs .\n",
            "ad03\t0\t*\ti have sent 0 letter to environmental heath\n",
            "ad03\t0\t*\twhat jason asked whether was the potion was ready\n",
            "ad03\t1\t\tthose pigs love truffles\n",
            "ad03\t0\t*\twe all thought he to be unhappy\n",
            "ad03\t1\t\ti 'd planned to have finished by now .\n",
            "ad03\t1\t\thas the potion not worked ?\n",
            "ad03\t1\t\twhat i love is toast and sun dried tomatoes\n",
            "ad03\t1\t\tmary ran .\n",
            "ad03\t0\t*\tthe man i saw shaved myself .\n",
            "ad03\t0\t*\treadings shakespeare satisfied me\n",
            "ad03\t0\t*\tthe picture of no one hung upon any wall .\n",
            "ad03\t1\t\the replied that he was happy .\n",
            "ad03\t1\t\tno one could remove the blood from the wall\n",
            "ad03\t1\t\tjulie maintained that the barman was sober .\n",
            "ad03\t0\t*\ti kicked me\n",
            "ad03\t1\t\tbenjamin gave lee the cloak .\n",
            "ad03\t1\t\taphrodite wanted hera to persuade athena to leave .\n",
            "ad03\t1\t\tgilgamesh is fighting the dragon .\n",
            "ad03\t1\t\ti claimed she was pregnant\n",
            "ad03\t1\t\tfor jenny , i intended to be present .\n",
            "ad03\t1\t\tgilgamesh missed aphrodite\n",
            "ad03\t1\t\tshe might be pregnant .\n",
            "ad03\t0\t*\tthe pig grunt\n",
            "ad03\t1\t\tanson demonized david at the club .\n",
            "ad03\t1\t\tjason asked whether the potion was ready\n",
            "ad03\t1\t\tfrieda closed the door\n",
            "ad03\t0\t*\tpeter is the old pigs .\n",
            "ad03\t1\t\tmedea might have given jason a poisoned robe ( just treat a poisoned robe as an np\n",
            "ad03\t1\t\tquickly kiss anson !\n",
            "ad03\t0\t*\tanson believed jenny to have hurt himself .\n",
            "ad03\t1\t\tjulie felt hot\n",
            "ad03\t1\t\tagamemnon expected esther to seem to be happy .\n",
            "ad03\t0\t*\thim book\n",
            "ad03\t1\t\tthat the answer is obvious upset hermes .\n",
            "ad03\t0\t*\tthe consul 's gift of himself to the gladiator .\n",
            "ad03\t1\t\thomer recited the poem about achilles ?\n",
            "ad03\t1\t\tno vampire can survive sunrise .\n",
            "ad03\t1\t\tunder the bed is the best place to hide\n",
            "ad03\t1\t\tanson appeared\n",
            "ad03\t1\t\tthere seems to be a problem .\n",
            "ad03\t0\t*\tanson became that he was happy\n",
            "ad03\t1\t\ti intoned that she was happy\n",
            "ad03\t0\t*\twe all thought him was unhappy\n",
            "ad03\t1\t\tmedea saw who ?\n",
            "ad03\t1\t\tno one expected that agamemnon would win .\n",
            "ad03\t1\t\tbelieving that the world is flat gives one some solace .\n",
            "ad03\t1\t\tkick them !\n",
            "ad03\t0\t*\tthe bears sniffs\n",
            "ad03\t0\t*\twhere did you disappear before you hid the gold ?\n",
            "ad03\t0\t*\tshe tried to do go .\n",
            "ad03\t1\t\tmedea wondered if the potion was ready\n",
            "ad03\t1\t\twho all did you meet when you were in derry ?\n",
            "ad03\t1\t\twho did you hear an oration about ?\n",
            "ad03\t1\t\talison ran\n",
            "ad03\t1\t\tromeo sent letters to juliet .\n",
            "ad03\t1\t\trichard 's gift of the helicopter to the hospital and of the bus to the school .\n",
            "ad03\t1\t\tnathan caused benjamin to see himself in the mirror .\n",
            "ad03\t1\t\ta. madeleine planned to catch the sardines and she did .\n",
            "ad03\t0\t*\tmedea tried medea to poison her children .\n",
            "ad03\t0\t*\twhich temple did athena contemplate the reason that her devotees had built ?\n",
            "ad03\t1\t\ti did not understand .\n",
            "ad03\t1\t\tgilgamesh loved ishtar and aphrodite did too\n",
            "ad03\t1\t\twe believed him to be omnipotent\n",
            "ad03\t0\t*\tron captured quickly a phoenix\n",
            "ad03\t1\t\tdavid ate mangoes and raffi should too .\n",
            "ad03\t1\t\tjulie and fraser ate those delicious pies in julie 's back garden .\n",
            "ad03\t1\t\tthe old pigs love truffles\n",
            "ad03\t1\t\tthe boys all should could go\n",
            "ad03\t1\t\taphrodite quickly freed the animals\n",
            "ad03\t1\t\tpaul had two affairs\n",
            "ad03\t1\t\twhat alison and david did was soak their feet in a bucket\n",
            "ad03\t1\t\tanson demonized david almost constantly .\n",
            "ad03\t0\t*\tagamemnon seemed that left .\n",
            "ad03\t1\t\tanson 's hen nibbled his ear .\n",
            "ad03\t0\t*\twhat a kind of actor is he ?\n",
            "ad03\t0\t*\tthe constantly reading shakespeare satisfied me\n",
            "ad03\t1\t\tbefore the executioner arrived , poseidon had escaped\n",
            "ad03\t1\t\tgilgamesh did n't leave .\n",
            "ad03\t1\t\tgenie intoned that she was tired\n",
            "ad03\t1\t\tlook at all these books . which book would you like ?\n",
            "ad03\t0\t*\tthere were killed three men by the assassin .\n",
            "ad03\t0\t*\tpeter is those pigs .\n",
            "ad03\t1\t\ti do n't remember what i said all ?\n",
            "ad03\t1\t\tthe pig grunts\n",
            "ad03\t0\t*\tthe poison was neutralised was claimed that by everyone\n",
            "ad03\t1\t\tpeople are stupid\n",
            "ad03\t1\t\twhat i arranged was for jenny to be present .\n",
            "ad03\t1\t\ti compared ginger to fred\n",
            "ad03\t0\t*\tpeter is pigs\n",
            "ad03\t1\t\twhich poet wrote which ode ?\n",
            "ad03\t1\t\thow did julie ask if jenny left ?\n",
            "ad03\t1\t\tdracula thought him to be the prince of darkness .\n",
            "ad03\t0\t*\the ca n't possibly do that , possibly he ?\n",
            "ad03\t1\t\ti must eat macaroni .\n",
            "ad03\t1\t\ti asked who john would introduce to who .\n",
            "ad03\t0\t*\tthe owl hated the and loved the bat .\n",
            "ad03\t1\t\treading shakespeare satisfied me\n",
            "ad03\t1\t\thumans love to eat owners .\n",
            "ad03\t1\t\tgilgamesh fears death and achilles does as well\n",
            "ad03\t0\t*\tthe pigs grunts\n",
            "ad03\t0\t*\tconstant reading shakespeare satisfied me\n",
            "ad03\t0\t*\tanson believed to be happy .\n",
            "ad03\t1\t\thow did julie say that jenny left ?\n",
            "ad03\t1\t\tshow me letters !\n",
            "ad03\t1\t\tthe readings of shakespeare satisfied me\n",
            "ad03\t1\t\tanson demonized david every day .\n",
            "ad03\t1\t\tthe students demonstrated this morning\n",
            "ad03\t1\t\twe believed aphrodite to be omnipotent .\n",
            "ad03\t1\t\temily caused benjamin to see himself in the mirror .\n",
            "ad03\t0\t*\tanson left before jenny saw himself .\n",
            "ad03\t1\t\tnothing like that would i ever eat again .\n",
            "ad03\t1\t\twhere has he put the cake ?\n",
            "ad03\t1\t\tjason persuaded medea to desert her family\n",
            "ad03\t1\t\tgilgamesh perhaps should be leaving .\n",
            "ad03\t1\t\tgilgamesh has n't kissed ishtar .\n",
            "ad03\t0\t*\tanson thought that himself was going to the club .\n",
            "ad03\t0\t*\tposeidon appears to own a dragon\n",
            "ad03\t0\t*\tdigitize is my happiest memory\n",
            "ad03\t1\t\tit is easy to slay the gorgon .\n",
            "ad03\t1\t\ti had the strangest feeling that i knew you .\n",
            "ad03\t1\t\twhat all did you get for christmas ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um9Rl8lfGnal",
        "outputId": "843c77e8-e3f1-4273-84d3-72ac5d9f8d4d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8551, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8Pk1WYmWT9M",
        "outputId": "82a2c865-4f34-4e89-bc89-c5d47bf8c683"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sentence_source', 'label', 'label_notes', 'sentence'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['sentence'][0])\n",
        "print(df['label'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDxuL4kWWb3E",
        "outputId": "2f2ce3f1-40bc-4075-c4f8-921f91773ff0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our friends wo n't buy this analysis , let alone the next one we propose .\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Preparing the pretraining input(next seq prediction or NSP)\n",
        "sentences = df.sentence.values\n",
        "sentences = [\"[CLS]\" + sentence + \"[SEP]\" for sentence in sentences]\n",
        "labels = df.label.values"
      ],
      "metadata": {
        "id": "ZIgt4yQqWhrD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(sentences)\n",
        "print(sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sctEMJc4XfGo",
        "outputId": "64eb540c-c042-45eb-ec89-8290b20a2b0e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]our friends wo n't buy this analysis , let alone the next one we propose .[SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Activating the BERT Tokenizer\n",
        "\n",
        "# from transformer import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True )\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339,
          "referenced_widgets": [
            "ca4b5520a43f45b89e58df5b9801e921",
            "eb05565de7934a089be5467bded24fa0",
            "7b02c4e6e9e84f1aa42eb014a57b3912",
            "f984eaa1d42949e1b4c3dd370e99a4bb",
            "e7889f7f978f487cb3232fd616909250",
            "7b2b8f8f6b65464da4ce72ae281523d1",
            "af8648ce146c4c038a895ba5aa625311",
            "d391c7e66b5a45fba2ffae2e719c500d",
            "f2959e5cf24647508516bbd984d13ea9",
            "4852ff90289e40d8977ad4cc80f80ccf",
            "8a69116df3e0413b83ea6403928f7cb7",
            "85075457aeb849f380df18db6a3c2cfe",
            "9b7be2ad1ada462a8e79cd6ec3581467",
            "629b762ec60645b6b1a06adc04e794f2",
            "155db28de8aa43a0b4ec03c1c201d617",
            "5e50c37906374440b7d6fbba7f206713",
            "0606d4303f804f2c89b0f498a17c0b19",
            "23129f3980b648ea96147d6fa8db513c",
            "efd689604f9747108387f83db089a662",
            "690f5fa6fe84472a9d1d2289b35e0cad",
            "e088cad24a4944ed917eac23560efd14",
            "af70b6091b18424d8d607516915a37a9",
            "8d821a4dd86349e49e5cbcd20a2704d1",
            "8fe34b49485a428080134b33d2aece12",
            "79c7c9550cf148399e28809952aa14dc",
            "052ced167fb34d5e9ab91546bf2145fa",
            "12ae05af6d944e7788961f85f534af81",
            "1cf1a7a0fd674c479e7e7f0970e19d0e",
            "079f75fc3bca48eaa41994a223143db4",
            "5720c6530c78407e95d3be5c0ffefe96",
            "5246db88d33f405d8ba763638f54712f",
            "45371a7cf1f34322816a24e7b7f217be",
            "4f91cb8255eb49e1928e6837bc05d6f6",
            "2db537ddb63448f2aee04b51b09f1ee8",
            "e4ca195b21044daa9209ee13b693876b",
            "7dbe2de0f3504151bdc7fddca286e437",
            "007d61256c6443e983de97e0927f23bf",
            "0016c01a0f354abda8f746586facaef0",
            "52644a5e642d4b9dabd4e48b287d8111",
            "a34042f4559646859f55d4f52c775c67",
            "c4ab328a85ee490f8098c0dbd19bd196",
            "688e39bce18945409d9904c2274d54fb",
            "8e2e2d5b8e7e41a39de76cc55f8ed825",
            "054cec92fc8b41dcaf49531e647d5d8c"
          ]
        },
        "id": "dgumXBbSXgeh",
        "outputId": "295f3648-f2b9-409f-9752-a003daaee865"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca4b5520a43f45b89e58df5b9801e921"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85075457aeb849f380df18db6a3c2cfe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d821a4dd86349e49e5cbcd20a2704d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2db537ddb63448f2aee04b51b09f1ee8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'our', 'friends', 'wo', 'n', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# preprocess data :\n",
        "\n",
        "1- first check the maximum size of sentences len in our dataset\n",
        "\n",
        "2- need to determine a fixed maximum length and process the data for the model\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "jrH19NGY1ol8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 0\n",
        "for sen in tokenized_texts :\n",
        "    count = len(sen)\n",
        "    if count> max_len :\n",
        "        max_len=count\n",
        "print(max_len)\n",
        "\n",
        "## the longest is 47. in the orginal paper is 512.\n",
        "MAX_LEN = 128\n",
        "\n",
        "## Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "print(f'input_ids[0] : {input_ids[0]}')\n",
        "\n",
        "## Pad our input tokens\n",
        "# from keras.preprocecing.sequence\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', truncating='post', padding='post')\n",
        "print(f'input_ids[0] after padding : {input_ids[0]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt4DDMAylQj0",
        "outputId": "c3adbb60-57d6-47f4-a537-f10d3f27adeb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n",
            "input_ids[0] : [101, 2256, 2814, 24185, 1050, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n",
            "input_ids[0] after padding : [  101  2256  2814 24185  1050  1005  1056  4965  2023  4106  1010  2292\n",
            "  2894  1996  2279  2028  2057 16599  1012   102     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create attention masks\n",
        "## Create a mask of 1s for each token followed by 0s for padding\n",
        "attention_masks = []\n",
        "\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "print(f\"attention_masks[0] : {attention_masks[0]} \" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dRFMxJ2g0H-",
        "outputId": "978d977e-a3a7-4015-ff21-15afe5e2b9a5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention_masks[0] : [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# in this step should convert to torch and be ready to set the model\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "UUff4WbXxon6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## title Splitting data into train and validation sets\n",
        "## Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
        "                                            labels, random_state=2018, test_size=0.1)\n",
        "\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "metadata": {
        "id": "rKL2ivtuhTYm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## convert data to torch tensor\n",
        "## Torch tensors are the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "metadata": {
        "id": "53JEoXn_hL4b"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Selecting a Batch Size and Creating and Iterator\n",
        "##For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "##Create an iterator of our data with torch DataLoader. This helps save on memory during training because :\n",
        "    ##unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "train_data = TensorDataset(train_inputs , train_masks , train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data , batch_size=BATCH_SIZE , sampler=train_sampler)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs , validation_masks , validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data , batch_size=BATCH_SIZE , sampler=validation_sampler)"
      ],
      "metadata": {
        "id": "bhoo-83axey-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Bert Configuration :\n",
        "\n",
        "## Initializing a BERT bert-base-uncased style configuration\n",
        "try:\n",
        "    import transformers\n",
        "except:\n",
        "    print(\"Installing transformers\")\n",
        "    !pip -qq install transformers\n",
        "\n",
        "from transformers import BertModel, BertConfig\n",
        "\n",
        "\n",
        "## Initializing a model from the bert-base-uncased style configuration\n",
        "\n",
        "#configuration = BertModel(BertConfig()).config\n",
        "configuration = BertConfig()\n",
        "model = BertModel(configuration)\n",
        "## Accessing the model configuration\n",
        "configuration = model.config\n",
        "print(configuration)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZWgJ3BX5ulD",
        "outputId": "184ec7d4-1a53-463d-8536-14baaadc1104"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Loading the Hugging Face Bert Uncased Base Model\n",
        "\n",
        "# from transformers import BertForSequenceClassification\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model = nn.DataParallel(model)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919,
          "referenced_widgets": [
            "03acda89295b4a3d842384049ed3ed9a",
            "e1de0d1802d947669495c961e3ada510",
            "18873deb1d6f4d3f9bf3f64436f072ba",
            "057959767b4449459379d55776860aae",
            "01ad09bbd733480289d14a720fe15ae7",
            "be3821aa0af4445c85fcb0dc40452bc7",
            "64ebac649fbc45faafbb58cafd498c6c",
            "1d4c0876046543689a5e5c16545f3a8e",
            "965f7fa431ad44cbaa296fbfa6393127",
            "a3247900cb71425fb4d95112bce16aa4",
            "e16e565bd7f34977bcbf79cee05c26af"
          ]
        },
        "id": "pE_qttOzGjwX",
        "outputId": "34b426b7-9a7b-4efd-a35a-e7eeab14d89e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03acda89295b4a3d842384049ed3ed9a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): BertForSequenceClassification(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSdpaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): BertPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[print(i) for i in model.named_parameters()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gYpBIIvRPpF",
        "outputId": "c77b303e-aa2f-443a-debb-2e276eac5746"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "         2.3162e-02, -6.5850e-03, -4.5545e-03, -1.3545e-02, -3.3110e-03,\n",
            "         1.2560e-02, -1.8602e-02, -8.0113e-03,  7.2269e-03, -2.9990e-02,\n",
            "        -2.8031e-02, -1.7690e-02, -1.5990e-02, -4.3190e-03,  5.8396e-03,\n",
            "         2.2739e-02, -1.1243e-02, -2.3899e-02, -9.9552e-03, -3.2969e-03,\n",
            "         2.5843e-04, -5.0730e-03, -1.7659e-02, -2.7961e-02, -3.1366e-02,\n",
            "        -3.4438e-02, -1.9468e-02,  8.3128e-03,  1.5317e-02, -1.7636e-02,\n",
            "        -3.9919e-03, -3.8846e-03,  1.1868e-03, -6.3348e-03,  2.9717e-02,\n",
            "        -5.9271e-03, -2.4939e-02,  9.1898e-03, -6.9647e-03,  7.7740e-02,\n",
            "         8.2349e-03, -1.8818e-03, -2.0718e-02, -3.8484e-02,  2.0732e-02,\n",
            "        -1.1053e-02, -1.8700e-02,  3.6481e-02,  1.5331e-02, -9.2603e-03,\n",
            "         3.8407e-02,  1.4102e-02, -4.8100e-02,  8.3475e-03, -2.5904e-02,\n",
            "        -1.7927e-02,  4.1530e-04, -2.4523e-02,  7.8518e-03,  2.7492e-02,\n",
            "        -1.4832e-03,  4.7044e-03, -3.0004e-02, -3.1918e-02, -2.3863e-03,\n",
            "         5.4427e-02,  6.8771e-03,  1.7696e-02,  2.0295e-02,  2.0656e-02,\n",
            "        -3.5771e-02,  4.6067e-02, -2.5592e-02, -8.0864e-03,  3.8550e-02,\n",
            "         4.9336e-02,  1.3200e-02,  3.2137e-02,  5.6092e-02,  4.0812e-04,\n",
            "        -3.7615e-02, -4.6335e-02, -8.1233e-03, -3.0353e-02, -2.1916e-02,\n",
            "        -6.1130e-02, -1.6654e-02,  1.8767e-02,  5.5183e-02, -1.5565e-02,\n",
            "         6.1049e-02, -6.5147e-02, -1.6593e-02,  1.8497e-02, -6.0566e-03,\n",
            "         3.2047e-02, -2.2300e-02,  4.6605e-02,  2.9127e-02, -4.0899e-03,\n",
            "        -1.8908e-02, -1.2942e-02,  1.4077e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.8.attention.output.dense.weight', Parameter containing:\n",
            "tensor([[ 0.0397, -0.0239,  0.0145,  ..., -0.0429,  0.0207, -0.0108],\n",
            "        [ 0.0531,  0.0100, -0.0091,  ..., -0.0104, -0.0167, -0.0250],\n",
            "        [-0.0130, -0.0448, -0.0697,  ..., -0.0032, -0.0190, -0.0317],\n",
            "        ...,\n",
            "        [-0.0361,  0.0353, -0.0670,  ...,  0.0118, -0.0048,  0.0087],\n",
            "        [ 0.0360,  0.0201,  0.0097,  ..., -0.0420,  0.0272,  0.0253],\n",
            "        [ 0.0142,  0.0111, -0.0127,  ...,  0.0122,  0.0119,  0.0474]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.8.attention.output.dense.bias', Parameter containing:\n",
            "tensor([ 1.4681e-02, -5.4066e-02, -6.2891e-02,  4.4840e-03,  2.4082e-02,\n",
            "        -7.3098e-02, -2.9787e-02,  4.5154e-03, -1.3620e-02, -3.0817e-02,\n",
            "         4.0890e-02, -6.8334e-04,  2.9737e-02, -2.7498e-02,  2.0787e-02,\n",
            "        -1.0357e-01, -7.0940e-02, -7.5689e-02,  2.5420e-02, -2.7607e-02,\n",
            "        -7.9087e-02,  2.3951e-02, -6.2280e-02, -5.3199e-02,  1.2222e-02,\n",
            "        -3.9758e-02,  3.4428e-02, -2.7722e-02,  3.9835e-02, -5.5624e-02,\n",
            "         2.7014e-03, -1.2508e-02, -8.2543e-02,  5.5015e-02,  4.0294e-04,\n",
            "         2.5569e-02,  4.4042e-04,  5.6152e-02, -3.2360e-02, -4.6095e-03,\n",
            "         2.6647e-02,  1.9164e-02,  2.1054e-02, -2.9984e-02,  4.1976e-02,\n",
            "        -1.4949e-03, -7.2505e-03, -4.2456e-02,  6.2808e-02, -1.1283e-02,\n",
            "         5.2470e-02, -1.4231e-02, -9.6822e-02,  3.6358e-03,  4.6504e-02,\n",
            "        -1.5774e-02, -3.9079e-02,  4.1418e-02,  8.9697e-03,  7.3899e-02,\n",
            "        -1.9247e-02,  9.2285e-03,  2.2218e-02, -8.3857e-02, -3.0109e-02,\n",
            "        -9.7241e-03,  1.9096e-02, -1.4194e-02, -9.5801e-03,  4.7747e-02,\n",
            "         2.3901e-02, -6.0742e-02, -1.5152e-02,  4.6145e-02, -7.9244e-02,\n",
            "         1.1301e-01,  6.3320e-02, -8.3932e-02, -6.8041e-02, -5.4522e-02,\n",
            "        -6.2869e-02,  2.4266e-02,  9.2899e-04, -5.1252e-02, -5.6326e-02,\n",
            "        -5.5143e-03,  3.1643e-03,  1.1350e-02,  7.5113e-03,  3.6056e-02,\n",
            "        -1.0175e-03,  3.0507e-02, -1.5428e-01, -3.4266e-02,  4.4927e-02,\n",
            "         6.1221e-03,  1.4266e-02,  2.0828e-03, -3.4110e-02,  1.4717e-02,\n",
            "         2.1498e-02, -1.0509e-02, -8.6130e-02,  6.4144e-02, -2.4359e-03,\n",
            "         7.6318e-03,  8.3538e-03, -2.1387e-02,  8.3970e-02, -1.0363e-02,\n",
            "         3.9162e-03,  5.5387e-02,  6.2889e-03,  5.4142e-03,  2.0808e-02,\n",
            "        -2.4140e-02, -3.9297e-03,  5.0387e-02, -3.3358e-02, -6.9041e-02,\n",
            "        -2.7131e-02,  6.5914e-02,  1.5775e-02, -1.0327e-01, -4.7423e-02,\n",
            "         2.3013e-02, -3.2198e-03, -1.6355e-02,  3.5229e-02,  8.8725e-02,\n",
            "         6.2658e-03, -1.1406e-02, -7.4799e-02, -7.7722e-02,  1.6772e-02,\n",
            "         1.3844e-02, -1.0805e-01, -2.7898e-02, -2.1714e-02, -1.0015e-01,\n",
            "         2.0424e-02, -4.7858e-03, -2.3213e-02,  5.2894e-03, -6.1647e-02,\n",
            "         5.1289e-02, -1.2153e-02,  5.2169e-02, -1.0038e-02, -4.7817e-02,\n",
            "         9.6150e-02,  2.2410e-02,  1.3261e-02,  6.4548e-03,  2.2933e-02,\n",
            "        -3.2256e-02, -3.3254e-02, -3.7654e-02, -1.7746e-02, -8.6890e-02,\n",
            "        -3.2358e-02,  2.4124e-03,  2.0466e-02,  5.1601e-02, -2.6273e-03,\n",
            "         1.7871e-02, -6.0208e-02,  5.2685e-03,  7.7299e-02,  1.0162e-03,\n",
            "        -3.0960e-02,  2.9450e-02,  3.6294e-02, -2.8493e-03, -1.3697e-02,\n",
            "        -3.4026e-02, -2.7654e-02,  2.6397e-02,  6.0340e-02,  4.1470e-02,\n",
            "        -4.1489e-03, -2.2562e-02,  2.7738e-02, -6.5800e-04, -7.8615e-03,\n",
            "         5.4576e-02, -4.1042e-02,  2.7778e-02, -1.0358e-01,  7.5783e-03,\n",
            "         2.7314e-02,  1.1898e-01, -6.1984e-02,  4.0939e-02, -3.7525e-02,\n",
            "        -1.1893e-01, -4.1150e-02, -1.0247e-01, -4.8443e-02,  5.7311e-02,\n",
            "         7.1129e-02, -6.8642e-02, -1.4135e-02,  9.7113e-02,  2.3788e-02,\n",
            "         5.8958e-02,  4.3074e-02, -4.6767e-02, -7.6072e-03, -3.2971e-02,\n",
            "        -4.9040e-02,  5.1826e-02,  9.0072e-03, -6.2082e-02, -1.6710e-02,\n",
            "        -5.8393e-03,  1.0344e-01,  5.7900e-02, -5.9893e-02,  6.6854e-02,\n",
            "         2.9629e-02, -3.9012e-02, -3.8866e-02, -7.6514e-02, -1.1047e-02,\n",
            "        -9.2434e-02, -2.5678e-02,  6.9779e-02, -6.4792e-02,  2.5517e-02,\n",
            "         6.7643e-02, -1.5618e-02, -5.4297e-02,  6.3464e-02,  2.3966e-03,\n",
            "        -2.5232e-02, -4.6245e-02,  5.8592e-02, -9.2165e-02,  3.8205e-02,\n",
            "        -2.9618e-02, -8.0456e-02,  1.1557e-01,  2.0671e-02,  6.8024e-02,\n",
            "        -1.4486e-02, -5.8533e-02,  5.5495e-02,  4.8581e-02, -2.6991e-02,\n",
            "         1.0926e-02, -1.9179e-01, -5.8153e-02, -2.8856e-02, -1.1110e-01,\n",
            "         7.6401e-02,  3.4105e-02,  3.1304e-02, -8.7253e-02,  3.3290e-02,\n",
            "         3.9091e-02,  3.8653e-02,  3.5667e-03,  1.9834e-02, -5.1177e-03,\n",
            "         1.0319e-01, -2.4676e-02,  3.2521e-02, -2.8721e-02,  2.0794e-02,\n",
            "         2.5350e-03, -8.8907e-03, -2.6278e-02, -4.9622e-02, -6.3749e-03,\n",
            "        -5.7303e-02, -4.2266e-02,  2.5278e-03, -7.6742e-02,  4.0946e-02,\n",
            "         2.6616e-02, -3.7431e-02, -1.8410e-02,  8.7063e-02,  1.9218e-02,\n",
            "        -3.5425e-02, -1.9042e-02,  6.4625e-02,  3.3676e-02,  3.3825e-02,\n",
            "        -7.7980e-02, -1.0784e-01,  2.3079e-03,  1.0262e-02, -4.0258e-02,\n",
            "        -2.0764e-02, -2.8118e-02, -2.6973e-02, -5.4145e-02, -2.8428e-02,\n",
            "        -1.6195e-02,  6.4060e-04, -6.9473e-03,  1.1217e-02, -1.9563e-01,\n",
            "         7.3998e-02, -8.9053e-03, -1.4043e-02,  1.2157e+00,  3.2087e-02,\n",
            "         4.0108e-02,  2.0360e-02, -3.8572e-03,  7.4940e-03, -5.0169e-02,\n",
            "        -6.1510e-02, -3.5024e-02, -2.4799e-02,  2.4922e-02, -3.0134e-02,\n",
            "        -6.1142e-03, -5.0137e-02, -6.3522e-02,  1.1062e-01,  8.8626e-03,\n",
            "         2.0531e-02, -5.5657e-02,  4.3044e-02, -1.1003e-02,  4.9463e-02,\n",
            "        -3.7874e-02, -3.1286e-02,  1.4668e-03, -4.7391e-02,  8.3346e-02,\n",
            "         1.1376e-02, -4.2750e-02,  2.3355e-02,  1.9958e-02,  4.7395e-02,\n",
            "        -6.2040e-02, -1.7714e-02, -1.0600e-01,  4.2801e-03,  2.0182e-02,\n",
            "         4.4002e-02,  9.6383e-03,  3.8340e-02, -2.1841e-02, -4.7436e-03,\n",
            "         3.7288e-02,  5.9915e-02,  5.7653e-03,  4.6285e-02, -1.6712e-02,\n",
            "         1.2529e-02, -2.6969e-02, -8.3025e-02,  5.3555e-02, -2.9242e-02,\n",
            "         6.8520e-02, -3.1568e-02, -2.0418e-02, -3.9028e-02, -3.9742e-02,\n",
            "        -2.5031e-02, -4.9433e-02, -7.5576e-02, -3.5555e-02, -9.7042e-05,\n",
            "         2.8356e-02,  6.1653e-02, -6.0994e-03,  2.2635e-02, -3.4275e-02,\n",
            "        -3.2411e-02,  1.9559e-02,  1.1804e-02,  7.0637e-02, -6.9882e-04,\n",
            "        -3.5074e-02,  2.2629e-01,  8.4978e-03,  3.0544e-02,  1.4565e-02,\n",
            "        -2.2957e-02,  5.7673e-02,  1.7200e-02, -3.6492e-02,  4.5881e-02,\n",
            "         2.8431e-02, -1.1186e-02, -5.1710e-02,  1.3281e-02, -5.1293e-02,\n",
            "         2.1434e-02,  4.6834e-02,  2.2433e-02, -3.8009e-02, -1.5834e-02,\n",
            "         7.3873e-02,  1.7258e-03, -1.0754e-01, -7.1107e-03,  6.5692e-02,\n",
            "         1.3519e-02, -2.6093e-02, -1.1058e-02, -4.4562e-03,  1.5061e-02,\n",
            "         1.8063e-02,  1.9262e-02,  2.2487e-02,  1.9683e-02,  2.6294e-02,\n",
            "        -2.8451e-02, -1.5648e-02,  6.6467e-02,  1.0338e-02, -4.9542e-02,\n",
            "        -9.9936e-03,  6.9006e-02, -3.9705e-02,  6.1353e-02,  2.1877e-02,\n",
            "        -5.0988e-02, -2.8155e-02,  1.5162e-02,  8.1590e-02, -1.2260e-02,\n",
            "         6.1998e-02,  2.9923e-02,  7.1398e-02, -5.6479e-02, -2.0166e-02,\n",
            "        -2.6077e-02, -5.5910e-02,  4.4736e-02,  8.1582e-03, -5.4216e-02,\n",
            "         2.6645e-02,  6.3330e-03, -8.6785e-03,  9.5597e-02, -2.9095e-02,\n",
            "         3.6620e-04,  3.2107e-03, -1.7101e-02,  6.3900e-02, -3.5177e-02,\n",
            "        -5.4500e-02,  4.5418e-02, -2.5977e-02,  3.6403e-02, -8.0653e-03,\n",
            "        -2.5721e-02,  4.3829e-02, -9.6265e-03,  5.3269e-02,  7.9986e-03,\n",
            "        -1.9754e-02,  1.0664e-01,  3.0417e-03,  1.1992e-02,  3.6192e-02,\n",
            "        -7.5087e-02, -2.6586e-02,  6.8239e-02,  6.0076e-03,  2.8654e-02,\n",
            "         2.4770e-02, -3.8975e-02,  1.3452e-02, -2.8505e-02, -7.9431e-03,\n",
            "         5.4149e-02, -2.8257e-02, -5.7499e-03,  3.1737e-02,  5.4220e-02,\n",
            "         3.8931e-02, -4.9403e-03,  2.9054e-02,  1.4637e-02,  1.0058e-02,\n",
            "        -2.5202e-03, -5.5117e-02, -4.8385e-02,  2.5112e-02, -4.3502e-02,\n",
            "         6.9383e-02,  7.3598e-04, -2.3009e-03,  2.1506e-02,  2.0524e-02,\n",
            "        -2.6463e-02, -1.4139e-02,  2.3322e-02, -7.9954e-02, -1.6620e-02,\n",
            "         2.6249e-03, -8.3204e-02, -3.9843e-03,  5.2497e-02,  1.7727e-02,\n",
            "        -3.4119e-02,  2.1007e-02,  9.3587e-03,  4.7775e-02, -2.5852e-02,\n",
            "         2.1770e-02, -9.9064e-03,  8.6589e-02,  4.4021e-02,  5.7886e-02,\n",
            "        -1.2250e-01,  2.1355e-02,  6.0990e-03,  1.0273e-02,  9.3130e-02,\n",
            "         3.5367e-03, -1.6839e-02, -7.6280e-03,  4.4022e-02,  1.0221e-02,\n",
            "         5.0774e-02,  4.4902e-02,  2.6917e-02, -2.3493e-02, -6.7071e-02,\n",
            "        -1.3298e-03,  2.3782e-02, -2.4929e-02, -6.7760e-02, -5.5360e-03,\n",
            "         7.3648e-02, -7.2348e-02,  4.6346e-02, -5.0559e-02,  4.2068e-02,\n",
            "        -2.8992e-02, -1.0759e-02, -1.8697e-03,  3.5629e-02,  7.4038e-03,\n",
            "         5.6024e-02, -5.9133e-03,  2.5873e-03, -4.2873e-02,  3.4346e-03,\n",
            "         3.5722e-02,  4.2984e-02,  4.2554e-02,  6.6562e-04,  6.5072e-02,\n",
            "         3.2047e-02, -2.8033e-02, -4.0067e-02, -1.9896e-02,  2.6844e-02,\n",
            "         1.5713e-02,  2.7007e-02,  1.7208e-03,  2.7382e-02, -2.8528e-02,\n",
            "        -9.5764e-03, -3.7873e-03,  4.8190e-02, -3.1590e-02,  2.1014e-02,\n",
            "        -2.0368e-02,  5.4421e-02, -7.1725e-02, -8.6915e-03,  2.6952e-02,\n",
            "        -7.8349e-02, -9.0707e-03,  9.5074e-02,  3.2150e-02,  1.1448e-02,\n",
            "         5.6572e-02, -7.6614e-02, -2.5835e-02,  6.2667e-02,  5.7868e-02,\n",
            "         3.0811e-02, -7.5745e-02,  2.4123e-02, -1.3416e-02,  2.8002e-03,\n",
            "         3.0532e-02,  3.7419e-02, -6.3580e-03,  3.0664e-02, -1.6023e-02,\n",
            "         1.3474e-02, -4.7145e-02,  1.6537e-02,  3.0591e-02, -1.4612e-02,\n",
            "        -4.8427e-02,  8.4595e-03,  1.5825e-02, -4.0959e-02, -4.3765e-03,\n",
            "        -3.5155e-02,  1.7474e-02,  1.6672e-02, -2.6941e-02, -6.5927e-03,\n",
            "         1.3952e-02,  5.1979e-02,  3.4947e-02,  2.1694e-02,  5.3444e-02,\n",
            "         4.4460e-02,  2.3744e-02,  3.1567e-03,  3.1133e-02,  5.1904e-02,\n",
            "        -7.2177e-02,  5.8114e-02,  1.6292e-02, -9.4155e-04,  4.1362e-02,\n",
            "         1.0547e-02, -6.4471e-02, -8.1723e-02, -8.5331e-02,  3.6440e-02,\n",
            "        -3.4340e-02, -4.3104e-02, -3.2928e-02, -1.0634e-01,  6.1039e-03,\n",
            "        -1.0104e-02, -4.3732e-02,  1.1884e-02, -1.1804e-01,  3.0080e-02,\n",
            "        -3.3518e-02, -3.2584e-02, -1.1679e-02,  2.3576e-02,  6.7879e-02,\n",
            "         4.8604e-02,  1.2419e-02,  3.3853e-02,  4.8899e-02, -6.6467e-02,\n",
            "        -2.8840e-02, -3.0531e-03, -5.1256e-02,  2.3428e-02, -2.9860e-02,\n",
            "         4.6447e-02, -1.1162e-01, -3.4130e-02, -2.4756e-02,  3.2605e-02,\n",
            "        -2.0136e-02,  3.9484e-02, -1.0728e-01,  4.5709e-02, -9.8205e-03,\n",
            "        -6.3821e-03, -1.0939e-02,  2.4898e-02, -1.0018e-01, -4.9592e-02,\n",
            "        -1.0217e-01, -1.2094e-02,  6.5450e-02, -1.0138e-02, -2.9353e-02,\n",
            "         6.4286e-02, -5.8459e-02, -1.1556e-01,  2.9296e-02,  3.7099e-03,\n",
            "        -9.7565e-03,  1.9144e-02, -4.2326e-02,  3.3376e-02,  1.3709e-02,\n",
            "         1.0707e-02, -6.7155e-02, -2.7553e-02,  1.4038e-02,  2.4912e-02,\n",
            "        -4.0199e-02, -3.0072e-02,  6.5731e-02,  3.5857e-02,  2.3238e-02,\n",
            "        -4.4355e-02,  5.4498e-02, -1.8013e-02,  4.8736e-02,  1.5143e-02,\n",
            "         2.6003e-02, -2.8261e-02,  6.9108e-02, -2.3402e-02, -4.0987e-02,\n",
            "        -5.1446e-02, -3.5296e-02, -1.3113e-02, -2.8889e-04,  1.2197e-02,\n",
            "         4.2267e-02,  4.3723e-02,  7.0688e-04,  2.9120e-02,  1.3186e-02,\n",
            "        -5.9160e-02, -7.0748e-03,  2.0481e-03, -1.4148e-02,  3.9235e-02,\n",
            "         9.1184e-03, -4.1008e-02, -4.4105e-02,  3.2389e-02, -5.8922e-02,\n",
            "         1.6732e-02,  1.0010e-01, -1.0132e-02,  2.0730e-02,  2.3643e-02,\n",
            "        -4.7588e-03, -1.9256e-02, -5.3117e-03, -3.4298e-02, -2.2995e-02,\n",
            "         6.8836e-02,  2.8596e-02, -5.7906e-02,  1.5085e-02,  3.2886e-02,\n",
            "         3.7642e-02, -3.7154e-02,  5.9706e-02,  2.3366e-02, -4.5664e-02,\n",
            "        -6.9182e-02, -1.1802e-03, -1.1014e-02,  1.1098e-02, -7.9132e-02,\n",
            "        -1.1768e-03,  2.6919e-02,  6.2008e-02, -2.1814e-02,  5.6559e-02,\n",
            "         5.0561e-02, -2.1259e-02, -4.2579e-02,  3.2463e-02,  5.1930e-02,\n",
            "         9.0223e-03,  2.2249e-02, -1.0605e-01,  4.3226e-02,  9.7467e-02,\n",
            "        -3.1670e-02,  1.8082e-02, -6.1662e-03], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.8.attention.output.LayerNorm.weight', Parameter containing:\n",
            "tensor([0.8908, 0.8912, 0.8116, 0.7810, 0.9046, 0.8539, 0.8949, 0.8632, 0.8615,\n",
            "        0.8553, 0.7586, 0.7825, 0.7810, 0.7848, 0.8592, 0.8605, 0.8111, 0.8043,\n",
            "        0.9238, 0.8334, 0.8583, 0.7965, 0.8517, 0.8636, 0.7861, 0.8805, 0.7569,\n",
            "        0.8774, 0.7820, 0.8401, 0.8882, 0.7960, 0.8682, 0.8827, 0.8477, 0.8515,\n",
            "        0.8630, 0.8383, 0.8712, 0.8518, 0.8452, 0.8744, 0.8910, 0.8639, 0.8180,\n",
            "        0.7845, 0.9137, 0.8614, 0.8759, 0.8616, 0.8605, 0.8446, 0.9184, 0.7615,\n",
            "        0.8322, 0.7946, 0.8233, 0.7546, 0.7876, 0.7706, 0.8726, 0.7936, 0.7487,\n",
            "        0.8152, 0.8308, 0.8398, 0.8232, 0.8691, 0.8340, 0.8828, 0.8837, 0.8959,\n",
            "        0.8483, 0.7884, 0.8597, 0.7645, 0.8296, 0.7724, 0.7680, 0.8156, 0.8494,\n",
            "        0.9142, 0.8876, 0.8160, 0.7374, 0.8071, 0.8276, 0.8066, 0.8716, 0.8896,\n",
            "        0.8099, 0.8310, 0.8187, 0.7616, 0.8063, 0.7962, 0.7728, 0.7769, 0.8203,\n",
            "        0.8972, 0.9047, 0.9095, 0.8652, 0.8382, 0.7504, 0.6943, 0.7765, 0.8866,\n",
            "        0.8538, 0.8118, 0.8457, 0.8148, 0.7895, 0.8516, 0.8314, 0.7906, 0.8118,\n",
            "        0.8653, 0.8519, 0.8154, 0.8068, 0.7781, 0.8017, 0.8584, 0.7417, 0.8088,\n",
            "        0.8404, 0.7744, 0.8649, 0.8039, 0.8376, 0.8145, 0.7568, 0.8571, 0.8112,\n",
            "        0.8787, 0.8299, 0.8236, 0.8412, 0.7311, 0.8927, 0.9002, 0.8956, 0.9526,\n",
            "        0.8160, 0.7533, 0.8686, 0.8274, 0.8601, 0.8464, 0.7887, 0.8272, 0.7487,\n",
            "        0.8802, 0.8552, 0.7876, 0.8271, 0.8050, 0.8548, 0.7456, 0.8622, 0.8773,\n",
            "        0.8034, 0.9093, 0.8087, 0.8698, 0.8437, 0.7889, 0.8080, 0.8516, 0.8541,\n",
            "        0.8410, 0.7939, 0.8558, 0.9246, 0.7931, 0.7500, 0.8215, 0.7794, 0.8158,\n",
            "        0.8194, 0.8959, 0.8334, 0.8791, 0.8777, 0.8736, 0.8987, 0.8401, 0.8618,\n",
            "        0.7983, 0.8627, 0.8933, 0.8764, 0.7962, 0.8502, 0.8755, 0.8680, 0.8051,\n",
            "        0.8403, 0.7528, 0.8400, 0.8171, 0.7393, 0.7880, 0.7982, 0.8491, 0.8180,\n",
            "        0.7542, 0.8392, 0.7935, 0.7821, 0.8689, 0.8121, 0.8717, 0.8074, 0.7300,\n",
            "        0.7703, 0.9228, 0.7964, 0.8053, 0.8403, 0.8694, 0.8544, 0.7474, 0.8244,\n",
            "        0.8529, 0.8174, 0.8006, 0.8451, 0.9096, 0.8334, 0.8603, 0.8288, 0.8595,\n",
            "        0.8513, 0.8241, 0.8387, 0.7908, 0.8525, 0.7837, 0.7803, 0.8755, 0.8537,\n",
            "        0.7834, 0.8043, 0.7114, 0.7878, 0.8242, 0.7866, 0.7796, 0.8351, 0.8923,\n",
            "        0.8236, 0.8335, 0.8933, 0.8200, 0.7647, 0.8362, 0.7864, 0.8011, 0.9132,\n",
            "        0.8127, 0.8728, 0.8181, 0.8005, 0.8268, 0.8181, 0.8178, 0.8092, 0.8571,\n",
            "        0.8283, 0.8543, 0.8434, 0.7961, 0.8637, 0.8499, 0.7304, 0.8047, 0.7929,\n",
            "        0.8762, 0.8035, 0.9016, 0.8823, 0.7172, 0.8587, 0.7966, 0.8824, 0.8976,\n",
            "        0.8129, 0.8565, 0.8010, 0.7947, 0.8766, 0.8497, 0.8542, 0.7666, 0.7894,\n",
            "        0.8424, 0.8681, 0.8573, 0.9176, 0.8383, 0.8636, 0.8612, 0.9353, 0.8505,\n",
            "        0.8138, 0.9061, 0.6144, 0.8552, 0.8251, 0.7582, 0.8132, 0.8204, 0.8560,\n",
            "        0.7999, 0.8871, 0.8531, 0.8443, 0.7255, 0.8467, 0.8181, 0.8060, 0.8861,\n",
            "        0.8707, 0.8313, 0.7747, 0.8537, 0.8461, 0.8346, 0.7796, 0.6952, 0.7939,\n",
            "        0.9130, 0.9183, 0.8161, 0.8095, 0.8728, 0.8895, 0.8295, 0.7896, 0.9133,\n",
            "        0.7711, 0.8350, 0.7630, 0.7445, 0.8752, 0.8700, 0.8233, 0.9533, 0.8351,\n",
            "        0.8709, 0.8997, 0.7026, 0.8592, 0.8451, 0.8210, 0.8102, 0.7948, 0.8165,\n",
            "        0.8504, 0.7895, 0.7816, 0.8117, 0.8293, 0.8808, 0.8025, 0.8145, 0.8593,\n",
            "        0.7861, 0.8604, 0.8284, 0.8838, 0.8390, 0.9301, 0.8983, 0.8570, 0.8159,\n",
            "        0.8291, 0.9241, 0.8437, 3.4025, 0.8622, 0.8977, 0.8531, 0.8417, 0.8489,\n",
            "        0.9048, 0.8595, 0.8661, 0.8051, 0.8298, 0.8414, 0.8068, 0.8558, 0.8749,\n",
            "        0.8263, 0.8196, 0.8555, 0.8793, 0.8215, 0.7524, 0.7894, 0.8324, 0.7785,\n",
            "        0.7926, 0.8605, 0.8168, 0.8578, 0.8750, 0.8242, 0.8330, 0.9023, 0.8560,\n",
            "        0.8105, 0.7883, 0.7978, 0.8335, 0.8215, 0.7734, 0.8058, 0.7318, 0.8638,\n",
            "        0.8398, 0.8118, 0.8254, 0.8009, 0.8072, 0.8383, 0.8164, 0.8269, 0.8372,\n",
            "        0.8352, 0.8031, 0.8644, 0.9068, 0.8410, 0.7822, 0.8493, 0.8297, 0.8099,\n",
            "        0.8726, 0.7960, 0.7305, 0.8213, 0.9007, 0.9344, 0.8041, 0.8674, 0.8086,\n",
            "        0.8351, 0.9016, 0.9196, 0.8307, 0.9276, 0.8836, 0.8135, 0.8452, 0.8368,\n",
            "        0.8793, 0.8405, 0.7980, 0.8582, 0.8969, 0.8847, 0.7909, 0.8089, 0.8608,\n",
            "        0.7822, 0.7908, 0.9202, 0.7465, 0.8523, 0.8637, 0.7618, 0.8501, 0.7981,\n",
            "        0.9070, 0.8185, 0.8287, 0.8591, 0.8356, 0.7556, 0.8299, 0.8172, 0.8345,\n",
            "        0.8250, 0.8551, 0.7645, 0.8471, 0.8827, 0.8612, 0.8263, 0.9176, 0.8656,\n",
            "        0.7951, 0.7627, 0.7865, 0.9047, 0.7967, 0.8391, 0.8771, 0.7732, 0.8569,\n",
            "        0.7854, 0.8818, 0.8078, 0.8280, 0.8746, 0.8532, 0.7675, 0.7908, 0.8141,\n",
            "        0.8202, 0.8285, 0.8988, 0.8489, 0.7125, 0.8795, 0.8200, 0.8836, 0.7692,\n",
            "        0.8953, 0.8182, 0.8134, 0.8332, 0.8025, 0.8398, 0.8481, 0.7610, 0.7973,\n",
            "        0.7827, 0.8684, 0.8341, 0.8832, 0.7954, 0.8414, 0.8262, 0.7873, 0.8964,\n",
            "        0.7905, 0.7861, 0.8404, 0.8136, 0.8673, 0.8462, 0.8371, 0.7794, 0.8470,\n",
            "        0.8483, 0.8981, 0.8013, 0.8592, 0.8954, 0.8770, 0.8335, 0.8438, 0.8172,\n",
            "        0.8766, 0.8461, 0.8540, 0.8068, 0.8132, 0.9077, 0.8307, 0.8664, 0.7748,\n",
            "        0.8673, 0.8287, 0.8649, 0.7932, 0.8382, 0.8437, 0.7486, 0.8215, 0.8351,\n",
            "        0.8260, 0.7630, 0.9100, 0.8811, 0.8021, 0.8530, 0.8757, 0.7984, 0.8577,\n",
            "        0.8063, 0.9532, 0.8885, 0.7906, 0.8843, 0.8951, 0.8479, 0.8964, 0.8463,\n",
            "        0.7880, 0.8156, 0.8228, 0.8268, 0.8376, 0.8557, 0.7977, 0.8970, 0.8879,\n",
            "        0.8272, 0.8630, 0.8020, 0.9120, 0.7860, 0.8591, 0.8219, 0.8207, 0.8158,\n",
            "        0.8674, 0.7504, 0.7055, 0.8135, 0.7805, 0.9132, 0.8418, 0.8258, 0.8220,\n",
            "        0.8092, 0.8873, 0.8419, 0.8420, 0.8269, 0.8598, 0.8256, 0.7964, 0.8981,\n",
            "        0.8372, 0.7730, 0.8037, 0.8646, 0.8230, 0.7974, 0.8706, 0.8562, 0.7691,\n",
            "        0.8452, 0.8311, 0.8114, 0.8247, 0.7928, 0.8806, 0.8628, 0.8399, 0.8610,\n",
            "        0.7891, 0.8500, 0.9076, 0.8322, 0.8328, 0.8067, 0.7474, 0.8107, 0.8018,\n",
            "        0.8562, 0.8479, 0.8404, 0.8762, 0.9094, 0.9014, 0.9081, 0.8485, 0.7985,\n",
            "        0.8785, 0.8346, 0.8532, 0.8013, 0.7661, 0.8480, 0.8435, 0.8264, 0.8738,\n",
            "        0.8541, 0.8386, 0.8279, 0.8713, 0.8745, 0.8828, 0.9309, 0.8279, 0.8401,\n",
            "        0.8710, 0.8519, 0.7929, 0.8608, 0.8962, 0.8455, 0.8347, 0.7980, 0.8267,\n",
            "        0.8964, 0.8296, 0.8335, 0.8335, 0.7587, 0.7868, 0.7578, 0.9360, 0.8573,\n",
            "        0.7926, 0.8074, 0.8578, 0.7929, 0.8097, 0.7944, 0.9015, 0.8594, 0.7642,\n",
            "        0.8223, 0.8303, 0.8192, 0.7780, 0.8906, 0.8487, 0.8197, 0.8667, 0.8022,\n",
            "        0.6826, 0.8482, 0.8942, 0.8156, 0.8485, 0.8896, 0.8325, 0.8286, 0.8003,\n",
            "        0.8506, 0.8416, 0.8493, 0.8530, 0.8996, 0.8223, 0.7813, 0.8961, 0.8402,\n",
            "        0.8854, 0.7511, 0.8210, 0.8531, 0.8829, 0.8752, 0.7644, 0.8361, 0.8323,\n",
            "        0.8168, 0.8641, 0.8670, 0.9012, 0.8673, 0.7959, 0.8590, 0.8414, 0.8077,\n",
            "        0.8279, 0.8624, 0.8288, 0.7537, 0.8382, 0.8065, 0.8248, 0.8501, 0.8324,\n",
            "        0.7729, 0.8069, 0.8780], device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.8.attention.output.LayerNorm.bias', Parameter containing:\n",
            "tensor([-6.0043e-02,  4.5727e-02,  8.6881e-02, -1.4417e-01, -5.5005e-02,\n",
            "        -1.0474e-02,  2.5043e-01,  9.0392e-02, -8.2090e-02, -1.7247e-01,\n",
            "         4.2001e-02,  1.0978e-01, -1.4906e-01,  2.4737e-02, -8.9109e-02,\n",
            "         2.9581e-01,  1.9528e-01,  9.7223e-02, -6.8090e-02,  4.3327e-04,\n",
            "         1.7526e-01, -6.0136e-03, -4.7597e-02,  9.9404e-02,  4.7321e-02,\n",
            "         8.6658e-02, -6.5222e-03, -2.8466e-02, -1.7913e-01,  7.4106e-02,\n",
            "         7.8910e-04,  5.6712e-02,  2.0257e-01, -9.9306e-02, -8.5908e-02,\n",
            "         1.0107e-01, -2.1124e-01, -1.4615e-01, -5.0545e-02, -1.0593e-01,\n",
            "        -1.7662e-01, -1.1018e-01, -4.3122e-02,  9.1960e-02,  6.0818e-02,\n",
            "        -4.0597e-02,  5.4103e-02,  9.0968e-02, -9.6644e-02, -4.9721e-02,\n",
            "        -1.3164e-01,  6.5675e-02,  2.3594e-01,  2.8029e-02,  8.5331e-02,\n",
            "         8.3291e-02,  5.0295e-02, -2.5654e-01, -5.1320e-02, -2.0234e-01,\n",
            "         4.7726e-02,  1.8026e-01, -1.8444e-03, -9.4534e-02, -4.9336e-02,\n",
            "         1.2817e-01, -1.1062e-01,  1.0549e-02, -1.2287e-01, -1.2144e-01,\n",
            "        -8.1803e-02,  1.2332e-01, -9.2295e-02, -6.5228e-02,  3.2322e-02,\n",
            "         7.1861e-04, -8.9351e-02,  1.8048e-02,  1.2386e-01,  5.9615e-02,\n",
            "        -6.2845e-04, -3.5705e-02,  1.2916e-01,  1.1454e-01,  6.1890e-02,\n",
            "        -1.3117e-01, -2.9013e-02, -9.9673e-02, -1.8641e-01,  8.3851e-02,\n",
            "        -1.2004e-01, -2.0634e-01,  2.1076e-01, -2.1131e-02,  1.6728e-02,\n",
            "        -2.2085e-02,  6.7730e-02, -5.8642e-02,  8.0292e-02, -1.6058e-01,\n",
            "        -4.2313e-02, -1.0736e-01,  1.5815e-01, -8.4193e-02, -2.3722e-03,\n",
            "        -2.3385e-02, -6.1833e-02, -1.6079e-01, -1.0358e-01,  3.9939e-02,\n",
            "        -7.7048e-02,  1.8026e-02, -7.5784e-02, -9.0620e-02, -9.3887e-02,\n",
            "         2.6258e-02, -5.9140e-02, -1.2390e-01, -2.1069e-02,  1.8856e-02,\n",
            "        -2.0466e-02,  2.4606e-02,  4.5392e-02,  2.1690e-01,  1.1361e-01,\n",
            "        -2.7944e-01,  5.0200e-02,  5.4876e-02, -1.8211e-01, -2.6893e-02,\n",
            "        -8.2728e-02,  8.3858e-03, -1.3455e-01, -1.3325e-02, -1.9354e-01,\n",
            "        -4.4718e-02,  1.3886e-01, -1.9235e-02, -2.3992e-03, -2.3986e-02,\n",
            "        -7.4799e-02, -3.4916e-02, -6.4329e-02,  1.1733e-01,  1.3396e-01,\n",
            "        -1.8087e-01, -2.8187e-02, -1.0119e-01, -1.9703e-01,  1.7343e-01,\n",
            "        -1.9150e-01,  1.3186e-01, -1.6688e-01, -2.2447e-03, -1.5719e-01,\n",
            "         4.8532e-02, -6.9248e-03, -5.6537e-03, -5.4428e-02, -2.2368e-02,\n",
            "        -5.9284e-02, -1.0984e-01, -5.9494e-02, -1.0380e-01,  9.7611e-02,\n",
            "         1.5778e-02,  1.5968e-01,  8.5311e-02, -2.0677e-01,  1.7927e-02,\n",
            "        -7.6926e-02, -1.3392e-01, -6.8154e-02, -8.3804e-02, -4.3180e-02,\n",
            "         1.2472e-01,  1.0270e-01, -7.3696e-02,  2.5068e-02, -2.8000e-02,\n",
            "        -8.2574e-02, -4.1151e-03, -6.2540e-02, -1.2839e-01,  1.0978e-02,\n",
            "        -1.1873e-01,  1.8827e-02, -1.3362e-01,  1.6466e-01, -2.8234e-02,\n",
            "        -1.2024e-01, -1.3766e-01,  1.3024e-01, -1.9420e-01,  1.1318e-01,\n",
            "        -4.5297e-02, -5.0479e-02,  1.6549e-01, -3.7485e-02, -9.7978e-02,\n",
            "        -1.6516e-01, -2.8699e-02,  1.7943e-02, -2.0056e-01, -9.9338e-02,\n",
            "        -1.9311e-03, -5.4235e-02, -1.1880e-02,  2.3034e-02,  1.1718e-01,\n",
            "        -1.4116e-01, -9.8754e-02,  2.1158e-02,  2.5026e-01, -1.0230e-01,\n",
            "        -3.5182e-02, -2.7351e-01, -7.4305e-02,  2.4034e-01, -1.7391e-01,\n",
            "         4.2071e-02,  1.1843e-01, -6.8441e-02,  6.3684e-02, -1.4631e-01,\n",
            "         3.6316e-01,  6.8980e-02, -1.7664e-01,  5.1110e-02, -1.6003e-02,\n",
            "        -2.5529e-01,  3.1410e-02,  1.7439e-01, -1.2248e-01, -9.4493e-02,\n",
            "         2.1938e-02,  1.7721e-02, -1.4337e-01,  1.2564e-01, -1.6899e-01,\n",
            "         6.0880e-03,  1.8392e-01, -2.8349e-01,  1.4458e-02, -7.3361e-02,\n",
            "         7.2291e-03, -3.2562e-02, -6.2404e-02, -9.7708e-02, -2.6319e-02,\n",
            "        -1.7973e-01,  2.5711e-01,  9.0294e-02, -9.4203e-02,  1.4639e-01,\n",
            "        -2.6298e-02, -1.9028e-01, -2.0116e-01,  1.3522e-01,  5.8927e-02,\n",
            "         1.1420e-02, -1.5650e-02, -5.3074e-02, -2.7232e-02, -9.8585e-02,\n",
            "        -3.1707e-01, -1.4956e-02, -6.1652e-02,  5.5418e-02, -2.8167e-03,\n",
            "        -7.4810e-02,  9.4731e-02, -1.6868e-01,  9.3867e-02, -6.9943e-03,\n",
            "        -3.1934e-03,  6.3079e-04,  1.2580e-02, -4.1347e-02, -1.5103e-01,\n",
            "        -1.0031e-01,  1.2231e-01, -1.8633e-01, -6.6870e-02, -7.8260e-02,\n",
            "        -2.5207e-02,  1.4011e-02, -1.2848e-01, -9.8367e-02, -8.6895e-02,\n",
            "        -9.4088e-03,  5.7982e-03, -1.3161e-01, -1.3052e-02,  1.0334e-01,\n",
            "        -8.9752e-03,  2.7451e-02,  1.2971e-01,  1.9561e-01, -1.2013e-01,\n",
            "        -1.1927e-01,  9.9004e-02, -8.2399e-02,  1.3359e-01,  2.7740e-01,\n",
            "        -1.0971e-01, -9.1488e-02, -1.4878e-01, -2.7871e-01, -1.6647e-01,\n",
            "        -1.3939e-01, -2.0124e-01,  1.2942e-01,  4.5379e-02, -3.1917e-02,\n",
            "         4.2488e-02, -5.8852e-02, -6.8999e-02, -1.0001e-01, -4.0453e-02,\n",
            "        -3.6279e-02,  7.7971e-02,  9.2112e-02, -3.0593e-01, -1.0182e-01,\n",
            "        -1.3465e-01,  8.3798e-02, -3.6713e-02, -1.1855e-01, -9.5890e-02,\n",
            "         1.0298e-01,  6.3240e-03,  8.0082e-02,  3.9121e-03, -1.8091e-01,\n",
            "         4.6502e-02, -1.1558e-01, -8.1314e-02, -8.5864e-02, -2.0801e-01,\n",
            "         7.6071e-02, -1.1224e-02,  9.5689e-02,  1.0339e-02,  4.1806e-02,\n",
            "        -1.7120e-01, -1.6901e-02, -2.8978e-01, -9.2227e-03, -1.8044e-01,\n",
            "        -1.5280e-01, -1.8074e-01,  6.0165e-02, -9.9807e-02, -5.1576e-02,\n",
            "        -4.9287e-02, -7.5768e-02,  1.8297e-01, -6.2253e-02,  4.7436e-02,\n",
            "        -8.9444e-02, -2.2755e-02,  5.2678e-02,  9.6818e-02,  1.1571e-01,\n",
            "         7.6331e-02, -6.5298e-02,  4.1881e-02, -1.5688e-01, -5.6627e-02,\n",
            "        -1.0717e-01, -1.5422e-01, -3.5659e-02, -8.0523e-02, -1.8417e-01,\n",
            "        -5.6303e-02,  1.7676e-02, -1.5988e-01, -1.7127e-01, -1.2788e-01,\n",
            "        -1.7134e-02, -1.7460e+00, -1.3341e-01, -1.7917e-01,  1.1111e-01,\n",
            "        -7.4008e-02, -1.0803e-01,  7.1065e-02,  2.5900e-02, -1.9544e-01,\n",
            "        -9.4846e-02,  6.1110e-02,  1.7410e-03, -1.7919e-01, -5.5191e-02,\n",
            "        -5.9096e-02, -7.0384e-02, -4.9825e-02,  1.5515e-01, -1.3629e-01,\n",
            "        -1.2711e-01, -1.9731e-02,  1.2140e-01, -4.4647e-02, -3.2350e-01,\n",
            "        -1.2600e-01, -3.3796e-02, -1.9992e-01,  1.6340e-01, -9.0332e-02,\n",
            "        -6.7900e-02, -4.6519e-02, -1.2251e-01, -7.5851e-02, -8.3525e-02,\n",
            "         1.1551e-01, -2.8183e-02, -2.3382e-01, -5.1152e-02, -4.0449e-02,\n",
            "        -1.6998e-02, -1.3577e-01,  6.7647e-02, -1.7192e-01, -6.8739e-02,\n",
            "         9.3880e-02, -5.3132e-02, -2.8285e-01, -1.5483e-01,  6.7752e-02,\n",
            "        -2.2627e-01, -2.4394e-01, -1.9411e-01,  1.3652e-01, -1.0448e-02,\n",
            "         6.7057e-02,  1.2362e-01, -1.4181e-01,  5.9248e-02, -2.6126e-02,\n",
            "         2.5776e-02, -7.1952e-02, -2.0760e-02, -1.6529e-01, -7.0032e-02,\n",
            "        -6.9645e-03, -1.5499e-01,  6.8741e-02,  1.3762e-02, -3.5218e-02,\n",
            "         4.9549e-02, -7.5311e-02,  1.3664e-02, -8.7883e-02,  1.8292e-01,\n",
            "         9.6423e-02, -1.2774e-01,  1.4762e-02, -3.0042e-01, -1.3641e-01,\n",
            "        -8.3239e-02, -1.5048e-01,  5.6542e-02,  1.6011e-01,  1.5608e-01,\n",
            "        -4.6714e-02,  1.1039e-01, -3.3949e-01,  9.2549e-02,  5.1425e-04,\n",
            "        -3.2317e-02, -3.0743e-02,  7.0931e-03,  1.6091e-01,  5.1715e-04,\n",
            "        -1.4044e-01, -1.7579e-02, -7.7398e-02, -5.3600e-02, -2.5986e-01,\n",
            "         1.5481e-02,  6.2275e-02, -2.0836e-01, -4.1511e-02, -1.7083e-01,\n",
            "        -7.5686e-02,  1.4853e-02,  1.4612e-01,  4.8640e-03, -7.4044e-03,\n",
            "        -1.2766e-01, -9.5883e-02, -4.2891e-02, -4.4178e-02,  7.9012e-03,\n",
            "        -6.1651e-03, -5.3386e-02,  5.6057e-03,  6.7158e-03,  7.4913e-02,\n",
            "         9.0871e-02,  1.8149e-01, -1.7557e-01, -2.5779e-01, -7.8879e-02,\n",
            "         4.9418e-02, -4.4383e-02,  6.0883e-02, -1.6172e-01, -1.2085e-01,\n",
            "        -1.5022e-01, -6.5437e-02, -8.1825e-02, -1.3083e-01,  1.2543e-02,\n",
            "         2.5780e-01,  1.9836e-02,  1.6917e-02, -6.5315e-02, -4.9451e-02,\n",
            "        -7.9245e-02, -5.8966e-03,  1.2537e-01, -1.8426e-01, -3.1040e-02,\n",
            "        -1.6180e-01, -1.8361e-01, -1.0397e-01, -1.1564e-02,  6.4058e-02,\n",
            "         9.8893e-02, -1.1950e-01, -1.1321e-01, -7.6212e-03,  5.7129e-02,\n",
            "        -2.4944e-01,  8.9628e-02, -1.0366e-01,  6.5120e-02, -2.0047e-01,\n",
            "         1.0281e-02,  1.6675e-02,  3.5397e-02, -3.1448e-04, -8.9842e-02,\n",
            "        -8.5394e-02, -4.1007e-02, -4.3577e-02,  1.4979e-01,  1.1193e-01,\n",
            "        -2.9281e-02, -4.0975e-02, -1.3547e-02,  1.1486e-02,  2.8582e-02,\n",
            "        -1.6404e-01, -2.0149e-02,  1.2374e-01, -5.7698e-03, -7.0315e-02,\n",
            "        -5.0054e-02, -2.9453e-03,  1.1590e-01, -1.4309e-01, -1.1349e-01,\n",
            "         6.3019e-02,  1.9525e-02,  2.9545e-02,  5.0243e-02, -2.3065e-02,\n",
            "        -6.4063e-02,  3.0920e-03,  1.7227e-01, -2.4722e-02, -7.5007e-02,\n",
            "         6.2572e-02,  8.5089e-02, -2.5624e-01,  7.9046e-02, -3.0510e-01,\n",
            "        -1.3232e-01,  8.3871e-02,  1.9946e-02, -3.8351e-02, -1.4455e-01,\n",
            "        -1.3392e-01,  2.0380e-01, -7.6327e-02, -1.4311e-01, -8.4853e-02,\n",
            "        -5.9510e-02, -1.0840e-01,  3.0047e-02,  1.2915e-02,  3.1888e-02,\n",
            "         2.9743e-02, -8.6132e-02,  4.9797e-02, -6.4389e-02,  1.3277e-01,\n",
            "        -5.3022e-02,  9.5512e-04, -1.5403e-01, -7.9885e-03,  9.7154e-02,\n",
            "         1.1641e-01, -1.3559e-02,  1.0035e-01, -1.1032e-01,  3.5650e-02,\n",
            "        -2.3193e-02, -1.5914e-01, -2.4085e-02,  1.0567e-01, -9.2590e-02,\n",
            "        -9.5098e-02, -6.9623e-02, -2.8955e-02, -1.0071e-01, -1.6246e-01,\n",
            "         1.0943e-01, -5.3557e-02, -4.6878e-02,  7.7014e-02,  2.3381e-02,\n",
            "         2.0776e-02,  2.0008e-01, -3.7203e-02,  4.1036e-02, -5.5812e-02,\n",
            "         4.0951e-02,  1.0148e-01,  1.2284e-01,  2.5115e-01, -1.2672e-02,\n",
            "         3.0992e-02,  1.3421e-01, -4.7092e-02,  1.4394e-01, -1.0366e-01,\n",
            "        -8.5281e-02,  1.0297e-01, -6.8762e-02, -2.4550e-01, -9.6800e-02,\n",
            "        -2.1981e-01, -3.0771e-02,  4.6445e-02,  7.3241e-02,  3.0206e-01,\n",
            "         3.4701e-02,  1.6843e-02,  1.3209e-01,  1.4769e-01, -4.9531e-02,\n",
            "        -6.9604e-02,  1.3091e-01,  1.3750e-01,  9.0891e-02, -1.6751e-01,\n",
            "         3.2562e-02, -2.1636e-01,  1.3645e-01, -1.4492e-01,  3.0106e-02,\n",
            "        -8.8176e-02, -7.3143e-02,  1.6979e-01,  1.7351e-01,  8.7303e-02,\n",
            "         2.2707e-01, -8.7175e-02, -1.7258e-01, -1.8927e-02,  1.4742e-01,\n",
            "        -2.4594e-03,  1.2034e-01,  2.8535e-02, -1.2334e-01, -8.3777e-02,\n",
            "        -4.2388e-02, -1.6021e-01,  9.3676e-02, -9.8286e-02, -5.2567e-02,\n",
            "         1.0417e-01,  5.7425e-02,  1.2465e-01, -1.3079e-01, -3.1143e-02,\n",
            "         2.7148e-02,  1.6231e-02, -1.2204e-01, -1.0895e-01, -1.0246e-02,\n",
            "         4.4583e-02, -1.9248e-01, -8.7517e-02, -1.3809e-01, -9.5399e-02,\n",
            "         1.4375e-01,  3.0366e-03, -6.9659e-02,  1.8529e-02,  1.5480e-01,\n",
            "         1.0105e-01,  2.6293e-01, -2.0592e-02, -3.5529e-02,  4.4947e-02,\n",
            "        -5.5688e-02,  8.0089e-03,  6.0394e-02,  7.8154e-02, -5.8051e-02,\n",
            "         5.8847e-02, -1.1170e-01, -6.5167e-03,  9.4915e-02, -5.5376e-02,\n",
            "        -1.7667e-01,  6.4343e-03, -8.6166e-02, -8.9625e-02,  1.2279e-01,\n",
            "        -1.1036e-01, -7.4790e-02, -1.3269e-01,  4.7373e-02, -1.3886e-01,\n",
            "        -2.3142e-02,  7.9503e-02, -1.1848e-02, -1.0863e-01,  2.1293e-01,\n",
            "        -9.9184e-02, -1.2978e-02,  3.1980e-01, -1.2452e-01, -2.7241e-02,\n",
            "        -4.6770e-02,  3.8339e-02, -1.4157e-01, -8.0027e-02, -3.7430e-02,\n",
            "         9.3445e-02,  2.0585e-02, -1.4760e-01,  1.1813e-02,  1.3194e-01,\n",
            "        -7.9695e-02, -8.6726e-02, -5.4680e-02,  1.4956e-01, -2.0107e-01,\n",
            "         5.8653e-02, -8.7965e-02,  7.8894e-02, -8.2017e-02, -1.2939e-01,\n",
            "        -4.2024e-02, -7.4845e-02,  8.6725e-02, -1.2495e-01, -2.3734e-01,\n",
            "         2.1638e-03, -8.5525e-02, -5.3259e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.8.intermediate.dense.weight', Parameter containing:\n",
            "tensor([[-0.0196,  0.0376, -0.0376,  ...,  0.0439, -0.0123,  0.0721],\n",
            "        [-0.0148, -0.0122, -0.0227,  ...,  0.0224,  0.0644,  0.0213],\n",
            "        [-0.0229,  0.0389, -0.0498,  ..., -0.0072, -0.0314, -0.0272],\n",
            "        ...,\n",
            "        [ 0.0196,  0.0255,  0.0161,  ..., -0.0100,  0.0623,  0.0040],\n",
            "        [ 0.0225,  0.0927, -0.0035,  ...,  0.0421, -0.0129,  0.0253],\n",
            "        [-0.0048, -0.0316,  0.0238,  ..., -0.0479,  0.0726,  0.0032]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.8.intermediate.dense.bias', Parameter containing:\n",
            "tensor([-0.0813, -0.1691, -0.1068,  ..., -0.1253, -0.1729, -0.1696],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.8.output.dense.weight', Parameter containing:\n",
            "tensor([[-0.0468,  0.0277, -0.0290,  ..., -0.0075,  0.0207, -0.0080],\n",
            "        [-0.0269,  0.0292, -0.0309,  ...,  0.0270,  0.0171,  0.0191],\n",
            "        [ 0.0298,  0.0131,  0.0516,  ...,  0.0265,  0.0246,  0.0651],\n",
            "        ...,\n",
            "        [-0.0069,  0.0759, -0.0072,  ...,  0.0424, -0.0309,  0.0179],\n",
            "        [-0.0296, -0.0164, -0.0148,  ...,  0.0421, -0.0585, -0.0530],\n",
            "        [-0.0313,  0.0059,  0.0231,  ..., -0.0405,  0.0459,  0.0123]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.8.output.dense.bias', Parameter containing:\n",
            "tensor([-9.1555e-02, -4.4654e-02,  5.6491e-02, -9.6286e-02,  1.1875e-01,\n",
            "         6.6897e-02,  4.8816e-02,  1.2960e-01, -7.1578e-02, -8.7752e-03,\n",
            "        -2.1243e-02,  4.3078e-02, -1.0626e-01,  1.0124e-01, -2.8071e-02,\n",
            "         1.0340e-01,  2.4345e-01,  7.8615e-02, -8.3008e-03,  3.6738e-02,\n",
            "        -1.1169e-02,  4.6649e-02,  3.0814e-02,  2.1457e-02, -6.1908e-02,\n",
            "         5.1173e-02,  4.2563e-02,  7.4575e-02, -8.5062e-02,  4.3984e-02,\n",
            "         7.9614e-02,  7.1508e-03,  1.2408e-01, -9.7839e-02, -4.2278e-02,\n",
            "         5.1789e-02, -1.0574e-01, -6.1029e-02, -8.9943e-02, -1.0701e-01,\n",
            "        -2.3958e-02, -1.0077e-01,  3.4481e-02,  1.0143e-02,  4.0633e-02,\n",
            "         3.9812e-02,  3.3266e-02,  1.4665e-01, -4.6542e-02,  1.3007e-02,\n",
            "        -8.9174e-02,  4.1687e-02,  1.5263e-01, -3.7148e-03,  9.6751e-02,\n",
            "         5.2584e-02,  8.6290e-02, -6.2600e-02, -4.1002e-02, -9.6493e-02,\n",
            "        -2.0483e-02,  6.7362e-02, -1.6493e-02, -9.1123e-02,  4.7165e-02,\n",
            "         8.2540e-02, -8.0268e-02,  8.7723e-02, -1.2255e-01, -3.1663e-02,\n",
            "        -5.4993e-02,  7.2299e-02, -6.8906e-03,  3.1563e-02,  3.8778e-02,\n",
            "        -3.9711e-02, -5.9711e-02,  1.2560e-01,  4.4593e-02,  6.2863e-02,\n",
            "         5.2514e-02,  1.1505e-02, -2.2539e-03,  9.4653e-02, -2.7180e-02,\n",
            "        -1.2112e-01, -3.7160e-02, -5.9889e-02, -7.0279e-02,  6.8154e-02,\n",
            "        -7.0395e-02, -1.0965e-01,  1.2779e-01, -1.2857e-01,  3.0743e-02,\n",
            "         4.8478e-02,  2.0595e-01, -2.3261e-02,  1.0158e-01, -8.1809e-02,\n",
            "        -4.7813e-03, -4.6773e-02,  5.1295e-02, -3.7282e-02, -2.8254e-02,\n",
            "         1.4519e-01,  3.1605e-02, -6.3591e-02, -7.7935e-02, -4.8828e-02,\n",
            "        -1.8449e-02,  5.7466e-02,  1.0019e-01, -1.1420e-01, -3.0192e-02,\n",
            "         6.0317e-02, -8.0003e-02, -3.2628e-02, -5.6084e-02, -8.8470e-03,\n",
            "        -3.0365e-02,  7.0132e-02,  4.7756e-03,  4.3277e-01,  7.5580e-02,\n",
            "        -1.7407e-01,  6.9992e-03,  2.5195e-03, -3.2736e-02,  3.2257e-02,\n",
            "        -7.1650e-02,  3.0422e-03, -5.0784e-02, -4.9409e-03, -4.2620e-03,\n",
            "        -1.4601e-02,  1.3933e-01, -2.7689e-03, -2.7433e-02,  3.6132e-02,\n",
            "        -5.9497e-02,  2.1788e-02, -2.7984e-03,  9.3079e-02,  1.2619e-01,\n",
            "        -4.8189e-02,  1.0338e-02, -1.3532e-01, -1.2118e-01,  9.5276e-02,\n",
            "        -1.4839e-01, -2.7315e-02, -8.8108e-02,  3.7632e-02, -1.1297e-01,\n",
            "         6.4946e-02,  6.5480e-02,  2.8203e-02, -6.4563e-02,  2.9565e-02,\n",
            "        -2.8938e-03, -2.7725e-02,  1.8429e-02, -1.5080e-02,  7.7302e-02,\n",
            "         9.2142e-02,  1.3001e-01,  8.2931e-02, -5.7834e-02,  3.6551e-02,\n",
            "         1.3632e-02,  2.6189e-02, -1.2115e-01, -1.2106e-01, -1.7079e-02,\n",
            "        -3.8322e-02,  8.3917e-02,  9.1239e-03,  3.0347e-02, -3.7421e-02,\n",
            "        -1.7180e-01,  3.7316e-02, -1.9021e-02,  2.0761e-03,  1.0720e-02,\n",
            "        -4.4113e-02,  6.8322e-02, -7.0362e-02,  8.3522e-02,  7.2227e-02,\n",
            "        -5.5812e-02, -1.2637e-02,  1.3840e-01, -1.0074e-01,  5.4622e-03,\n",
            "        -4.5558e-02, -7.4318e-02,  6.1256e-03, -6.3666e-02, -1.2848e-02,\n",
            "        -4.3610e-02, -4.7699e-03,  5.0522e-02, -1.2359e-01,  1.1833e-02,\n",
            "         1.1656e-01, -4.5377e-02, -3.3351e-02, -5.7017e-02, -9.0479e-04,\n",
            "        -1.0306e-01, -2.7260e-03,  5.4226e-02,  1.9462e-01, -4.1110e-02,\n",
            "         6.1040e-02, -1.7743e-01, -2.4451e-02,  7.6675e-02, -8.5080e-02,\n",
            "         9.5880e-02,  5.4492e-02, -5.2131e-02, -3.1138e-02,  4.3596e-02,\n",
            "         2.6647e-01,  8.0188e-02, -5.9918e-02,  6.1889e-04, -3.5530e-02,\n",
            "        -1.6193e-01, -1.6480e-03,  1.1908e-01, -7.5580e-02, -3.8610e-02,\n",
            "         6.5897e-03,  7.3580e-02,  5.6847e-02,  4.8766e-02, -5.3880e-02,\n",
            "         1.3021e-01,  4.2222e-02, -3.1803e-02, -2.0715e-03,  1.0613e-02,\n",
            "        -2.1373e-02,  6.1544e-02, -4.1929e-02, -8.9789e-02,  4.2011e-02,\n",
            "        -6.7479e-02,  7.6875e-02,  4.5813e-03, -4.4654e-02,  5.5526e-02,\n",
            "        -5.6868e-02,  5.9146e-02, -6.5452e-02,  1.0210e-02,  7.0691e-02,\n",
            "         2.3685e-02, -3.4035e-02, -6.1317e-02, -5.1398e-03,  1.6972e-02,\n",
            "        -1.7194e-01,  1.1918e-03, -8.8972e-02,  5.1491e-02,  3.3194e-04,\n",
            "        -2.8210e-02,  8.8037e-02, -9.4858e-03,  1.2592e-01,  3.4913e-03,\n",
            "        -7.8537e-02,  1.0526e-01,  4.5024e-02, -9.9960e-03, -3.8078e-02,\n",
            "        -3.2635e-04,  8.5017e-02, -2.3547e-02,  2.8517e-02, -1.0174e-01,\n",
            "        -7.8573e-04,  2.6300e-02,  5.1944e-03, -4.6560e-02, -4.0735e-02,\n",
            "        -3.7499e-02, -1.9160e-02, -8.8914e-02, -5.8932e-02,  8.4886e-02,\n",
            "         5.4656e-02,  1.5795e-02,  2.2030e-02,  3.2333e-02, -9.0523e-02,\n",
            "        -5.2046e-02,  7.0533e-02,  2.2760e-02,  1.0900e-01,  3.4628e-02,\n",
            "        -2.6557e-02, -5.9474e-02, -3.9204e-02, -8.0948e-01, -7.2304e-02,\n",
            "        -4.5809e-02, -1.0018e-01,  9.4118e-02,  7.7483e-02, -1.7504e-02,\n",
            "         1.0185e-01, -1.7053e-02, -1.2195e-01, -1.7779e-02, -5.8146e-03,\n",
            "         6.3126e-02,  1.2210e-01,  1.1152e-01, -1.7735e-01, -2.9973e-02,\n",
            "        -8.8277e-02,  5.0881e-02,  7.7947e-02,  1.6046e-02,  3.1690e-02,\n",
            "         5.0952e-02,  6.9659e-02, -4.6370e-02,  4.6315e-02, -7.9155e-02,\n",
            "         5.5212e-03, -7.0713e-02,  3.3550e-02, -5.7695e-02, -7.5452e-02,\n",
            "         1.4210e-01,  3.8523e-02,  8.4111e-02,  5.9510e-02, -2.3753e-02,\n",
            "        -3.1123e-02,  9.8634e-02, -9.0619e-02,  7.4661e-02, -3.1158e-02,\n",
            "        -5.9607e-03, -9.2624e-02,  1.4327e-01, -1.6428e-01,  5.9165e-02,\n",
            "         3.5132e-02, -5.8488e-02,  4.2980e-02,  4.2041e-02,  5.7695e-02,\n",
            "         5.9694e-02, -1.2585e-01,  1.0116e-01,  1.0457e-01,  1.6101e-01,\n",
            "         1.0425e-01, -6.5785e-02,  1.6451e-02, -6.2165e-02, -3.0994e-02,\n",
            "        -1.0957e-01, -2.9063e-02,  1.0838e-03, -7.1089e-02, -2.1410e-02,\n",
            "        -1.7967e-02,  2.8260e-02, -4.1097e-02, -1.3215e-01, -5.1793e-03,\n",
            "         1.9337e-02, -7.3226e-01, -7.0402e-02, -1.2592e-01,  1.3085e-01,\n",
            "         2.0948e-03, -6.2291e-02,  5.2951e-02,  1.0671e-02, -4.7363e-02,\n",
            "        -2.9571e-02, -2.0520e-03,  5.9692e-02, -1.1292e-01, -3.4722e-02,\n",
            "        -2.4619e-02, -1.1288e-01, -4.4506e-02,  1.5694e-01,  1.6479e-02,\n",
            "        -4.9139e-02, -4.6141e-02,  1.2154e-01, -4.5294e-02, -6.8460e-02,\n",
            "         8.2905e-02, -5.1454e-02, -1.3213e-01,  5.5452e-02,  2.7606e-02,\n",
            "        -1.7301e-02,  1.4594e-01, -3.9856e-02, -3.3299e-02, -4.7933e-02,\n",
            "        -6.6703e-03, -7.6020e-02, -8.8420e-02, -5.4488e-02,  1.7546e-02,\n",
            "        -1.1262e-02, -1.1244e-01,  4.5473e-02, -7.6613e-02, -1.1551e-02,\n",
            "         5.6473e-02, -7.5254e-03, -9.7075e-02, -6.2775e-02,  8.1501e-02,\n",
            "        -1.0970e-01, -1.2536e-01, -3.7206e-02,  7.6179e-02,  3.1974e-02,\n",
            "         1.7963e-02,  4.1551e-02, -5.0572e-02,  3.2873e-02,  3.1030e-02,\n",
            "         4.8469e-02,  5.5628e-03, -5.1760e-02, -1.1697e-01, -2.8380e-02,\n",
            "        -1.0519e-01, -6.9322e-02,  9.5309e-02, -1.1252e-02,  3.1118e-02,\n",
            "         5.8415e-02, -6.4343e-02,  5.9302e-02, -1.2628e-01,  7.4608e-02,\n",
            "         4.5645e-02,  4.9558e-02, -1.1602e-02, -3.0191e-01,  1.7126e-03,\n",
            "        -1.2306e-02, -1.4154e-01, -1.1961e-02,  1.1135e-02,  8.9661e-02,\n",
            "         4.3307e-02,  1.8112e-01, -1.6345e-01, -2.4491e-02,  1.3022e-02,\n",
            "         4.1310e-03, -3.9362e-02,  1.2061e-02,  6.5923e-02,  2.2768e-02,\n",
            "        -3.3115e-02, -5.4854e-02, -7.4674e-02, -1.3092e-01, -1.2131e-01,\n",
            "        -5.5934e-03, -8.6006e-02, -1.4455e-01,  4.5123e-02, -1.2537e-01,\n",
            "        -8.7328e-03,  2.9963e-02,  5.3944e-02, -1.9016e-02,  5.7332e-02,\n",
            "        -1.0872e-01, -2.6657e-02, -2.6316e-02,  8.7106e-02,  1.0272e-02,\n",
            "        -3.1050e-02, -2.5202e-03, -2.3700e-02, -1.5075e-02,  4.2988e-02,\n",
            "        -4.5972e-04,  2.8189e-02,  3.8057e-02, -7.2333e-02, -3.8516e-02,\n",
            "         6.6651e-02, -6.8975e-02,  1.0745e-01, -1.4713e-01,  1.0875e-02,\n",
            "         5.5039e-03, -1.3332e-01, -4.6649e-02,  1.1980e-03,  6.7277e-03,\n",
            "         1.5349e-01, -2.0419e-02, -8.0585e-02,  3.1838e-02,  1.1117e-01,\n",
            "         2.1918e-02,  2.9982e-02,  8.1723e-02, -1.8992e-02, -4.6943e-02,\n",
            "        -4.5348e-02, -1.4594e-01, -6.4863e-02,  5.0684e-02,  6.3399e-02,\n",
            "         6.5548e-03,  5.2162e-02, -3.2086e-02,  2.8139e-02,  2.0336e-02,\n",
            "        -5.1686e-02,  1.6549e-02,  9.7592e-03,  6.0088e-02, -1.3918e-01,\n",
            "         3.6904e-02, -6.1635e-02,  6.1292e-05, -2.9618e-02, -7.2230e-03,\n",
            "        -7.4127e-02, -3.2037e-02,  3.8631e-02,  3.7884e-02,  6.3458e-02,\n",
            "        -3.4274e-02,  2.5123e-02,  9.3946e-03,  6.4055e-02,  9.1846e-03,\n",
            "        -4.2031e-02,  4.1415e-02,  5.3456e-02, -1.1636e-02, -4.2090e-02,\n",
            "         2.5382e-02,  4.1783e-02,  1.1138e-01, -7.0360e-02, -7.1505e-02,\n",
            "         5.5541e-02,  1.9128e-03,  1.2220e-01,  5.0960e-03,  1.0768e-01,\n",
            "        -9.1054e-03,  1.3206e-02,  6.6382e-02, -1.0436e-01, -8.1415e-02,\n",
            "        -3.4986e-02,  7.7435e-02, -1.9259e-01,  2.5770e-02, -1.0653e-01,\n",
            "         9.8523e-03,  7.3112e-02,  1.0827e-01,  7.8133e-02, -1.3252e-01,\n",
            "        -2.3007e-02,  1.5554e-01, -1.3331e-03, -1.3471e-01,  1.0872e-02,\n",
            "        -1.6264e-02, -1.1778e-01,  3.9070e-03,  1.8290e-02, -1.0065e-02,\n",
            "         1.1456e-01,  7.5414e-03,  3.2451e-02, -1.6716e-02,  1.2536e-01,\n",
            "         3.4859e-02,  2.6031e-02,  1.8494e-03,  1.1959e-01,  9.2029e-02,\n",
            "         9.7261e-02,  6.1421e-02,  6.6285e-02, -1.3142e-01,  6.9378e-02,\n",
            "         1.4688e-01,  6.8263e-03,  3.7917e-02, -1.9300e-02, -1.6156e-02,\n",
            "        -5.9789e-02, -8.1488e-02, -5.0506e-02, -5.9910e-02, -1.1464e-01,\n",
            "         1.5368e-01, -1.7974e-02,  2.5808e-02,  2.0889e-02,  6.2885e-03,\n",
            "         4.2314e-02,  9.3887e-02,  7.5613e-03,  6.4320e-02, -1.9764e-02,\n",
            "         1.6588e-01,  1.0584e-02,  9.8279e-02,  1.1575e-01, -5.0032e-03,\n",
            "         1.0459e-01,  8.5060e-02,  6.1192e-02,  1.0315e-01, -5.0588e-02,\n",
            "        -1.2036e-01,  7.3793e-02, -6.1256e-02, -1.1899e-01,  9.9655e-03,\n",
            "        -1.1367e-01, -1.2256e-02,  2.0358e-01, -1.9536e-03,  2.3521e-01,\n",
            "         1.9291e-02,  5.9019e-02,  9.2041e-02,  1.7384e-01, -9.7989e-02,\n",
            "        -3.3187e-02,  1.0656e-01,  4.0050e-02,  7.8605e-03, -2.0018e-02,\n",
            "        -7.1673e-03, -8.6108e-02,  1.0743e-01,  1.1870e-02,  1.4138e-02,\n",
            "         1.1051e-02, -1.2513e-03,  1.2446e-01,  1.3893e-01,  1.0496e-01,\n",
            "         1.2115e-01,  4.0829e-03,  2.3537e-02, -2.8697e-02,  1.0635e-01,\n",
            "        -1.5224e-03,  6.6991e-02,  8.7317e-03, -9.7830e-02, -1.5091e-02,\n",
            "        -2.1814e-02, -7.0198e-02,  9.7433e-02, -3.9316e-02,  3.0703e-02,\n",
            "         2.6333e-02,  1.9475e-02,  1.4384e-01, -3.7045e-02, -1.0915e-02,\n",
            "        -2.0406e-02, -5.8466e-02, -3.7232e-02, -3.7696e-02,  1.3165e-01,\n",
            "         3.9667e-02, -1.6338e-01, -1.0876e-01, -6.5494e-02, -1.7859e-02,\n",
            "         1.5690e-01,  4.0397e-02,  1.9921e-02,  4.7067e-02,  1.2507e-01,\n",
            "         4.3233e-02,  9.1374e-02,  1.9799e-02,  3.7023e-02,  1.8405e-02,\n",
            "        -4.1450e-02, -6.8685e-03,  5.4057e-02,  3.9020e-02, -7.2173e-03,\n",
            "         9.3989e-03, -5.0828e-02,  6.8553e-03,  1.1304e-01,  4.2380e-03,\n",
            "        -3.8414e-01,  1.4165e-02, -2.5416e-02, -1.6064e-02,  6.4136e-02,\n",
            "        -2.4504e-02, -3.9681e-02, -1.1092e-01, -5.2864e-02,  1.5435e-02,\n",
            "         3.5241e-02,  1.1724e-01, -1.6482e-02,  5.2745e-03,  2.0544e-01,\n",
            "        -5.4327e-02, -1.0500e-02,  1.7230e-01, -7.9888e-02,  8.0434e-02,\n",
            "        -3.1743e-02,  1.9764e-03, -1.4253e-01, -5.8595e-02,  3.6786e-03,\n",
            "         5.2437e-02,  5.7553e-02, -9.4242e-02,  6.4327e-02,  1.3649e-02,\n",
            "        -6.8153e-02,  1.1261e-02, -3.1071e-02,  8.1090e-02, -5.6856e-02,\n",
            "        -2.7045e-02, -5.8044e-02,  7.7529e-02,  6.5637e-02, -1.3842e-01,\n",
            "        -9.4013e-02,  7.2198e-03,  5.0275e-02, -7.0681e-02, -7.5821e-02,\n",
            "        -3.8702e-02, -3.5890e-02, -7.5177e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.8.output.LayerNorm.weight', Parameter containing:\n",
            "tensor([0.8447, 0.8442, 0.8258, 0.8455, 0.8208, 0.8428, 0.8014, 0.8315, 0.8506,\n",
            "        0.8288, 0.8373, 0.8114, 0.8265, 0.8163, 0.8410, 0.7896, 0.8201, 0.8325,\n",
            "        0.8292, 0.8472, 0.8465, 0.8390, 0.8539, 0.8206, 0.8208, 0.8336, 0.8174,\n",
            "        0.8165, 0.8008, 0.8484, 0.8400, 0.8324, 0.8278, 0.8337, 0.8602, 0.8533,\n",
            "        0.8069, 0.8424, 0.8268, 0.8183, 0.8542, 0.8400, 0.8340, 0.8211, 0.8384,\n",
            "        0.8360, 0.8707, 0.8529, 0.8540, 0.8516, 0.8367, 0.8424, 0.8203, 0.8160,\n",
            "        0.8070, 0.8461, 0.8529, 0.7834, 0.8565, 0.8075, 0.8263, 0.8082, 0.8158,\n",
            "        0.8209, 0.8608, 0.8285, 0.8379, 0.8374, 0.8303, 0.8295, 0.8477, 0.8417,\n",
            "        0.8570, 0.8266, 0.8440, 0.8125, 0.8371, 0.8174, 0.8394, 0.8444, 0.8428,\n",
            "        0.8401, 0.7906, 0.8377, 0.8147, 0.8330, 0.8232, 0.8649, 0.8449, 0.8428,\n",
            "        0.8240, 0.8489, 0.7576, 0.8193, 0.8217, 0.8403, 0.8016, 0.8487, 0.8452,\n",
            "        0.8169, 0.8462, 0.8443, 0.8307, 0.8294, 0.8279, 0.8224, 0.8457, 0.8292,\n",
            "        0.8404, 0.8061, 0.8327, 0.8385, 0.8351, 0.8418, 0.8674, 0.8555, 0.8330,\n",
            "        0.8414, 0.8331, 0.8298, 0.8276, 0.8485, 0.8490, 0.5818, 0.8169, 0.8390,\n",
            "        0.8207, 0.8096, 0.8085, 0.8675, 0.8509, 0.8506, 0.7892, 0.8300, 0.8412,\n",
            "        0.8445, 0.8473, 0.8278, 0.8254, 0.8340, 0.8491, 0.8401, 0.8189, 0.7842,\n",
            "        0.8236, 0.8249, 0.8389, 0.8335, 0.8130, 0.8133, 0.7952, 0.8066, 0.7988,\n",
            "        0.8345, 0.8452, 0.8188, 0.8313, 0.8132, 0.8485, 0.8386, 0.8515, 0.8356,\n",
            "        0.8529, 0.8556, 0.8083, 0.8520, 0.8221, 0.8123, 0.7733, 0.8390, 0.8419,\n",
            "        0.8331, 0.8281, 0.8379, 0.8266, 0.8145, 0.8231, 0.8415, 0.8382, 0.8261,\n",
            "        0.6309, 0.8448, 0.8362, 0.8623, 0.8420, 0.8190, 0.8276, 0.8515, 0.8360,\n",
            "        0.8321, 0.8372, 0.8388, 0.8257, 0.8146, 0.8172, 0.8320, 0.8399, 0.8223,\n",
            "        0.8448, 0.8263, 0.8265, 0.8256, 0.8499, 0.8130, 0.8297, 0.7713, 0.8565,\n",
            "        0.8211, 0.8447, 0.8282, 0.8283, 0.8298, 0.8342, 0.7783, 0.8612, 0.7975,\n",
            "        0.8151, 0.8323, 0.7772, 0.8376, 0.8327, 0.8219, 0.8451, 0.8303, 0.8326,\n",
            "        0.6361, 0.8249, 0.8154, 0.8542, 0.8213, 0.7948, 0.8413, 0.8201, 0.8315,\n",
            "        0.8435, 0.8288, 0.8338, 0.8264, 0.8300, 0.8370, 0.8200, 0.8414, 0.8364,\n",
            "        0.8013, 0.8449, 0.8201, 0.8555, 0.8430, 0.8133, 0.8330, 0.8271, 0.8637,\n",
            "        0.8209, 0.8396, 0.8299, 0.8395, 0.8153, 0.7952, 0.8422, 0.8328, 0.8494,\n",
            "        0.8553, 0.8565, 0.8573, 0.8108, 0.7636, 0.8508, 0.8421, 0.8396, 0.8279,\n",
            "        0.8194, 0.8517, 0.8319, 0.8423, 0.8395, 0.8481, 0.8152, 0.8572, 0.8419,\n",
            "        0.8380, 0.8371, 0.8232, 0.8456, 0.8261, 0.8302, 0.8446, 0.8069, 0.8534,\n",
            "        0.8401, 0.8494, 0.8096, 0.8187, 0.8300, 0.8129, 0.8057, 0.8245, 0.8453,\n",
            "        0.8108, 0.8099, 0.8323, 0.8241, 0.8162, 0.8384, 0.8404, 0.8136, 0.8227,\n",
            "        0.8146, 0.8117, 1.6179, 0.8173, 0.8288, 0.8146, 0.8182, 0.8552, 0.8439,\n",
            "        0.8370, 0.8430, 0.8479, 0.8643, 0.8390, 0.8352, 0.8180, 0.8427, 0.8040,\n",
            "        0.8310, 0.8465, 0.8456, 0.8525, 0.8575, 0.8486, 0.8229, 0.8038, 0.8576,\n",
            "        0.8372, 0.8169, 0.8291, 0.7992, 0.8272, 0.8324, 0.8276, 0.8250, 0.8650,\n",
            "        0.8311, 0.8521, 0.8143, 0.8314, 0.8428, 0.7867, 0.8465, 0.7968, 0.8432,\n",
            "        0.8449, 0.8400, 0.8341, 0.8381, 0.8296, 0.8051, 0.8199, 0.8313, 0.8140,\n",
            "        0.8288, 0.8149, 0.8175, 0.8423, 0.8281, 0.8229, 0.8219, 0.8500, 0.8418,\n",
            "        0.8374, 0.8405, 0.8400, 0.8487, 0.8523, 0.7743, 0.8452, 0.8413, 0.8274,\n",
            "        0.8256, 0.8345, 0.8230, 0.2616, 0.8310, 0.8185, 0.8153, 0.8533, 0.8601,\n",
            "        0.8467, 0.8478, 0.8307, 0.8456, 0.8353, 0.8388, 0.8402, 0.8540, 0.8607,\n",
            "        0.8311, 0.8172, 0.8431, 0.8426, 0.8203, 0.8223, 0.8080, 0.8479, 0.8052,\n",
            "        0.8210, 0.8449, 0.8410, 0.7975, 0.8370, 0.8196, 0.8087, 0.8325, 0.8776,\n",
            "        0.8205, 0.8276, 0.8354, 0.8336, 0.8440, 0.8164, 0.8555, 0.8157, 0.8323,\n",
            "        0.8145, 0.8394, 0.8432, 0.8563, 0.8376, 0.8363, 0.8288, 0.8391, 0.7856,\n",
            "        0.8411, 0.8088, 0.8614, 0.8448, 0.8362, 0.8304, 0.8486, 0.8371, 0.8287,\n",
            "        0.8607, 0.8137, 0.8189, 0.8302, 0.8201, 0.8140, 0.8251, 0.8340, 0.8143,\n",
            "        0.8597, 0.8312, 0.8357, 0.8427, 0.8404, 0.8547, 0.8348, 0.8524, 0.8161,\n",
            "        0.8214, 0.8170, 0.8262, 0.8672, 0.8238, 0.8156, 0.8211, 0.8124, 0.7963,\n",
            "        0.8405, 0.8367, 0.8628, 0.8195, 0.8393, 0.8450, 0.8301, 0.8269, 0.8367,\n",
            "        0.8469, 0.8233, 0.8172, 0.8439, 0.8368, 0.8320, 0.8349, 0.8426, 0.8279,\n",
            "        0.8528, 0.8247, 0.8354, 0.8403, 0.8224, 0.8458, 0.8458, 0.8213, 0.8562,\n",
            "        0.8452, 0.8198, 0.8405, 0.8469, 0.8527, 0.8151, 0.8307, 0.8111, 0.7746,\n",
            "        0.8165, 0.8438, 0.8367, 0.8429, 0.8395, 0.8649, 0.8063, 0.8252, 0.8409,\n",
            "        0.8346, 0.8306, 0.8122, 0.8533, 0.8034, 0.8301, 0.8263, 0.8433, 0.8316,\n",
            "        0.8067, 0.8378, 0.8446, 0.8643, 0.7550, 0.8403, 0.8295, 0.7943, 0.8429,\n",
            "        0.8481, 0.8489, 0.8590, 0.8370, 0.7877, 0.8655, 0.8457, 0.7998, 0.8556,\n",
            "        0.8312, 0.8072, 0.8758, 0.8321, 0.8440, 0.8091, 0.8323, 0.8460, 0.8207,\n",
            "        0.8248, 0.8564, 0.8331, 0.8312, 0.8442, 0.8295, 0.8521, 0.8392, 0.8046,\n",
            "        0.8335, 0.8597, 0.8558, 0.8242, 0.8492, 0.8403, 0.8191, 0.8288, 0.8204,\n",
            "        0.8260, 0.8447, 0.8378, 0.8448, 0.8438, 0.8331, 0.8302, 0.8218, 0.8319,\n",
            "        0.8657, 0.8049, 0.8029, 0.8064, 0.8315, 0.8414, 0.8355, 0.8488, 0.8542,\n",
            "        0.8420, 0.7802, 0.8629, 0.8011, 0.8223, 0.8500, 0.8484, 0.8460, 0.8498,\n",
            "        0.8385, 0.8279, 0.8610, 0.8513, 0.8527, 0.8253, 0.8313, 0.8297, 0.8351,\n",
            "        0.8035, 0.8283, 0.8446, 0.8344, 0.8170, 0.8368, 0.8450, 0.8398, 0.8054,\n",
            "        0.8321, 0.8283, 0.8070, 0.8446, 0.8296, 0.8343, 0.8676, 0.8221, 0.7967,\n",
            "        0.8229, 0.8578, 0.8651, 0.8409, 0.8460, 0.8264, 0.8310, 0.8284, 0.8543,\n",
            "        0.8361, 0.8392, 0.8257, 0.8105, 0.8582, 0.8196, 0.8246, 0.8526, 0.8279,\n",
            "        0.8269, 0.8242, 0.8076, 0.8396, 0.8265, 0.8370, 0.8301, 0.8274, 0.8170,\n",
            "        0.8281, 0.7974, 0.8149, 0.8549, 0.8173, 0.8337, 0.8475, 0.8441, 0.8154,\n",
            "        0.8303, 0.8452, 0.8222, 0.8412, 0.8273, 0.8342, 0.8289, 0.8395, 0.8216,\n",
            "        0.8455, 0.7872, 0.7967, 0.8433, 0.7560, 0.8304, 0.8243, 0.8051, 0.8456,\n",
            "        0.8372, 0.8151, 0.8468, 0.8452, 0.8296, 0.8506, 0.8245, 0.8186, 0.8403,\n",
            "        0.8407, 0.8445, 0.8247, 0.8466, 0.8265, 0.8462, 0.8468, 0.8161, 0.8353,\n",
            "        0.8473, 0.8413, 0.8491, 0.8072, 0.8217, 0.8258, 0.8248, 0.8308, 0.8406,\n",
            "        0.8218, 0.8552, 0.8188, 0.8036, 0.8079, 0.7939, 0.8472, 0.8502, 0.8185,\n",
            "        0.8479, 0.8111, 0.8234, 0.8553, 0.8411, 0.8377, 0.8371, 0.8568, 0.8214,\n",
            "        0.6456, 0.8565, 0.8414, 0.8478, 0.8395, 0.8347, 0.8481, 0.8320, 0.8236,\n",
            "        0.8321, 0.8410, 0.8473, 0.8432, 0.8804, 0.7857, 0.8464, 0.8286, 0.7936,\n",
            "        0.8464, 0.8247, 0.8539, 0.8375, 0.8597, 0.8197, 0.8270, 0.8315, 0.8363,\n",
            "        0.8561, 0.8477, 0.8218, 0.8480, 0.8548, 0.8228, 0.8339, 0.8328, 0.8076,\n",
            "        0.8016, 0.8303, 0.8528, 0.8277, 0.8526, 0.8531, 0.8448, 0.8132, 0.7983,\n",
            "        0.8375, 0.8206, 0.8446], device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.8.output.LayerNorm.bias', Parameter containing:\n",
            "tensor([-6.0434e-02, -6.6574e-02, -5.3411e-02, -3.7473e-03, -1.0855e-01,\n",
            "        -2.5190e-02, -1.2747e-01, -8.0729e-02,  1.2454e-03,  2.1830e-02,\n",
            "        -7.4600e-02, -1.1655e-01,  3.7097e-02, -5.2080e-02, -2.3333e-02,\n",
            "        -1.4724e-01, -8.1681e-02, -9.0290e-02, -1.3640e-02, -6.4035e-02,\n",
            "        -1.3044e-01, -1.5306e-02, -6.1007e-02, -1.6939e-01, -5.6227e-02,\n",
            "        -1.2686e-01, -3.7969e-02,  2.9934e-02,  3.8393e-02, -8.9227e-02,\n",
            "        -3.2811e-02, -4.2680e-02, -1.3964e-01, -2.8615e-03, -1.9903e-02,\n",
            "        -4.5810e-02,  3.9919e-02, -3.9523e-02, -5.0239e-02,  7.6281e-02,\n",
            "         2.8437e-02,  5.1372e-02, -1.2479e-02, -9.7882e-02, -7.1457e-02,\n",
            "        -1.0540e-04, -2.6029e-02, -4.5152e-02, -2.9636e-02, -3.9736e-02,\n",
            "         5.1432e-02, -8.1544e-02, -1.1939e-01, -2.4400e-02, -1.2422e-01,\n",
            "        -6.6767e-02, -8.0397e-02,  7.6838e-02, -2.0085e-02,  2.3329e-02,\n",
            "        -4.2266e-02, -1.5425e-01, -5.9279e-02, -3.3451e-02, -1.9278e-02,\n",
            "        -1.1394e-01, -7.8742e-03, -3.7558e-02, -1.7364e-02,  2.0172e-02,\n",
            "         3.0023e-02, -1.0167e-01,  2.0659e-02, -7.7266e-03, -9.0087e-02,\n",
            "        -3.8153e-02, -2.1861e-02, -5.8918e-03, -1.0826e-01, -1.2729e-01,\n",
            "        -7.3373e-02, -6.8534e-02, -1.0027e-01, -9.7525e-02, -3.7095e-02,\n",
            "         1.0309e-02, -3.0280e-02, -3.0040e-02,  1.3849e-02, -1.0163e-01,\n",
            "         2.2553e-02,  5.0153e-02, -1.7362e-01, -5.3377e-02, -3.9739e-02,\n",
            "         2.1268e-02, -1.0780e-01, -1.2092e-03, -4.8188e-02,  2.9974e-02,\n",
            "        -2.0472e-02, -2.2645e-04, -1.0002e-01, -2.3018e-02, -4.0754e-02,\n",
            "        -5.2450e-02, -3.5255e-02,  2.6819e-02,  4.6068e-03, -4.4663e-02,\n",
            "         2.9248e-02, -2.6605e-02,  1.1381e-02,  6.7287e-03, -3.6448e-03,\n",
            "        -5.1187e-02, -6.0767e-04, -6.8383e-03,  2.1426e-02, -6.9556e-02,\n",
            "        -2.5026e-02, -3.7308e-02, -7.1907e-02, -1.5625e-01, -1.1506e-01,\n",
            "         5.4456e-02, -6.8356e-02, -1.0098e-01,  1.4837e-02, -2.7549e-02,\n",
            "        -3.9342e-02, -8.9119e-02, -8.2797e-04, -1.6720e-02,  2.3796e-02,\n",
            "         1.2355e-03, -8.0419e-02, -5.6699e-02,  2.0797e-03,  5.0174e-03,\n",
            "        -6.6264e-03, -6.4845e-02, -2.5441e-02, -1.4580e-01, -1.0586e-01,\n",
            "         4.3328e-02, -5.0488e-02,  1.6404e-03,  3.6633e-02, -1.4401e-01,\n",
            "         1.5159e-02, -1.6362e-01,  3.8709e-02, -5.2711e-02, -1.8626e-02,\n",
            "        -7.5729e-02, -5.5352e-02, -7.8262e-02, -4.4010e-02, -3.5784e-02,\n",
            "        -3.9811e-02,  2.5602e-02, -9.2414e-04, -4.9390e-02, -8.2070e-02,\n",
            "        -5.6103e-02, -1.2689e-01, -6.5351e-02,  1.0304e-01, -4.6269e-02,\n",
            "         1.1972e-02, -1.0165e-03, -5.8980e-02, -1.0035e-01, -2.3426e-02,\n",
            "        -1.0944e-01, -1.0950e-01, -5.5063e-02, -6.3111e-02, -6.1464e-02,\n",
            "         5.8724e-02, -7.1554e-02,  5.3993e-02, -3.1396e-02, -4.5347e-02,\n",
            "        -9.3896e-03, -1.2156e-01, -7.2647e-02, -9.9593e-02, -2.1577e-02,\n",
            "         2.3320e-02,  4.9790e-02, -1.2477e-01,  2.2985e-02, -9.4525e-02,\n",
            "        -7.1457e-02, -1.7645e-02, -1.1834e-01, -2.2370e-02, -2.2903e-02,\n",
            "         4.6797e-02, -2.3321e-02, -5.8783e-02,  2.3218e-02, -1.0205e-03,\n",
            "        -1.2596e-01, -8.5174e-02, -2.0600e-02, -7.0131e-02, -7.8297e-02,\n",
            "         9.3474e-03,  2.3925e-02, -1.1426e-01, -1.6562e-01, -5.1602e-02,\n",
            "        -1.6173e-02,  5.2140e-02, -2.1642e-02, -1.4453e-01,  1.3547e-02,\n",
            "        -8.1211e-02, -1.1853e-01, -1.5630e-02, -6.8336e-02,  5.3673e-02,\n",
            "        -5.7086e-02, -6.0969e-02,  7.5101e-03, -1.1143e-01, -8.0739e-03,\n",
            "         1.2283e-02, -8.6508e-02, -1.0784e-01, -1.5422e-02, -1.1286e-02,\n",
            "        -5.5254e-02, -2.6176e-02, -2.1493e-02, -1.1141e-01,  5.7634e-02,\n",
            "        -7.1005e-02, -8.5546e-02,  7.0352e-02, -1.0610e-01, -4.5532e-02,\n",
            "        -4.6055e-02, -9.2581e-03, -3.4471e-02,  3.4109e-02,  3.6170e-02,\n",
            "         4.2674e-03, -1.5947e-01, -6.2083e-02, -1.5680e-02, -8.7279e-02,\n",
            "        -3.3763e-02,  4.8817e-02,  2.6907e-02, -9.5887e-02, -5.6328e-02,\n",
            "        -9.0758e-02, -3.7893e-02, -3.5122e-02, -3.6924e-02, -2.8473e-02,\n",
            "         1.0867e-01, -1.6066e-02, -2.0317e-02, -7.1949e-02, -3.2856e-02,\n",
            "        -2.6639e-02, -7.2007e-02,  4.1333e-02, -9.2563e-02,  7.6693e-03,\n",
            "        -3.2851e-02, -2.9394e-02, -3.4945e-02, -1.7228e-02,  4.8761e-02,\n",
            "        -7.4312e-02, -1.1608e-01,  1.7526e-02, -1.2904e-02,  1.7517e-02,\n",
            "        -5.1080e-02, -1.0684e-01, -8.5033e-02, -1.0802e-02, -2.6085e-02,\n",
            "        -5.2837e-02, -3.8999e-02,  3.1314e-02, -5.2433e-02, -1.0462e-01,\n",
            "        -5.5095e-02, -8.2696e-02, -1.4362e-01, -1.1650e-01,  2.2637e-02,\n",
            "        -6.4085e-04, -9.5339e-02, -4.1822e-03, -1.1618e-01, -1.4910e-01,\n",
            "         5.0148e-03,  4.5802e-03,  2.8265e-02,  1.2884e-01,  3.4291e-02,\n",
            "         2.5331e-02,  1.3152e-02, -1.0017e-01, -5.4800e-02, -4.2957e-02,\n",
            "        -5.9099e-02, -4.9694e-02, -4.6802e-02, -2.9493e-02, -3.1559e-02,\n",
            "        -4.3551e-03, -1.1286e-01, -7.7375e-02,  1.0809e-01, -3.0017e-02,\n",
            "        -1.4722e-03, -7.0181e-02, -5.3466e-02,  5.7469e-02,  5.0175e-02,\n",
            "        -9.7988e-02, -4.2461e-02, -5.8318e-02, -9.6333e-02,  9.4398e-03,\n",
            "        -6.8912e-02,  4.7488e-03, -1.1016e-02, -7.0517e-03,  1.3384e-02,\n",
            "        -5.2445e-02, -1.0048e-01, -5.2579e-02, -5.3505e-02, -7.9745e-02,\n",
            "         4.6614e-02, -6.7940e-03,  1.2135e-01, -1.0074e-02,  5.4526e-02,\n",
            "         1.7721e-02,  1.4867e-02, -8.1992e-02, -1.0205e-02, -1.3016e-02,\n",
            "         1.0559e-02, -3.2391e-02, -9.0619e-02,  1.8612e-03, -8.4933e-02,\n",
            "        -3.4331e-02, -6.6296e-02, -1.1073e-01, -5.0133e-02, -1.1443e-01,\n",
            "        -7.5488e-02,  1.7416e-02, -6.3808e-02,  4.2427e-02, -2.5619e-02,\n",
            "         1.5155e-02,  3.5429e-03, -7.7792e-02, -2.4987e-03,  7.6519e-02,\n",
            "        -1.4248e-02, -6.0049e-02, -5.8134e-03,  3.8734e-03, -1.5892e-02,\n",
            "        -2.8932e-02,  6.9523e-01,  2.8123e-03,  4.3016e-02, -9.9778e-02,\n",
            "         7.7378e-03,  2.9819e-02, -9.8470e-02, -4.6133e-02,  7.7382e-02,\n",
            "        -3.6013e-02, -8.2106e-02, -7.0760e-02,  1.9589e-02, -2.7839e-02,\n",
            "        -2.9317e-02,  3.1324e-02,  1.8205e-02, -1.2056e-01,  6.0795e-02,\n",
            "         4.4223e-02, -3.7467e-02, -8.3812e-02, -3.4703e-02,  1.0543e-01,\n",
            "         6.7851e-02, -5.5800e-02,  4.1706e-02, -1.5438e-01,  3.2448e-02,\n",
            "         8.2413e-03,  8.8163e-03, -2.1921e-02, -2.1825e-02, -7.7382e-03,\n",
            "        -9.8243e-02, -3.2763e-02,  4.3209e-02, -6.1806e-02, -3.8630e-02,\n",
            "        -4.2999e-02,  8.0443e-03, -7.4465e-02,  5.2662e-02,  2.9299e-02,\n",
            "        -5.5018e-02, -1.3068e-02,  7.2473e-02,  6.3509e-02, -9.6659e-02,\n",
            "         3.2465e-02,  4.4939e-02,  3.6742e-02, -1.3732e-01, -5.0668e-03,\n",
            "        -1.0867e-01, -3.6120e-02, -5.0998e-03, -5.8174e-02, -1.4730e-02,\n",
            "        -7.2590e-02, -4.3036e-02, -8.6519e-02, -2.8613e-02, -1.8881e-02,\n",
            "        -1.8718e-02,  9.3062e-02, -4.9632e-02, -3.2292e-02,  1.9902e-02,\n",
            "        -8.6374e-02, -2.2647e-02, -9.1689e-02,  2.6578e-02, -1.2772e-01,\n",
            "        -1.0356e-01,  3.1135e-02, -9.5284e-02,  5.4222e-02,  4.7273e-02,\n",
            "        -4.4214e-02,  2.1283e-02, -4.1030e-02, -1.4686e-01, -1.6848e-01,\n",
            "         5.2594e-03, -1.0676e-01,  4.8309e-02, -9.3499e-02, -5.5058e-02,\n",
            "        -5.7908e-02, -1.7933e-02, -4.3438e-02, -1.3059e-01, -2.4160e-02,\n",
            "        -6.9908e-03, -2.5864e-02, -5.0902e-02,  1.6604e-02,  8.9533e-02,\n",
            "        -6.5033e-02, -7.9915e-02,  3.5131e-02, -4.3749e-03,  2.8347e-02,\n",
            "         9.2552e-03, -8.5458e-02, -1.3573e-01,  1.7077e-02, -3.4571e-02,\n",
            "        -3.7896e-04, -1.3808e-02, -4.7865e-02, -4.6461e-02, -3.9436e-02,\n",
            "        -2.5896e-02, -1.0460e-02, -5.9742e-02, -5.1335e-02, -7.9553e-02,\n",
            "        -6.5883e-02, -8.0358e-02,  2.6505e-02,  7.1873e-02, -1.9227e-02,\n",
            "        -7.0099e-02, -2.3376e-02, -7.9206e-02,  8.2827e-04,  1.4612e-02,\n",
            "         1.2630e-02, -1.2755e-02, -2.2332e-02,  2.5340e-02, -6.9585e-02,\n",
            "        -1.5328e-01,  1.1375e-03, -8.6955e-02,  6.8137e-02, -4.7797e-02,\n",
            "         1.8844e-02, -7.9612e-02, -1.4139e-01,  7.2054e-03, -8.6015e-03,\n",
            "         1.8380e-02, -7.3243e-03,  1.8561e-02, -5.2474e-02, -8.2518e-02,\n",
            "        -4.2025e-02, -6.7892e-03,  8.0981e-03, -6.4537e-02, -9.9288e-02,\n",
            "         8.8514e-02, -7.7035e-02, -2.4197e-02, -1.0051e-01,  1.4705e-01,\n",
            "        -6.2921e-02, -6.3178e-02, -3.8450e-02, -2.1408e-02,  2.5654e-02,\n",
            "         1.1040e-01,  9.0883e-03, -3.2891e-02, -1.4906e-01, -1.2870e-01,\n",
            "         9.5794e-04,  1.6663e-02, -7.0759e-02, -6.3398e-02, -5.2613e-02,\n",
            "         2.6132e-02, -2.0406e-02, -1.3057e-01, -7.9481e-02,  2.2808e-02,\n",
            "        -1.7124e-02, -2.8898e-02, -1.0358e-01, -2.1171e-02,  3.8249e-02,\n",
            "        -7.3625e-02, -8.3378e-02, -7.7610e-02, -3.4281e-02, -4.2846e-03,\n",
            "        -8.1903e-03, -1.0042e-01, -1.1125e-01, -3.7076e-02,  7.4860e-03,\n",
            "        -6.7294e-02, -5.2037e-02,  7.2620e-02, -1.0061e-01,  5.9421e-02,\n",
            "         3.0626e-02, -8.8924e-02, -5.0402e-02, -5.8265e-03, -8.6237e-03,\n",
            "         1.2796e-03, -2.5012e-01, -1.3213e-02, -1.0232e-02, -2.1428e-02,\n",
            "        -2.7362e-02,  9.1895e-03, -8.9266e-02, -3.0052e-02, -6.3955e-02,\n",
            "        -2.1762e-02, -3.4595e-02, -6.5752e-02, -4.1349e-02, -1.0338e-01,\n",
            "        -4.0735e-02, -4.0802e-02,  5.5953e-02, -6.2148e-02, -1.0414e-01,\n",
            "        -1.2424e-01, -5.4106e-02, -4.6655e-02, -4.4401e-02, -6.0556e-02,\n",
            "        -5.7612e-02,  3.6996e-02, -2.6795e-02, -1.0736e-01, -1.6758e-02,\n",
            "        -2.8592e-03, -2.0220e-02, -2.6132e-02, -1.6469e-02,  2.2311e-02,\n",
            "        -1.0203e-01, -3.6910e-02, -3.7493e-02, -6.9833e-02, -7.1135e-02,\n",
            "        -5.5654e-02, -1.5174e-01,  3.9553e-02, -7.4312e-02, -2.9360e-02,\n",
            "        -5.6188e-02, -1.0084e-01, -8.4631e-02, -1.2945e-01, -5.0129e-02,\n",
            "        -6.2667e-02, -9.0259e-02, -6.9464e-02, -1.0825e-01, -5.5985e-02,\n",
            "         4.1804e-02, -1.0288e-01, -2.6528e-02,  6.7392e-02, -3.5018e-02,\n",
            "         5.5683e-02, -5.7320e-02, -8.3557e-02, -5.2974e-02, -1.3502e-01,\n",
            "        -6.3524e-02, -5.5015e-02, -9.2393e-02, -8.9890e-02, -1.8675e-02,\n",
            "        -1.7783e-02, -1.0325e-01, -8.2868e-02, -5.5405e-02,  1.1491e-02,\n",
            "        -6.3522e-02,  4.3123e-02, -1.1421e-01,  1.5681e-02, -5.2091e-02,\n",
            "        -5.5900e-03, -2.2682e-02, -1.2387e-01, -1.2129e-01, -4.4534e-02,\n",
            "        -1.4000e-01, -1.1038e-02, -6.8755e-04, -3.3310e-02, -6.8200e-02,\n",
            "        -7.4302e-03, -1.3760e-01, -6.3926e-02, -6.2201e-02, -2.1785e-02,\n",
            "        -3.9153e-02,  1.3362e-02, -1.2367e-01, -2.9912e-04,  1.2394e-02,\n",
            "        -6.9428e-02, -5.5328e-02, -7.4410e-02,  6.1534e-02, -3.0991e-02,\n",
            "        -4.2157e-02, -5.9767e-02, -5.0850e-02, -2.5900e-02, -4.2066e-02,\n",
            "        -4.1019e-02,  4.8172e-02, -5.5493e-02,  1.1516e-02,  4.3468e-03,\n",
            "        -8.6590e-02, -1.4976e-02,  1.5131e-03, -6.1278e-02, -1.0165e-01,\n",
            "        -9.4644e-02, -1.5040e-01, -8.0096e-02, -3.7907e-02, -1.2631e-01,\n",
            "        -4.7390e-02, -4.8927e-02, -1.0660e-01, -7.6180e-02, -2.4753e-02,\n",
            "        -7.6707e-02,  2.3271e-02, -7.5854e-02, -5.1075e-02, -5.1269e-02,\n",
            "         1.3336e-01, -3.4960e-02, -9.3685e-03,  3.8359e-03, -9.5763e-02,\n",
            "         3.2682e-02, -1.7301e-02,  3.9936e-02, -1.7392e-02, -2.4943e-03,\n",
            "        -2.4463e-02, -8.9341e-02, -7.1354e-02, -6.0614e-02, -1.7234e-01,\n",
            "         1.7273e-02, -6.3628e-02, -1.4222e-01,  2.6857e-02, -2.5513e-02,\n",
            "        -3.5461e-02, -6.6671e-02, -1.2773e-03,  3.5062e-03, -2.2206e-02,\n",
            "        -7.4084e-02, -3.2961e-02,  1.7992e-02, -7.0513e-02, -1.0699e-01,\n",
            "        -1.0735e-02,  3.3612e-02, -2.3259e-02, -1.3494e-01,  6.0601e-02,\n",
            "        -7.0173e-02,  6.8896e-02, -1.0629e-01, -1.3854e-02,  6.7230e-03,\n",
            "        -5.7314e-02, -7.5823e-02, -5.2118e-02,  4.6215e-02,  3.0333e-02,\n",
            "        -4.6445e-02, -2.3776e-02, -2.0884e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.9.attention.self.query.weight', Parameter containing:\n",
            "tensor([[ 0.0800,  0.0057, -0.0042,  ..., -0.0356, -0.0040,  0.0070],\n",
            "        [-0.0144, -0.0531, -0.0611,  ..., -0.0048, -0.0642,  0.0032],\n",
            "        [-0.0422,  0.0086, -0.0152,  ..., -0.0499,  0.0145,  0.0231],\n",
            "        ...,\n",
            "        [-0.0032, -0.0186,  0.0253,  ..., -0.0898, -0.0989,  0.0352],\n",
            "        [ 0.0277,  0.0576, -0.0266,  ...,  0.0038,  0.0111, -0.0412],\n",
            "        [-0.0708,  0.0044,  0.0179,  ..., -0.0138, -0.0296, -0.0328]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.9.attention.self.query.bias', Parameter containing:\n",
            "tensor([-1.4508e-01,  4.0926e-01, -3.2818e-01, -2.8698e-02, -2.6105e-01,\n",
            "        -8.8865e-03,  2.8272e-01,  2.9035e-01, -8.6735e-02,  3.9802e-01,\n",
            "        -4.6361e-01,  2.4600e-01, -2.4015e-03,  1.6364e-01, -1.6594e-01,\n",
            "         1.1448e-01, -1.6527e-01, -1.9590e-01, -4.7394e-01,  1.3412e-02,\n",
            "        -3.2246e-01, -2.0455e-01, -2.9871e-01, -7.5479e-01, -2.5058e-01,\n",
            "        -4.8019e-01,  3.2120e-01,  4.7050e-01, -4.2813e-01, -2.3382e-01,\n",
            "         1.5975e-01,  4.1236e-01,  4.7742e-02, -1.6802e-01,  3.1377e-01,\n",
            "         3.1468e-02,  3.5128e-01,  2.1830e-01,  4.0055e-01, -2.1037e-01,\n",
            "         9.8628e-02,  5.1369e-02,  1.0907e-01,  1.2204e-01, -1.9320e-01,\n",
            "         3.0569e-02,  3.9870e-01,  3.7900e-01, -4.4496e-02, -7.2506e-02,\n",
            "         2.7730e-01, -6.3626e-02, -1.2510e-01,  2.5065e-01, -1.4116e-01,\n",
            "         3.8364e-03, -3.4918e-01,  6.1462e-02, -4.0289e-02,  2.7615e-01,\n",
            "        -4.3128e-01,  3.8404e-01,  3.5214e-01,  8.8987e-02, -2.0994e-01,\n",
            "        -3.0099e-01,  4.4209e-01,  1.7082e-02,  2.6703e-01, -2.1336e-01,\n",
            "        -3.5081e-01, -2.4673e-01,  1.2711e-01, -5.1729e-02, -7.7975e-02,\n",
            "         2.9086e-01, -3.9449e-02, -4.0342e-02, -9.4289e-02, -1.9269e-01,\n",
            "        -2.8471e-01, -1.5820e-01, -1.2040e-01,  2.7786e-01, -4.6609e-01,\n",
            "         8.6754e-02, -4.4512e-02,  2.6015e-01,  3.0046e-01, -2.1110e-01,\n",
            "        -3.2694e-02, -3.2132e-01,  3.9820e-02,  1.2422e-01, -4.5760e-02,\n",
            "        -1.3882e-01,  1.0724e-02,  6.0416e-01,  5.6361e-02,  3.3893e-01,\n",
            "        -1.0727e-01, -2.9847e-01,  1.3799e-01,  1.4339e-01,  1.0604e-01,\n",
            "         3.0317e-02, -2.1509e-02, -2.8212e-01,  3.6434e-01, -7.3510e-04,\n",
            "         1.8325e-01,  5.7849e-01, -1.3259e-01, -5.4389e-02, -3.4781e-01,\n",
            "        -1.3035e-01,  1.4072e-01, -3.9989e-02, -2.4629e-01, -1.7324e-01,\n",
            "        -2.7020e-01, -1.1128e-01, -2.4905e-01,  5.6958e-02,  6.3161e-02,\n",
            "        -1.1693e-02,  3.0437e-01,  3.5988e-03,  2.7290e-01, -1.0965e-01,\n",
            "        -5.8476e-02, -6.2916e-02,  1.9371e-01, -9.6598e-02,  5.3410e-02,\n",
            "        -6.9992e-02,  1.6707e-01,  9.9596e-02,  6.8458e-02, -9.0280e-02,\n",
            "        -5.6491e-03,  1.0587e-01,  8.4272e-02, -6.0270e-02, -9.9839e-02,\n",
            "         9.5956e-02, -2.6354e-02,  1.8333e-01,  1.0886e-01, -3.4768e-02,\n",
            "         1.0146e-02,  4.3109e-02,  7.3632e-02, -1.8869e-01,  4.9009e-03,\n",
            "         9.7823e-02, -1.5463e-01,  9.6551e-02, -1.7016e-01, -4.2408e-03,\n",
            "         3.7589e-02,  1.0548e-01, -2.4917e-01, -6.7972e-02,  4.9516e-02,\n",
            "         5.8423e-02, -4.4414e-01, -1.8490e-01, -1.1509e-02, -8.6616e-02,\n",
            "        -1.2172e-01,  1.8567e-01,  9.8076e-04, -9.4050e-02, -1.4864e-02,\n",
            "         8.6890e-02,  4.7251e-02,  6.5057e-02, -7.7482e-02, -2.7404e-01,\n",
            "         8.3824e-02, -2.8188e-03,  3.5813e-02,  3.1096e-01, -4.1060e-02,\n",
            "         2.5648e-02, -1.0568e-01, -3.5177e-01,  9.1710e-02,  1.3746e-01,\n",
            "        -2.9364e-04, -6.5756e-03,  7.8764e-02, -2.5017e-02, -1.1843e-01,\n",
            "        -8.3804e-02,  9.4933e-03,  4.1693e-02, -1.8016e-01,  2.5127e-01,\n",
            "         1.1569e-02,  1.3666e-01,  2.3710e-01,  2.8105e-01,  6.2747e-02,\n",
            "         6.3472e-03,  5.2125e-02,  3.1336e-02,  9.0075e-02,  2.9148e-01,\n",
            "        -1.7410e-03,  4.2045e-02,  6.5335e-02, -8.4220e-02,  2.7035e-01,\n",
            "        -5.3156e-03, -9.5809e-02, -2.1902e-01, -1.4360e-01,  4.1428e-01,\n",
            "         5.8233e-02, -6.6352e-02,  1.6432e-01,  6.9439e-02, -1.2712e-01,\n",
            "        -1.2199e-01, -1.7467e-01,  5.6447e-02, -7.6137e-02, -7.6485e-02,\n",
            "        -1.5909e-01, -8.5181e-02,  4.9547e-02,  1.1584e-01, -1.7746e-01,\n",
            "        -9.2439e-02,  1.1206e-02,  6.6615e-02,  2.8272e-01,  1.8882e-02,\n",
            "         4.5987e-01,  1.4605e-01, -4.1574e-02, -1.1825e-01, -2.1566e-01,\n",
            "        -1.4596e-01,  8.5446e-02, -7.8131e-02,  7.0683e-02,  2.4640e-01,\n",
            "         1.2893e-02,  3.6038e-02,  1.6615e-01,  1.0816e-01,  7.0824e-02,\n",
            "        -3.0924e-02, -1.1722e-01,  1.4870e-01, -2.9171e-01, -1.8393e-01,\n",
            "        -9.4880e-02, -7.7596e-02,  2.7846e-01, -1.7737e-02, -1.3460e-01,\n",
            "         2.7650e-01,  1.5727e-01,  3.2214e-01,  1.4791e-01,  9.5211e-02,\n",
            "        -5.6741e-02, -3.2463e-01, -4.2387e-01,  7.3569e-01, -2.6549e-02,\n",
            "        -5.4388e-02,  1.1667e-01,  4.0011e-01,  3.0355e-01, -8.6620e-02,\n",
            "         2.8283e-01,  3.5839e-02,  1.7588e-01, -1.3057e-01, -2.8893e-01,\n",
            "        -1.1985e-01, -2.5533e-01,  2.4863e-01,  2.6188e-01, -4.8448e-02,\n",
            "        -1.2723e-01,  2.2823e-01,  4.7734e-01,  4.1029e-01,  1.2397e-01,\n",
            "         8.9113e-02,  1.3167e-01, -1.6434e-01,  1.7638e-01, -1.1045e-01,\n",
            "         1.1808e-01, -1.1646e-01, -1.8070e-02,  9.9072e-02,  2.3005e-01,\n",
            "         3.3255e-01,  6.0030e-02,  9.3791e-02,  1.6549e-01, -1.4476e-01,\n",
            "         1.3611e-01,  2.3269e-01,  8.3314e-02, -8.7980e-02,  2.6533e-02,\n",
            "        -1.6021e-01,  8.5629e-02, -2.0695e-01, -1.9889e-01, -3.0654e-02,\n",
            "        -2.2832e-02,  4.4552e-02, -2.5519e-02, -1.2239e-01,  1.1476e-03,\n",
            "         1.5598e-01,  9.5248e-02, -4.4761e-02, -8.0380e-02, -1.4541e-01,\n",
            "         2.6812e-01,  1.6802e-01,  7.8162e-02,  3.1746e-02,  9.4722e-03,\n",
            "         7.2411e-02,  2.1135e-01,  1.7691e-01,  7.0296e-02, -2.9222e-02,\n",
            "        -6.7535e-02,  7.9958e-02,  1.0588e-01,  1.0048e-01,  7.0618e-02,\n",
            "         1.0144e-01, -8.2547e-03,  1.1782e-01, -2.3025e-01, -3.9648e-01,\n",
            "         7.4790e-02, -1.8932e-01, -2.9609e-02,  3.0226e-01, -9.8183e-02,\n",
            "         6.4902e-02, -3.1430e-01,  4.6957e-02, -2.0649e-01,  1.1137e-01,\n",
            "        -1.2659e-01, -3.5504e-02, -8.8870e-02,  8.3719e-02, -5.1739e-02,\n",
            "        -5.2174e-02, -6.5855e-02, -5.0843e-02,  2.9274e-02, -5.3974e-02,\n",
            "         5.3451e-02,  1.3143e-02, -6.8927e-02, -3.5941e-02,  1.4883e-01,\n",
            "        -6.1372e-02, -3.3196e-02, -6.3640e-02,  1.9921e-02, -5.1276e-02,\n",
            "        -7.2964e-04, -1.1961e-01,  8.3787e-02, -1.2137e-01, -2.0852e-01,\n",
            "         2.9758e-01, -3.8699e-01,  7.7345e-02, -5.1349e-03, -8.6968e-02,\n",
            "        -8.0035e-03,  3.0792e-01,  1.9037e-01,  9.1157e-03, -1.8449e-01,\n",
            "        -1.0910e-02,  3.7047e-01, -2.1832e-01, -2.6093e-01,  3.8370e-01,\n",
            "         2.0055e-01, -2.9054e-01, -2.0295e-01, -2.7846e-01,  4.5800e-02,\n",
            "        -1.3268e-01,  2.2434e-03, -8.5056e-04,  5.8994e-01,  2.7035e-01,\n",
            "        -1.7660e-01,  5.2023e-02, -1.4849e-02,  1.5524e-01,  7.1256e-02,\n",
            "         1.5615e-01,  3.7199e-01, -1.6906e-01, -2.4232e-01, -1.9754e-01,\n",
            "         2.2402e-01, -2.6044e-01, -7.0554e-01,  1.0146e-01,  4.2055e-02,\n",
            "        -6.6369e-02, -1.6213e-01, -9.5376e-02, -4.5736e-01,  1.6891e-01,\n",
            "         8.3917e-02, -7.0126e-01,  1.6386e-01,  1.7145e-01,  5.2917e-02,\n",
            "        -1.0409e-01, -2.7164e-01, -8.6378e-02,  3.9808e-02,  1.4597e-01,\n",
            "         2.8720e-01, -1.1189e-01,  3.3640e-01, -3.4760e-02,  1.2804e-01,\n",
            "         2.9205e-02, -4.9959e-01, -2.6029e-01, -2.9684e-02,  2.5219e-01,\n",
            "        -1.6198e-01,  4.0059e-02, -7.1787e-02, -1.9445e-01, -3.2336e-01,\n",
            "         2.8838e-01,  1.2361e-01,  5.0745e-01,  5.0571e-01,  4.5394e-02,\n",
            "         4.2681e-01, -2.4060e-01, -4.4395e-02,  2.8781e-01, -3.1175e-01,\n",
            "        -1.1434e-02,  3.0151e-01, -1.3853e-02, -2.2816e-02,  1.7311e-01,\n",
            "         1.5809e-01,  4.0244e-02, -2.3952e-01,  3.0609e-01, -2.6309e-01,\n",
            "         9.7075e-02, -4.3569e-01, -2.2369e-01,  4.2203e-01,  1.6532e-01,\n",
            "        -2.8449e-01, -3.1811e-01, -1.3243e-01,  2.8926e-01, -1.6043e-01,\n",
            "        -1.5083e-01, -3.8745e-02, -8.5798e-02, -1.2027e-01,  2.3618e-01,\n",
            "         7.3756e-02, -6.0224e-02, -7.0630e-02, -2.9251e-01, -1.5558e-01,\n",
            "        -1.4424e-01, -2.9698e-01,  1.8194e-01,  1.6356e-01,  1.4792e-01,\n",
            "        -5.6139e-02, -1.5214e-01,  1.3954e-01,  2.0345e-01, -3.5457e-01,\n",
            "         3.1902e-01,  2.9132e-01, -2.5712e-02, -3.4282e-01,  2.8795e-01,\n",
            "        -1.0619e-01, -2.2829e-01, -1.9555e-02, -1.2334e-02, -3.7453e-02,\n",
            "         2.0738e-02,  1.7494e-02,  1.3835e-02, -1.4225e-02, -2.8143e-02,\n",
            "        -1.9602e-02, -3.0784e-02,  7.1251e-02,  1.5991e-02,  3.3960e-02,\n",
            "        -3.2359e-02,  2.7350e-02, -8.7807e-06, -5.5479e-02,  6.1875e-02,\n",
            "         2.5380e-02,  5.2908e-02, -1.1359e-01,  9.6852e-03,  6.4524e-02,\n",
            "        -7.2602e-03, -6.2563e-02, -6.1265e-02,  2.4494e-02, -2.9495e-02,\n",
            "        -1.7313e-01,  7.4921e-02,  4.5862e-02,  6.1688e-02, -9.3834e-03,\n",
            "         8.4144e-02,  4.5950e-03, -1.0053e-01, -5.6149e-02,  1.1004e-01,\n",
            "         1.4374e-02, -4.6802e-03,  5.2541e-02, -3.6170e-02, -8.5449e-03,\n",
            "        -5.5921e-03, -3.0417e-02,  1.2495e-02,  4.3075e-02,  2.1870e-02,\n",
            "         5.5257e-02,  2.2086e-02,  1.0849e-01,  2.0664e-03,  2.6770e-03,\n",
            "        -1.7639e-02,  3.0090e-02,  2.7612e-02,  1.7620e-02,  1.1148e-02,\n",
            "         8.8009e-03,  5.0295e-02, -1.9110e-02,  7.2534e-02,  5.2390e-02,\n",
            "         6.6740e-02, -1.2662e-01, -1.9853e-01,  8.5399e-02, -1.7892e-01,\n",
            "         3.9913e-01, -5.4832e-03,  7.6531e-01, -1.6634e-01,  9.4297e-02,\n",
            "        -1.9521e-01, -1.0406e-01, -1.7094e-01,  6.5702e-02, -2.9591e-01,\n",
            "         3.5312e-01,  1.4194e-01,  5.0648e-02,  1.2027e-01,  3.7924e-02,\n",
            "        -1.1917e-01, -4.7965e-01,  4.4047e-02, -2.6374e-02,  2.2073e-01,\n",
            "        -5.3977e-02, -1.7584e-01,  4.8521e-01,  2.1894e-01,  1.4986e-01,\n",
            "         4.1464e-03,  6.5722e-02,  1.4616e-01, -4.9844e-02, -6.8974e-03,\n",
            "        -2.8312e-01, -5.7417e-01, -9.2799e-02, -1.4554e-01, -3.8905e-01,\n",
            "         2.3642e-01, -5.8813e-02, -1.1237e-01,  1.3307e-01, -3.8011e-02,\n",
            "        -1.4164e-01, -4.4391e-02, -1.3205e-01, -9.9911e-03,  2.7255e-02,\n",
            "        -2.7728e-02,  4.6621e-03, -3.8725e-01,  1.7742e-01,  2.1446e-01,\n",
            "         3.1606e-01,  1.2446e-01, -8.7867e-03,  6.6143e-02, -5.5724e-01,\n",
            "         1.5215e-01,  5.1904e-01,  1.2371e-01, -3.4215e-02, -3.6434e-01,\n",
            "        -7.3646e-02, -1.3974e-01,  1.4744e-01, -5.5181e-02, -1.6336e-01,\n",
            "        -8.3859e-03,  1.2114e-01,  2.0644e-01,  3.3841e-01, -8.9701e-02,\n",
            "        -6.3812e-02,  1.7981e-01,  1.5882e-01,  1.8502e-02,  3.1529e-02,\n",
            "        -4.1496e-03, -9.2225e-02,  3.1884e-02,  9.3185e-02, -1.5027e-01,\n",
            "        -1.9594e-01, -2.9305e-02, -4.3637e-01, -1.9229e-01,  8.2728e-02,\n",
            "         2.6421e-01, -2.2657e-01, -1.2674e-01, -1.0844e-02, -1.1295e-02,\n",
            "         2.1947e-03,  5.3566e-02,  1.6224e-01,  1.4061e-01,  9.1433e-02,\n",
            "        -2.7725e-02, -1.3336e-01, -1.6402e-01,  8.1144e-03,  1.6365e-01,\n",
            "         3.3805e-02,  1.1567e-01,  8.9398e-02, -7.8129e-03, -7.8674e-02,\n",
            "        -1.2069e-01,  1.2097e-02,  1.9971e-02,  2.3145e-01,  5.1792e-02,\n",
            "         4.1265e-01, -7.7811e-02,  1.9249e-01,  2.1463e-01, -3.5297e-01,\n",
            "         2.0544e-01,  5.1848e-02,  2.1828e-01,  2.9808e-02, -4.2434e-01,\n",
            "        -8.1091e-03, -1.2185e-01, -4.8018e-02,  1.8182e-01, -1.8021e-02,\n",
            "         8.1570e-03,  3.5845e-02, -9.9781e-02,  9.8045e-02,  1.0515e-02,\n",
            "         5.8585e-02,  1.2970e-01, -3.9113e-02,  2.0388e-02,  9.9133e-02,\n",
            "         4.1355e-03, -7.0560e-02, -1.0300e-03, -1.1426e-01, -2.2303e-03,\n",
            "         7.0617e-02,  1.0825e-01, -3.2928e-02,  3.0130e-03,  1.4302e-01,\n",
            "         2.5474e-02,  2.0752e-02, -6.9888e-02,  2.6346e-03, -1.9667e-02,\n",
            "         6.8254e-02,  6.9824e-02, -7.5233e-02, -3.2858e-02, -7.6477e-03,\n",
            "        -9.2041e-02,  1.1518e-01,  1.1256e-02,  1.4313e-01, -5.3999e-02,\n",
            "         7.3000e-03,  6.2105e-02,  3.7213e-02,  8.1041e-02, -1.3787e-01,\n",
            "         5.5780e-02,  9.5053e-03, -8.1575e-02, -2.1757e-02, -6.3689e-02,\n",
            "         2.3786e-02,  2.5164e-02, -1.0112e-01,  3.1668e-02, -3.8108e-03,\n",
            "         3.5306e-02, -4.5158e-02,  8.9193e-02, -7.6088e-02, -4.0536e-02,\n",
            "        -3.5524e-02, -1.2220e-01,  7.1624e-02,  9.4083e-02,  4.6922e-02,\n",
            "        -5.8193e-03, -5.8785e-02, -1.7030e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.9.attention.self.key.weight', Parameter containing:\n",
            "tensor([[-0.0134,  0.0076, -0.0528,  ...,  0.0407,  0.0475,  0.0061],\n",
            "        [ 0.0062, -0.0475, -0.0587,  ...,  0.0382, -0.0662,  0.0153],\n",
            "        [-0.0456, -0.0293,  0.0124,  ..., -0.0534, -0.0399, -0.0444],\n",
            "        ...,\n",
            "        [-0.0419,  0.0134, -0.0004,  ...,  0.0382,  0.1147, -0.0226],\n",
            "        [-0.0432,  0.0020, -0.0379,  ...,  0.0291, -0.0586, -0.0410],\n",
            "        [ 0.0315,  0.0287, -0.0462,  ...,  0.0060,  0.0166, -0.0080]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.9.attention.self.key.bias', Parameter containing:\n",
            "tensor([-5.1302e-03,  8.3288e-03,  3.7764e-03,  9.2886e-03, -4.1818e-03,\n",
            "        -8.0358e-03,  3.5761e-03,  5.2791e-03, -3.3368e-03,  3.7632e-03,\n",
            "        -1.5047e-02, -2.0337e-04,  5.4255e-04,  1.0059e-02, -3.7468e-03,\n",
            "         3.0774e-03, -5.6145e-03, -5.8512e-03,  3.1476e-03, -3.6925e-03,\n",
            "        -3.6824e-03, -9.4856e-03, -8.8461e-03,  9.3814e-03, -8.3268e-04,\n",
            "        -8.7729e-06,  7.3185e-03, -1.2624e-03, -7.3559e-03, -7.0218e-04,\n",
            "         1.2133e-02,  5.2247e-03,  7.1147e-04, -1.1045e-03, -1.0163e-02,\n",
            "         1.6899e-03, -6.4589e-03,  4.5035e-03, -8.3532e-04, -1.3693e-02,\n",
            "        -6.7875e-04, -1.4988e-03, -5.6895e-03, -4.9859e-04, -5.1949e-03,\n",
            "        -5.2369e-03,  2.4274e-04, -1.3338e-03,  6.7811e-03,  4.1668e-03,\n",
            "         6.2696e-03,  1.1148e-02, -9.6457e-03,  1.2593e-02, -9.0818e-04,\n",
            "        -6.4721e-03, -4.5739e-04,  7.4203e-03,  6.2956e-03, -3.1220e-03,\n",
            "         3.9140e-03,  1.2706e-02, -6.6556e-03,  2.3061e-03, -6.5851e-03,\n",
            "         3.6104e-03, -8.9584e-03, -2.0235e-03, -1.1585e-03,  7.3765e-03,\n",
            "        -1.9621e-03,  8.3438e-03, -3.7611e-03,  7.8270e-03, -3.1439e-03,\n",
            "         3.2501e-05,  7.7944e-03,  7.3723e-03,  3.7257e-04,  1.3025e-03,\n",
            "        -5.0248e-03,  1.2767e-02,  6.4775e-04,  3.0374e-03,  2.4107e-03,\n",
            "         6.6254e-04, -1.6845e-03, -9.3327e-03, -3.7502e-03,  1.4687e-03,\n",
            "         4.7891e-03, -4.0374e-04, -2.3680e-03, -2.5337e-03,  1.6769e-02,\n",
            "         6.1376e-03, -5.8544e-03, -3.0495e-03,  1.0262e-02,  1.2358e-03,\n",
            "         5.9900e-04,  2.1606e-03, -4.6924e-03,  1.2841e-03,  8.3675e-03,\n",
            "        -3.1137e-03, -4.2472e-03, -5.3760e-03,  8.4747e-03, -3.6133e-04,\n",
            "        -8.0652e-03, -2.4218e-03,  2.0553e-03, -4.0989e-03, -2.7732e-03,\n",
            "        -1.7795e-03, -3.0304e-03, -1.2412e-02,  7.3258e-03, -9.0168e-03,\n",
            "         2.6175e-04,  3.2975e-03, -3.3541e-03, -7.1123e-03, -4.3704e-03,\n",
            "         1.6353e-03, -4.8155e-03,  5.7381e-03,  1.3486e-03,  1.9242e-03,\n",
            "         2.8553e-03, -1.3435e-03,  4.1447e-03,  8.5664e-03, -3.3285e-03,\n",
            "        -5.6373e-03,  3.7651e-03,  8.7291e-04,  9.6523e-03,  2.4397e-03,\n",
            "        -3.5199e-03, -4.2437e-03, -5.7500e-03,  3.3118e-03, -4.8787e-04,\n",
            "         5.5366e-03,  1.5445e-04, -4.2360e-03,  8.2384e-03,  6.7028e-03,\n",
            "        -4.4803e-03,  3.9777e-03,  7.7531e-04, -4.2377e-03,  4.4656e-03,\n",
            "        -6.9703e-04,  5.9160e-03,  2.5390e-03, -5.2017e-03, -9.4360e-04,\n",
            "        -4.0294e-03,  1.3683e-03, -6.2375e-03,  3.9049e-03, -2.4717e-03,\n",
            "        -3.9250e-03,  6.6070e-03, -1.5868e-03,  2.5860e-03,  2.5959e-03,\n",
            "        -2.5748e-03,  4.7034e-03,  3.0377e-03, -9.9056e-03, -6.4792e-03,\n",
            "         2.8331e-03,  6.7968e-03, -8.4656e-04, -2.7791e-03, -4.8440e-03,\n",
            "         5.0385e-03, -1.3060e-03, -3.0454e-04,  1.9929e-03, -5.7856e-03,\n",
            "        -1.7757e-03, -2.9631e-03,  1.1093e-02,  4.0936e-03,  2.6135e-03,\n",
            "        -3.0115e-03, -1.4701e-03,  3.9772e-03, -9.1698e-03,  1.0290e-03,\n",
            "        -4.5145e-03,  1.8637e-03,  1.3076e-02, -3.0637e-03,  9.1629e-04,\n",
            "        -1.0319e-03,  7.4526e-03,  5.5684e-04,  1.4114e-02, -3.1485e-03,\n",
            "         5.4861e-05,  7.7189e-03,  5.8066e-03,  1.1663e-03, -1.1941e-02,\n",
            "         5.2102e-03, -2.1057e-02,  1.0827e-02,  3.1306e-03,  1.0640e-03,\n",
            "         2.2251e-03, -5.2622e-03,  1.3634e-02,  2.5390e-03, -2.3443e-03,\n",
            "        -3.7504e-03,  3.1144e-03, -2.5482e-03, -3.2639e-03,  6.3506e-04,\n",
            "        -5.2913e-03, -6.2382e-03,  1.2539e-02, -6.3245e-03, -7.9098e-03,\n",
            "        -6.6960e-03,  1.9819e-02,  9.6758e-03, -4.0976e-03,  3.3031e-03,\n",
            "         1.5807e-03, -4.6316e-04, -2.4124e-02,  7.6012e-03, -6.6087e-03,\n",
            "         1.4942e-02,  4.3872e-03, -2.5019e-03, -5.9275e-03, -3.1618e-04,\n",
            "         8.4589e-04, -3.0197e-03, -6.9097e-03,  2.9668e-03,  1.6689e-04,\n",
            "        -1.2891e-02,  5.6164e-03, -9.5172e-03,  2.3087e-03,  3.2720e-04,\n",
            "        -1.6914e-02,  1.1818e-03, -2.1898e-03, -6.3370e-03,  7.7774e-03,\n",
            "         8.0652e-03,  4.5111e-03, -1.2571e-02,  1.3353e-04, -7.6691e-03,\n",
            "        -2.0226e-03,  6.9852e-03,  8.8881e-03,  1.3188e-03, -2.9915e-03,\n",
            "         2.2818e-03,  7.7778e-04,  7.1906e-03,  4.8195e-03, -1.4466e-03,\n",
            "        -5.4210e-03, -3.4155e-03, -5.4726e-03,  7.0005e-03,  3.6501e-03,\n",
            "        -1.4743e-03,  4.3023e-03,  7.7081e-05,  6.1213e-03, -1.0957e-03,\n",
            "        -5.0410e-03,  8.6026e-03, -1.9473e-03,  4.1328e-03, -3.6619e-03,\n",
            "         4.2601e-03, -2.4364e-04,  5.9566e-03,  1.0200e-02, -3.9990e-03,\n",
            "         5.4388e-04,  2.3225e-03,  3.7459e-03,  5.5977e-03, -4.7886e-03,\n",
            "        -3.9621e-03, -5.4964e-03, -3.9628e-03,  4.2827e-03, -3.2566e-03,\n",
            "         1.5054e-03, -1.0894e-03,  7.4682e-03, -1.0859e-03,  2.4069e-03,\n",
            "         7.3260e-03, -2.8343e-03,  2.9442e-03,  2.4487e-03, -3.8532e-03,\n",
            "         1.4515e-03, -4.2675e-03,  1.8276e-03,  6.2296e-03, -7.0243e-03,\n",
            "         2.8384e-03,  1.7949e-03, -4.9913e-03,  5.4674e-03,  1.5858e-03,\n",
            "        -2.7833e-03,  2.0097e-03, -4.2470e-04, -6.1039e-04,  7.1432e-03,\n",
            "         3.3823e-03, -7.2020e-03,  4.6793e-03,  1.1270e-02,  8.0252e-03,\n",
            "        -1.4247e-03, -6.0653e-03, -1.2493e-03,  1.6049e-03, -1.9654e-03,\n",
            "         2.5375e-03,  4.2491e-03,  2.4943e-03, -3.2281e-03,  5.1710e-03,\n",
            "         4.0816e-04, -3.2378e-04, -7.7038e-04, -1.9060e-03, -1.8283e-02,\n",
            "         1.7526e-03,  5.3038e-03,  3.0338e-03, -5.9947e-03, -4.8969e-03,\n",
            "         3.5227e-03,  2.5542e-04, -1.4685e-03,  4.5048e-03,  3.8025e-03,\n",
            "        -4.2944e-03,  8.1016e-03, -2.7421e-03, -3.5800e-03,  3.9776e-04,\n",
            "        -1.3870e-03,  4.7748e-03,  1.7043e-03,  3.2889e-03,  6.8928e-03,\n",
            "         3.6414e-03, -3.4847e-03, -3.4538e-03, -4.5311e-03,  2.8151e-03,\n",
            "         3.7045e-03, -1.6635e-03, -1.6178e-02,  1.2517e-03, -4.1803e-03,\n",
            "        -6.0054e-03,  3.1641e-03,  5.4233e-03,  3.9157e-03,  5.1750e-03,\n",
            "         5.2185e-03,  6.2142e-03, -1.2196e-02, -4.8407e-03,  1.3436e-02,\n",
            "         7.6290e-04,  1.0212e-03, -1.6260e-02,  1.1412e-02, -2.9017e-03,\n",
            "         1.3299e-03, -7.9741e-03,  1.5542e-02,  1.3202e-02,  7.7930e-03,\n",
            "         2.6094e-03, -4.9780e-03, -6.0236e-04, -5.4971e-03,  1.2049e-03,\n",
            "        -7.7591e-03, -2.9220e-03,  1.0948e-02,  1.2156e-02,  8.8086e-03,\n",
            "         2.7358e-03,  4.4100e-03,  3.4410e-03,  8.7051e-03, -1.6376e-02,\n",
            "        -1.0408e-03, -6.3309e-03, -6.5444e-06, -3.6561e-03, -2.8637e-03,\n",
            "         2.4621e-03,  4.8780e-03, -9.5517e-03, -1.8709e-03, -9.3360e-03,\n",
            "        -5.5626e-04,  1.2100e-02, -1.6965e-03, -4.5295e-03,  2.4009e-03,\n",
            "         3.5705e-03, -2.4740e-04,  9.4846e-04,  1.7978e-02,  1.1255e-02,\n",
            "        -1.4110e-02,  9.3378e-03,  5.9406e-04, -1.0816e-03, -1.9153e-03,\n",
            "        -9.9570e-03, -6.4231e-03,  8.3287e-03, -5.2733e-03, -9.2165e-03,\n",
            "        -7.8980e-03, -5.1210e-03, -4.9115e-03,  3.6627e-03,  5.3480e-03,\n",
            "        -4.6405e-03,  1.0611e-02, -1.3988e-02,  1.3989e-02,  5.0791e-03,\n",
            "         4.7481e-03, -1.4192e-02,  2.1132e-03,  5.6276e-03,  7.6014e-03,\n",
            "        -7.2236e-03,  6.4161e-03, -1.8278e-04, -8.9497e-04, -1.5053e-02,\n",
            "        -6.3033e-04, -1.0799e-02, -8.0744e-03, -1.6276e-04, -5.3081e-03,\n",
            "        -1.0770e-03,  7.9639e-03,  1.0000e-02, -9.9652e-03,  3.5813e-03,\n",
            "        -1.3652e-03,  1.1154e-03,  3.4591e-03, -1.9861e-03, -1.7278e-04,\n",
            "        -8.6864e-03, -1.3624e-02,  3.0985e-03, -8.8747e-03,  8.5841e-03,\n",
            "         3.5464e-03,  1.0931e-02, -1.5238e-03,  1.9248e-03, -7.4327e-03,\n",
            "         4.3876e-03,  3.7819e-03,  1.5111e-03,  6.6869e-03,  2.9391e-03,\n",
            "         6.2208e-03,  3.2333e-02,  2.1137e-04, -2.5180e-03, -2.4149e-03,\n",
            "        -1.0009e-02, -9.5479e-03,  1.6218e-03,  1.0529e-02,  1.4993e-02,\n",
            "        -4.4120e-03, -1.0896e-03,  1.0645e-03,  1.8548e-03, -7.7204e-03,\n",
            "        -2.4908e-03,  8.1565e-03,  9.4199e-03, -7.4813e-03, -2.0265e-02,\n",
            "        -3.8370e-04, -6.3512e-04, -3.5159e-03,  4.5740e-03,  3.7967e-03,\n",
            "        -4.6607e-03,  1.4347e-03, -1.0851e-02,  1.0144e-02,  7.2111e-03,\n",
            "         1.2161e-02,  6.5903e-03, -1.1021e-02,  3.9626e-03,  1.8723e-03,\n",
            "        -2.8675e-03,  3.4478e-03,  1.3836e-03, -2.3493e-02, -1.4485e-02,\n",
            "        -1.7388e-03,  1.3186e-03, -2.6020e-03, -9.3740e-03,  8.6373e-03,\n",
            "        -1.0377e-02,  8.4240e-03,  3.4755e-03,  8.0952e-03, -2.5862e-03,\n",
            "        -7.9496e-03,  9.6695e-03, -2.0219e-02,  2.0186e-03, -4.0384e-03,\n",
            "        -1.6687e-03,  6.1062e-03,  1.8790e-02, -5.7767e-03,  3.0407e-03,\n",
            "        -2.4765e-02, -4.4020e-03, -1.5588e-02,  4.6084e-03,  1.2905e-02,\n",
            "         1.6700e-02, -3.4979e-04,  2.8949e-03,  6.3047e-03,  3.4338e-03,\n",
            "        -1.0748e-02,  5.4402e-03, -1.0429e-02, -2.9999e-03, -3.9143e-03,\n",
            "         2.1626e-03, -1.2537e-02, -4.8363e-03, -6.4308e-04,  1.2598e-02,\n",
            "         3.5841e-03,  3.4678e-03, -9.8889e-03, -3.6054e-03, -8.6473e-04,\n",
            "        -6.6963e-03,  4.7846e-03, -1.1669e-03,  1.2587e-02, -4.3402e-03,\n",
            "        -5.4709e-03, -6.8251e-03, -7.8695e-03, -1.2599e-03, -1.1643e-03,\n",
            "         5.2548e-03,  6.2590e-03,  2.7680e-03, -1.6786e-03,  1.8554e-03,\n",
            "        -1.1359e-02,  1.3039e-03, -4.8453e-04, -3.8449e-03, -1.9607e-03,\n",
            "         4.2294e-03, -1.5974e-03, -1.6444e-03,  3.5837e-03,  2.4326e-03,\n",
            "         3.4737e-03,  2.6290e-04,  7.7000e-03, -3.4828e-03,  8.1388e-03,\n",
            "        -9.7263e-03, -4.7892e-03, -4.0172e-04,  7.5825e-03, -1.3446e-03,\n",
            "         7.5464e-03, -1.4075e-03, -7.4817e-03,  2.8995e-03, -7.8101e-03,\n",
            "        -5.9170e-04,  2.2066e-03,  4.9729e-03,  7.4648e-03, -2.4316e-03,\n",
            "         2.7751e-03, -1.0411e-02,  2.9722e-03,  1.2497e-03,  2.2138e-03,\n",
            "         6.4543e-03,  2.6322e-03,  5.0640e-03, -4.4102e-03, -8.2334e-03,\n",
            "        -9.0673e-04,  2.1743e-04,  7.5699e-03,  2.3795e-03, -8.4622e-03,\n",
            "         9.5279e-03, -2.1709e-03, -1.7924e-03, -7.7583e-03, -1.5809e-03,\n",
            "        -5.7801e-03, -8.4893e-03, -7.7946e-04, -2.3795e-03,  7.7800e-04,\n",
            "        -1.7315e-03, -1.4868e-03,  4.0505e-03,  1.4026e-03,  4.0124e-03,\n",
            "        -1.4545e-02,  7.0829e-03, -6.0565e-03, -2.3267e-03,  7.4206e-03,\n",
            "         2.8154e-03, -2.2772e-03, -8.9652e-03,  7.9110e-04, -6.2045e-03,\n",
            "        -1.6368e-03, -4.4610e-03,  7.6555e-03, -4.4458e-03, -4.9349e-03,\n",
            "         9.2547e-03, -3.1462e-03,  5.9065e-03, -3.1377e-03,  2.5414e-03,\n",
            "         2.1657e-03,  2.1611e-03, -7.8510e-03,  1.2627e-02,  3.6479e-03,\n",
            "         5.7595e-04, -4.5245e-03,  4.8133e-03, -9.7969e-04,  7.2643e-03,\n",
            "         2.6739e-03,  6.4362e-03,  6.5354e-03,  3.7420e-03,  4.4758e-03,\n",
            "        -1.5490e-03,  4.5458e-03,  2.2494e-03,  3.5938e-03, -1.0137e-02,\n",
            "        -5.3340e-03,  1.0491e-02,  3.2842e-03, -1.1330e-02,  6.4576e-03,\n",
            "        -7.0191e-03,  5.3850e-03, -4.3149e-03,  5.3758e-03, -3.5054e-03,\n",
            "         6.7394e-03,  3.2263e-04, -8.4865e-03,  6.4615e-03,  5.3898e-04,\n",
            "         5.9353e-03,  1.2575e-03, -3.1138e-03,  6.9690e-04,  9.3829e-03,\n",
            "         7.8740e-03,  1.4360e-03,  5.6550e-03,  9.8997e-03, -5.3634e-03,\n",
            "         6.8690e-03,  7.9267e-03,  1.8343e-03,  4.4525e-03,  8.5435e-03,\n",
            "        -1.0509e-02, -8.7481e-03, -8.5376e-03,  4.9372e-03,  4.4196e-04,\n",
            "        -6.9548e-04, -4.4598e-03,  4.7291e-03,  8.8131e-03, -3.1288e-03,\n",
            "         9.5505e-04, -6.1635e-03,  4.1042e-04,  5.0446e-03,  2.8490e-03,\n",
            "        -1.1715e-02,  5.7349e-04,  4.3664e-03, -9.7794e-03, -2.5288e-03,\n",
            "        -7.7175e-04, -4.1753e-03,  1.3881e-03,  2.0221e-03, -1.1649e-02,\n",
            "         2.8649e-03,  8.3072e-03, -2.5590e-03, -4.0360e-03, -1.3669e-02,\n",
            "         3.5502e-03,  1.0669e-03, -6.0694e-04, -1.4526e-02, -8.3033e-03,\n",
            "        -3.4418e-03, -1.1725e-02, -1.2696e-02,  1.1830e-02, -2.3799e-03,\n",
            "         4.6389e-03, -6.4471e-03,  3.8907e-04], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.9.attention.self.value.weight', Parameter containing:\n",
            "tensor([[-0.0249,  0.0168,  0.0445,  ...,  0.0120,  0.0158,  0.0610],\n",
            "        [-0.0302,  0.0346, -0.0628,  ...,  0.0240,  0.0537,  0.0081],\n",
            "        [ 0.0098, -0.0342,  0.0251,  ...,  0.0588, -0.0178, -0.0413],\n",
            "        ...,\n",
            "        [-0.0009,  0.0506, -0.0198,  ...,  0.0400, -0.0160,  0.0112],\n",
            "        [ 0.0252,  0.0059, -0.0076,  ..., -0.0653, -0.0188, -0.0565],\n",
            "        [-0.0066,  0.0396, -0.0256,  ...,  0.0181, -0.0054,  0.0137]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.9.attention.self.value.bias', Parameter containing:\n",
            "tensor([-4.2299e-02, -2.6367e-02,  6.0447e-03,  2.2283e-03,  4.9795e-02,\n",
            "        -2.6057e-02, -4.1140e-02,  7.3834e-03,  3.7393e-03,  4.1572e-03,\n",
            "        -7.2408e-03,  1.7293e-02, -9.9946e-03, -2.4248e-02, -1.0810e-02,\n",
            "        -5.4682e-03,  1.5715e-02, -3.3484e-02, -4.0313e-02, -2.1808e-02,\n",
            "        -2.2321e-03, -4.2111e-02, -8.2884e-02,  2.2820e-02, -6.0114e-02,\n",
            "        -1.2119e-02, -2.1905e-02, -1.7609e-02, -4.6987e-03, -7.8206e-03,\n",
            "        -5.8907e-03, -1.1677e-02,  2.4826e-02, -3.2271e-02,  2.8491e-02,\n",
            "        -3.4578e-02,  1.9115e-02, -2.2898e-02,  3.8614e-02, -1.2430e-02,\n",
            "         5.8304e-02, -3.3160e-02,  3.4545e-02,  1.7082e-02, -1.8707e-03,\n",
            "        -2.3045e-02, -2.1621e-02,  1.6515e-02, -3.5190e-02,  5.5340e-02,\n",
            "        -2.0109e-02,  4.5303e-02,  2.2088e-02, -4.6221e-02,  4.4516e-02,\n",
            "        -1.7678e-03, -1.6376e-02,  1.1652e-02,  3.1227e-02,  4.8573e-02,\n",
            "         3.7540e-03, -1.9776e-02, -2.0442e-02,  2.9682e-02,  3.0170e-02,\n",
            "        -6.3252e-02, -3.3656e-04, -1.0758e-03, -2.4733e-02, -3.6214e-02,\n",
            "         2.0490e-02,  6.1019e-02,  9.2092e-03, -1.0230e-02,  2.0029e-02,\n",
            "         1.4732e-02, -1.4377e-02,  7.3691e-03,  7.9386e-03,  7.0808e-03,\n",
            "        -6.7272e-03, -1.3726e-02,  1.7939e-02,  3.5181e-02, -1.3306e-03,\n",
            "        -2.7951e-03, -1.0222e-02,  4.5112e-02,  1.9769e-02,  2.6334e-02,\n",
            "         1.3393e-02,  1.2038e-03, -1.0152e-02, -1.1008e-02, -1.1050e-02,\n",
            "         2.9037e-02,  5.4345e-03,  9.1334e-03, -6.0438e-02,  5.3236e-03,\n",
            "        -4.1544e-02,  3.3784e-02, -2.0124e-02,  2.3379e-02, -4.8836e-03,\n",
            "         2.7771e-02,  8.5119e-03, -2.6334e-02,  1.9451e-02, -1.9259e-02,\n",
            "        -1.4547e-02, -1.5886e-02, -2.0019e-02, -1.4513e-02,  2.0345e-02,\n",
            "        -1.0771e-03, -7.3814e-03, -7.6110e-03, -1.4858e-02,  1.3192e-02,\n",
            "         3.1926e-03,  9.7025e-03,  2.0772e-02,  2.0624e-02, -2.1204e-02,\n",
            "         2.0259e-02,  2.3221e-02, -1.3380e-02,  5.2961e-03, -4.5727e-02,\n",
            "        -3.2335e-02,  1.2752e-02, -2.1655e-02,  8.4878e-03, -1.0461e-02,\n",
            "        -8.1323e-03, -2.2254e-03,  2.3296e-02, -3.1245e-02, -1.5255e-02,\n",
            "        -1.3938e-03, -2.1520e-03,  2.5842e-02,  7.0877e-03, -2.5437e-02,\n",
            "         1.9478e-02,  2.1854e-02,  2.1544e-02,  1.5807e-02, -2.7663e-02,\n",
            "        -7.8710e-03, -1.0028e-02, -1.9198e-02, -1.4362e-02,  1.0178e-02,\n",
            "         2.2768e-02,  1.5823e-02,  9.4488e-03,  2.3018e-02,  2.4629e-02,\n",
            "         1.2725e-03, -8.8380e-03, -3.3185e-03,  2.0701e-02,  1.6012e-02,\n",
            "         4.0784e-02, -9.2876e-03,  1.8457e-02,  6.1047e-03, -2.6329e-02,\n",
            "        -3.0907e-02, -3.3820e-02,  2.7721e-02,  8.6545e-03,  3.4921e-02,\n",
            "         2.4615e-02, -3.5041e-02, -3.6652e-02, -8.0920e-03, -4.0076e-03,\n",
            "         1.1025e-02,  1.4719e-02, -1.0178e-02, -6.7462e-03,  6.1475e-03,\n",
            "         1.0690e-02,  3.3948e-02,  4.0691e-03,  1.5484e-02,  1.4852e-02,\n",
            "        -4.8426e-02, -1.2340e-02, -3.9973e-02, -5.8563e-02, -5.6882e-04,\n",
            "        -2.8260e-02, -4.1899e-02,  4.3900e-02, -2.2101e-02, -5.5254e-02,\n",
            "         2.7403e-02, -1.2817e-02, -6.1388e-02,  3.1872e-02,  1.9924e-02,\n",
            "         1.1601e-01, -4.9726e-02,  1.6305e-02,  8.3564e-03, -8.9821e-02,\n",
            "         3.2523e-02,  5.5320e-04,  6.9919e-03, -4.2199e-02,  5.1612e-02,\n",
            "        -6.2525e-02,  8.1582e-02,  7.6495e-02, -2.6047e-02,  1.1794e-02,\n",
            "        -1.0404e-01, -2.7543e-02,  1.1203e-02,  1.6688e-02,  6.5550e-03,\n",
            "        -7.1559e-02, -3.7918e-02,  3.8551e-02, -1.3580e-03,  1.0540e-01,\n",
            "         5.0308e-02,  5.0287e-02, -1.7629e-01, -8.1310e-03, -8.8743e-02,\n",
            "         2.5511e-03,  4.9779e-02, -5.7165e-03, -9.5855e-02,  5.7614e-02,\n",
            "         5.0306e-03,  1.1981e-01, -1.1059e-01, -3.1255e-02, -3.6411e-02,\n",
            "         5.1931e-02,  7.2463e-02,  1.0570e-02, -6.8177e-02, -6.1226e-03,\n",
            "        -5.7269e-02, -7.7891e-03,  9.8154e-03, -1.2198e-01,  1.5314e-02,\n",
            "        -3.7611e-02, -3.1364e-03, -3.5201e-02, -3.5163e-03, -1.3749e-02,\n",
            "        -1.3749e-02,  8.1568e-03,  5.4526e-03,  1.1254e-02,  1.9219e-02,\n",
            "        -1.9473e-02,  5.6050e-04, -2.3288e-02, -2.0189e-02,  4.6020e-03,\n",
            "        -3.5094e-02,  1.6741e-03,  1.8203e-02,  1.8319e-02,  3.8532e-02,\n",
            "         4.7514e-02, -3.6065e-02,  3.4001e-02,  2.3714e-02,  1.3223e-02,\n",
            "        -3.5889e-02, -1.6353e-02,  1.4993e-02,  4.1764e-02, -9.2005e-03,\n",
            "        -3.2008e-03, -3.6946e-02,  3.1888e-03,  2.0773e-02,  2.4452e-02,\n",
            "        -1.3674e-02,  1.9615e-02, -3.4809e-02,  9.5632e-03, -1.3561e-02,\n",
            "         3.9977e-02,  2.4728e-02,  2.9580e-02, -2.1571e-02,  6.2634e-03,\n",
            "        -1.5969e-02, -1.0577e-02,  2.2131e-02,  2.7542e-02, -1.2137e-02,\n",
            "        -2.9295e-02,  1.8271e-02, -4.6944e-03,  1.0230e-02, -2.9445e-03,\n",
            "        -3.8199e-02, -1.8466e-02, -2.4100e-03,  5.4344e-03,  3.4232e-03,\n",
            "        -8.1761e-03, -2.2172e-02, -3.5269e-02, -1.1428e-02,  5.6232e-02,\n",
            "         4.9590e-02,  5.0683e-03,  4.1592e-03, -8.2871e-03,  3.3098e-03,\n",
            "         7.8449e-03, -1.6449e-02,  1.7421e-02,  2.0385e-02,  2.4047e-02,\n",
            "        -1.7413e-02,  1.5921e-02,  1.8624e-02,  1.9648e-02, -1.3769e-02,\n",
            "         9.9861e-03, -1.0944e-03, -1.6762e-02, -1.1414e-02, -1.5386e-02,\n",
            "        -1.4050e-02,  8.4930e-03,  1.6846e-03,  8.2454e-03, -2.8057e-04,\n",
            "         1.1073e-02, -2.7322e-03, -2.1658e-02, -9.7672e-04,  2.0676e-02,\n",
            "        -9.0476e-03, -1.2272e-02, -1.6939e-02,  1.8321e-05,  5.2752e-02,\n",
            "         1.6281e-02,  2.2557e-03, -4.6928e-03, -2.8436e-02,  2.1427e-02,\n",
            "        -1.8507e-02, -2.5014e-02,  4.9651e-03, -2.3305e-03, -4.3125e-02,\n",
            "        -8.9003e-03, -1.2526e-02, -2.4863e-03, -2.3802e-02,  1.2116e-02,\n",
            "         3.8443e-03,  2.6368e-03,  3.0709e-02,  8.5294e-03,  1.0057e-02,\n",
            "         2.2089e-04, -1.1063e-02, -4.5478e-03,  3.7331e-02, -2.5968e-02,\n",
            "         2.4441e-02, -1.6876e-02,  6.4540e-03,  4.7686e-03, -1.1398e-02,\n",
            "        -5.1747e-03, -8.1591e-02,  2.9000e-02,  1.2851e-02, -1.3771e-02,\n",
            "        -6.8794e-02,  3.0193e-02, -3.7804e-03,  1.9550e-02, -2.5658e-02,\n",
            "        -9.3446e-03, -2.8750e-02, -3.0803e-02,  4.4550e-03,  3.7309e-02,\n",
            "         2.2961e-02,  1.8816e-02,  5.6224e-02,  1.3746e-02,  4.4757e-02,\n",
            "         2.5148e-04, -9.0258e-03, -1.5699e-02,  1.6904e-02,  6.5660e-02,\n",
            "        -8.8467e-04,  6.6148e-03, -1.4283e-03, -2.8584e-02,  3.7046e-02,\n",
            "         3.5598e-02,  5.6849e-02,  2.2077e-03,  3.4387e-02, -4.3467e-02,\n",
            "        -4.4730e-02,  4.6105e-02,  6.6142e-02,  6.1135e-02, -6.0494e-03,\n",
            "        -3.5220e-02,  8.7637e-02,  2.0479e-02,  1.7422e-02,  4.5843e-03,\n",
            "        -2.5563e-02, -3.3276e-03, -1.2298e-02,  5.6015e-02, -2.1853e-02,\n",
            "         4.4572e-02, -7.2469e-02, -8.2292e-03,  3.1514e-02, -1.7087e-02,\n",
            "        -1.6349e-02, -4.8072e-02, -1.2006e-02,  1.4473e-02,  5.1638e-02,\n",
            "        -7.7877e-02, -2.3963e-01, -3.1920e-02, -1.4294e-02,  4.1624e-02,\n",
            "        -2.2896e-02, -2.6133e-02,  7.2512e-03,  2.4202e-02, -3.6245e-02,\n",
            "         2.2548e-02,  2.8968e-02, -4.5986e-02,  3.4617e-02, -9.8823e-03,\n",
            "        -1.9043e-02,  2.2954e-02,  7.6192e-02,  1.6216e-02, -1.8726e-02,\n",
            "        -1.4333e-02,  1.0262e-02, -2.0601e-02,  6.0842e-02, -8.2446e-03,\n",
            "        -7.1374e-02, -1.1112e-02,  1.9850e-02,  3.8210e-02, -4.3007e-02,\n",
            "         1.8476e-02, -7.7458e-03, -9.5591e-03,  1.6103e-02,  4.8699e-02,\n",
            "        -7.6612e-02, -1.7848e-02,  2.1309e-02,  2.7335e-02, -5.1940e-02,\n",
            "        -2.8181e-03, -4.2079e-03,  4.7544e-02,  1.3033e-02, -1.6478e-02,\n",
            "         1.0937e-02, -5.7132e-03,  3.8025e-02, -4.5309e-03,  1.9538e-02,\n",
            "         4.0640e-02,  4.3414e-02,  2.8984e-03,  2.3042e-02, -2.1066e-02,\n",
            "         1.9655e-02,  3.3952e-02, -5.4006e-03,  1.1982e-02, -1.2347e-02,\n",
            "         4.9834e-02,  3.4514e-02, -1.2949e-02, -4.8296e-02, -7.7759e-03,\n",
            "         1.6666e-02, -3.2815e-03, -1.6735e-02,  1.8327e-02, -6.0400e-04,\n",
            "         3.4366e-02, -2.1058e-02,  5.3747e-02,  9.4805e-02, -3.0550e-03,\n",
            "         6.0545e-03, -2.4723e-03,  1.9174e-02, -8.1860e-02, -6.5610e-03,\n",
            "         3.5513e-02, -1.0977e-03,  2.6217e-02,  3.0029e-02,  6.7290e-03,\n",
            "        -1.6525e-02, -8.8000e-02, -1.5388e-01,  2.0500e-02, -1.2735e-02,\n",
            "         1.9180e-02,  7.8665e-03, -6.7336e-03,  1.0310e-02,  2.9901e-02,\n",
            "         2.3626e-02, -4.5601e-02, -1.4132e-02,  8.7256e-02,  1.8397e-02,\n",
            "         7.0418e-03, -1.8900e-03,  1.1786e-01, -9.0956e-03, -2.5941e-02,\n",
            "         3.8710e-02, -7.2845e-03, -2.0759e-03, -2.8251e-02, -6.1092e-03,\n",
            "         1.9707e-02,  2.5508e-03,  2.3336e-02, -5.2097e-03,  2.2738e-02,\n",
            "         1.8183e-02,  2.4673e-02, -9.7911e-03,  2.1014e-02, -1.0311e-02,\n",
            "         3.5164e-02, -6.2241e-03, -1.4077e-01,  2.1790e-02,  4.2409e-03,\n",
            "         1.4800e-02,  2.2960e-03,  2.0558e-02,  1.0431e-02, -1.2270e-02,\n",
            "         4.3609e-02,  2.0709e-02, -6.9609e-02, -8.1132e-02, -8.4682e-02,\n",
            "        -2.9434e-02, -1.4355e-01, -1.5126e-02,  2.9518e-02, -1.1384e-01,\n",
            "        -1.6683e-02, -9.8539e-02, -2.2149e-02,  5.1461e-02,  5.8523e-02,\n",
            "        -1.5309e-01, -1.4919e-02,  3.5592e-03, -5.1674e-02,  4.2507e-02,\n",
            "         9.7244e-02,  9.7473e-03,  2.8738e-02, -8.9230e-03, -9.8830e-02,\n",
            "         7.7710e-03,  3.5607e-02,  8.0602e-02,  7.1595e-02,  2.9805e-02,\n",
            "        -4.4460e-02,  2.5833e-02,  2.6351e-02,  4.1435e-03,  2.5592e-01,\n",
            "         3.1922e-02, -5.3518e-02,  3.4014e-02, -2.1606e-01, -6.1330e-02,\n",
            "        -5.0663e-02, -1.1274e-02, -4.2331e-03, -1.0860e-02,  4.3373e-02,\n",
            "        -8.3659e-02, -5.0398e-02, -2.0691e-02, -1.3180e-02, -2.6989e-02,\n",
            "         6.2227e-02,  5.1282e-02,  7.7988e-02, -1.6864e-02, -6.7923e-02,\n",
            "        -7.8919e-02, -1.7750e-02,  2.7830e-02,  3.9274e-02,  2.2867e-02,\n",
            "         2.1030e-02, -2.2316e-02, -4.6299e-03, -1.1694e-02, -3.2891e-02,\n",
            "        -2.8493e-02,  8.4015e-03,  2.3036e-02, -3.0386e-03, -4.8378e-02,\n",
            "         9.9557e-03,  9.6439e-04,  5.3331e-02, -3.7254e-03, -2.2312e-02,\n",
            "         4.0513e-03, -2.5879e-02, -9.1014e-03, -2.0190e-02, -1.2775e-02,\n",
            "         1.9315e-02,  1.0687e-02,  1.5014e-02,  3.6512e-02, -1.1713e-02,\n",
            "        -5.2126e-02, -8.6976e-04, -1.0995e-02, -5.7366e-02,  2.6705e-02,\n",
            "        -4.5372e-03,  1.5001e-02,  3.8321e-02, -3.5469e-02, -1.6535e-02,\n",
            "        -6.0002e-02,  4.1784e-02, -6.3892e-03,  3.7532e-02,  1.6330e-02,\n",
            "        -3.4147e-02, -1.1365e-02,  2.4485e-02,  4.9513e-02, -1.5101e-02,\n",
            "         3.4222e-02, -1.3525e-02,  1.0752e-02, -1.0896e-02, -4.4762e-02,\n",
            "        -3.6113e-02,  7.9229e-03,  5.5873e-02, -2.0338e-03,  2.5597e-02,\n",
            "        -5.1385e-02,  8.9843e-03,  6.2625e-02, -1.2141e-03, -2.1407e-02,\n",
            "         2.7776e-03, -3.4962e-03, -2.0886e-02, -2.2423e-02,  9.7180e-03,\n",
            "         6.3014e-03, -1.8907e-02, -1.0427e-02,  1.1548e-02, -8.8326e-04,\n",
            "         1.3924e-03, -5.0359e-03,  2.1305e-02, -2.1989e-02,  2.1543e-02,\n",
            "        -2.3430e-03, -9.7738e-04,  1.7061e-02, -5.0949e-03, -1.4141e-02,\n",
            "        -3.0202e-03,  1.6349e-02,  1.5002e-02, -3.1997e-02,  1.3311e-02,\n",
            "         1.2690e-01, -4.4591e-02, -3.4404e-03,  6.2991e-02,  4.5215e-03,\n",
            "         1.5560e-02,  3.1768e-02, -1.5103e-02, -2.0088e-02,  1.6693e-02,\n",
            "        -1.3721e-02, -2.8177e-02, -9.0486e-03,  1.8221e-02,  3.9827e-02,\n",
            "        -5.5550e-02,  1.7972e-02, -1.0785e-02, -1.7799e-02, -2.9102e-03,\n",
            "        -7.6985e-03, -2.3899e-02,  1.0650e-02, -1.9104e-03, -1.5920e-02,\n",
            "         2.2434e-02,  1.6145e-02,  1.9242e-02, -4.7098e-03,  1.7613e-02,\n",
            "         9.6937e-03, -8.3046e-02,  1.3067e-02, -2.3601e-03,  2.8663e-02,\n",
            "         1.4150e-02, -5.6467e-03,  9.9477e-03, -1.0502e-02,  5.4545e-03,\n",
            "         1.1272e-02, -1.5929e-02, -2.7044e-02, -1.0705e-02,  1.9881e-02,\n",
            "        -1.0167e-02, -1.6105e-02, -1.9589e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.9.attention.output.dense.weight', Parameter containing:\n",
            "tensor([[-0.0126,  0.0292, -0.0079,  ...,  0.0294,  0.0179, -0.0117],\n",
            "        [ 0.0087, -0.0103, -0.0290,  ..., -0.0260, -0.0068, -0.0239],\n",
            "        [ 0.0048, -0.0441, -0.0299,  ...,  0.0441,  0.0336,  0.0135],\n",
            "        ...,\n",
            "        [-0.0101, -0.0307,  0.0436,  ..., -0.0460, -0.0108, -0.0138],\n",
            "        [-0.0026, -0.0193,  0.0014,  ..., -0.0260, -0.0113,  0.0132],\n",
            "        [ 0.0197, -0.0465,  0.0028,  ...,  0.0129, -0.0087, -0.0090]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.9.attention.output.dense.bias', Parameter containing:\n",
            "tensor([ 2.4578e-02,  5.0511e-02, -6.8908e-02, -9.6279e-03,  8.6479e-03,\n",
            "        -5.1858e-02, -3.5535e-02, -6.3147e-02,  2.5675e-02, -3.3494e-02,\n",
            "         1.9736e-02, -1.8779e-02,  5.7686e-02, -4.9109e-02, -5.7644e-02,\n",
            "        -1.1658e-02, -2.9885e-02,  9.4720e-04, -6.5287e-02,  4.3729e-02,\n",
            "        -6.6305e-02,  1.0682e-02, -4.5261e-02,  1.1219e-02,  3.4694e-02,\n",
            "        -3.7734e-02, -1.1385e-01, -6.8868e-02,  1.8240e-01, -2.6260e-02,\n",
            "        -3.0932e-03,  3.0622e-02, -6.5912e-02, -7.9868e-03,  1.6600e-02,\n",
            "         3.8526e-02,  2.8402e-02,  1.8586e-03,  1.9479e-02,  6.6594e-02,\n",
            "        -1.7092e-02, -1.3248e-02,  9.3206e-02,  1.0075e-02,  4.0507e-02,\n",
            "        -3.5256e-02,  2.5725e-02, -4.9262e-02,  6.7931e-02,  2.0545e-02,\n",
            "         1.0578e-01, -7.0147e-04, -1.0848e-01,  5.9003e-02,  1.6285e-02,\n",
            "        -3.5881e-02,  5.5070e-02, -7.4909e-02, -5.5673e-02,  3.0501e-02,\n",
            "        -7.1883e-02,  6.4878e-02, -7.8643e-02, -3.2429e-02,  1.9860e-02,\n",
            "        -1.1668e-02, -3.9158e-02,  5.0506e-02, -3.9235e-02,  2.1725e-02,\n",
            "         7.0619e-02, -2.0162e-02, -4.1833e-02,  1.1306e-01, -8.4074e-02,\n",
            "         8.9877e-02, -6.3786e-02, -6.1886e-02, -2.8581e-02, -8.9214e-02,\n",
            "        -7.7227e-02,  1.4581e-02, -2.3004e-03, -3.1740e-02,  3.7133e-02,\n",
            "         9.5641e-02, -8.5445e-02,  1.0434e-01,  1.5207e-02, -4.5802e-02,\n",
            "         7.7261e-02, -2.9115e-02, -1.5408e-01, -2.1505e-02, -4.7861e-03,\n",
            "         4.7817e-02,  5.6556e-03, -2.2699e-03, -4.3931e-02, -1.7476e-02,\n",
            "         7.5618e-02, -4.1800e-02,  2.7486e-02,  7.7542e-02,  9.5427e-02,\n",
            "        -3.0486e-02, -1.1847e-01,  2.1911e-02,  4.1940e-02,  5.1217e-02,\n",
            "        -3.6692e-02,  1.1423e-02, -7.3405e-03,  4.1624e-02,  6.0775e-02,\n",
            "        -5.8090e-02, -3.6631e-02, -7.1071e-03, -5.3239e-02,  2.9959e-02,\n",
            "        -5.8497e-02, -5.7950e-02,  2.1623e-02, -1.1494e-01, -3.0187e-02,\n",
            "         4.5352e-02, -1.5585e-02, -9.3303e-02,  3.1678e-03, -2.1818e-02,\n",
            "         1.0442e-02, -1.1039e-01, -5.8945e-02,  2.9058e-02, -3.8107e-02,\n",
            "        -3.5145e-02, -2.2938e-02, -9.2678e-02, -2.7075e-02,  5.3416e-02,\n",
            "         1.2700e-01, -1.4081e-02, -2.8807e-02,  2.4830e-02, -2.1511e-02,\n",
            "         2.3583e-02,  6.0056e-02,  8.2511e-02, -7.0251e-02, -2.4983e-02,\n",
            "         2.8887e-02, -2.5124e-02,  1.3311e-02, -2.0021e-02,  3.1304e-02,\n",
            "        -8.5807e-02, -5.7669e-02,  2.4945e-02, -1.6177e-02, -4.9407e-02,\n",
            "        -3.5859e-02,  1.6217e-02,  9.0022e-02,  2.2880e-02,  4.6763e-02,\n",
            "         4.4419e-02, -6.0377e-02, -4.7926e-02, -1.2443e-01,  3.6229e-03,\n",
            "         1.8132e-02,  6.6452e-02,  1.4037e-02, -1.6331e-02, -1.1329e-02,\n",
            "         8.4571e-02, -4.4872e-02, -7.4273e-02, -3.4837e-02,  2.8140e-02,\n",
            "         1.4246e-01, -2.9606e-02, -6.7629e-02, -1.7606e-02, -2.4591e-02,\n",
            "         2.4645e-02, -8.9900e-02, -7.0505e-03, -6.9583e-03,  1.2739e-03,\n",
            "         7.1222e-02,  1.7658e-02, -1.0881e-01,  1.6503e-02,  5.7598e-02,\n",
            "        -3.3600e-02,  2.9336e-02, -4.1844e-02, -3.1206e-02, -7.5020e-02,\n",
            "         3.0974e-02, -9.4930e-02, -5.1098e-03,  7.0990e-02,  8.3939e-02,\n",
            "         3.6935e-03,  2.7375e-02, -2.5401e-02, -5.8768e-02,  5.4717e-02,\n",
            "         3.7554e-02,  1.2396e-01, -7.8662e-02, -1.2683e-02, -8.7514e-02,\n",
            "         8.0383e-02,  3.2949e-02,  5.5452e-02,  2.6038e-02,  6.7109e-02,\n",
            "         7.8567e-03, -4.8984e-02, -7.6618e-02,  6.6398e-02,  4.7900e-02,\n",
            "        -8.3999e-02,  2.8516e-02,  4.5123e-02, -2.7835e-02,  6.5076e-03,\n",
            "         4.8228e-02, -4.1790e-03, -9.3680e-02,  3.6670e-02, -3.2465e-02,\n",
            "         5.5496e-02, -4.6320e-02, -6.6367e-03, -3.2869e-02,  3.6526e-02,\n",
            "        -5.0594e-03, -5.8102e-02,  1.5682e-01, -1.0111e-01,  7.9125e-02,\n",
            "         6.1713e-02, -5.3731e-02, -2.4562e-02,  7.7775e-03, -2.1732e-02,\n",
            "         2.6613e-02, -1.6021e-01,  3.9763e-03, -7.7463e-02, -8.0879e-02,\n",
            "         8.2168e-02,  4.6569e-02,  7.8040e-02, -3.3746e-02, -5.6778e-02,\n",
            "        -1.1840e-02, -7.3115e-02,  1.7688e-02,  3.5143e-02, -1.1684e-01,\n",
            "         6.5141e-02,  6.4736e-02, -3.8686e-02, -5.7529e-02, -4.7193e-02,\n",
            "        -7.7642e-02, -1.7592e-02, -4.1758e-02,  9.3316e-03, -1.2701e-02,\n",
            "        -5.4243e-03,  3.3596e-02,  2.6004e-02, -2.5457e-02, -6.0741e-02,\n",
            "         2.4781e-02, -7.5852e-02, -3.3704e-02,  1.4708e-01,  3.0936e-02,\n",
            "         4.5588e-02, -9.2661e-02,  6.7290e-02,  3.7958e-02, -7.1119e-02,\n",
            "        -3.7467e-02, -7.3489e-02,  6.4114e-02,  3.0491e-03,  8.7380e-04,\n",
            "        -1.7012e-02,  4.6758e-02, -1.2593e-02,  4.0421e-03, -4.1298e-02,\n",
            "        -1.7619e-02, -7.4921e-03,  5.7175e-02, -6.2310e-03, -1.6212e-01,\n",
            "         8.5179e-02,  4.7564e-02, -1.0077e-03,  6.4548e-01, -4.3875e-02,\n",
            "         1.1579e-02, -3.6563e-02, -6.6649e-03, -5.3211e-02, -2.1227e-02,\n",
            "        -2.5636e-02, -3.7652e-02, -7.9245e-02,  9.9556e-02, -9.0043e-02,\n",
            "         3.8389e-02,  4.2622e-03,  5.6319e-02,  4.1060e-02, -2.2288e-02,\n",
            "         6.3179e-02, -1.4952e-01, -1.1860e-03,  1.7443e-02, -6.3258e-02,\n",
            "         9.9497e-02, -5.0709e-02,  7.6648e-02, -8.4598e-02,  1.5326e-01,\n",
            "         5.1550e-02,  8.7317e-02, -1.5878e-02, -8.2778e-02, -6.2912e-02,\n",
            "        -3.0271e-02,  3.0919e-02, -1.1277e-01, -2.4009e-02, -2.4440e-03,\n",
            "        -3.4408e-03, -3.2376e-02, -7.9006e-03,  3.3287e-02, -6.1146e-03,\n",
            "         6.2806e-02,  3.9894e-02, -3.6993e-02,  2.7163e-02, -1.5087e-02,\n",
            "         4.1039e-03, -1.4492e-02,  1.9741e-04,  7.0495e-02, -3.5320e-02,\n",
            "        -6.1017e-02,  1.1243e-02, -4.0216e-02,  4.2701e-02, -3.5735e-02,\n",
            "         1.7713e-02,  6.8362e-02, -2.0039e-02, -5.8745e-02,  4.6663e-03,\n",
            "         7.4593e-02, -3.8464e-02,  3.2271e-02,  2.4718e-02,  4.6618e-03,\n",
            "        -3.9111e-04, -4.8578e-03,  2.3779e-03,  7.5503e-02, -6.3449e-02,\n",
            "        -3.5555e-02,  2.2189e-01,  9.9143e-02,  7.7582e-02, -2.8714e-02,\n",
            "         3.4316e-02,  8.5470e-02,  5.9932e-02,  7.9361e-02,  3.5441e-03,\n",
            "         7.2703e-03,  4.9419e-04, -4.4187e-02,  7.3914e-02,  2.6357e-02,\n",
            "        -8.6495e-02,  1.3624e-01,  2.4265e-02, -1.5150e-02,  3.7040e-02,\n",
            "        -4.8415e-02,  5.8911e-02, -1.3657e-01, -3.9960e-02, -5.4069e-04,\n",
            "         6.2952e-02,  2.6462e-02, -3.1714e-02,  7.8920e-03,  5.2236e-02,\n",
            "         3.8680e-02,  3.2994e-02,  4.0667e-02,  6.7528e-05, -6.6856e-04,\n",
            "         5.4397e-02, -7.8115e-02, -5.2359e-02,  4.3619e-02,  1.0659e-03,\n",
            "        -8.6306e-04,  5.5881e-02, -3.5918e-02,  1.1723e-01, -1.3541e-02,\n",
            "         9.0333e-04,  8.4520e-03,  2.0957e-02,  4.1977e-02, -1.2166e-03,\n",
            "        -6.4597e-02,  2.3944e-02,  5.6483e-02, -1.1565e-01,  4.0474e-02,\n",
            "         2.4641e-02, -5.3615e-02,  4.3786e-02, -2.7870e-02, -2.2432e-02,\n",
            "        -4.0537e-02, -2.0788e-03, -4.3421e-02,  2.3186e-03,  2.0491e-02,\n",
            "         3.5279e-02,  3.3340e-02,  1.0701e-01, -8.4434e-03,  3.0521e-02,\n",
            "        -7.4863e-02,  1.3370e-02, -6.6574e-02, -1.2657e-02,  1.9653e-02,\n",
            "        -4.3188e-02,  4.3777e-02,  2.3486e-02,  6.2949e-02, -7.7528e-03,\n",
            "        -7.4373e-02,  7.4555e-02,  4.7409e-02, -6.5177e-02, -2.9883e-02,\n",
            "        -8.6657e-02, -1.0243e-01, -1.8153e-02,  2.6155e-02,  1.4144e-01,\n",
            "         2.7440e-02,  6.9396e-02, -5.2212e-03, -4.2343e-02,  1.2134e-01,\n",
            "         2.4320e-02, -9.4093e-03,  8.0996e-03,  4.7630e-04, -3.9164e-02,\n",
            "         3.1741e-02,  1.0721e-01,  7.1610e-03,  3.7497e-02,  3.0498e-02,\n",
            "        -2.4046e-02, -5.5028e-02,  5.7012e-02, -3.7875e-02, -9.9769e-02,\n",
            "         5.5580e-03, -4.1575e-02,  1.6190e-02,  1.4083e-03,  9.5304e-02,\n",
            "        -1.5768e-03,  1.1316e-01, -4.0348e-02, -1.0600e-01,  1.6626e-02,\n",
            "         7.0695e-03, -4.7526e-02,  1.0149e-02,  8.3472e-02,  1.1190e-01,\n",
            "        -4.0908e-02,  4.6374e-02,  8.0253e-02,  1.1143e-01, -4.5841e-02,\n",
            "         1.3492e-01, -5.7379e-02,  2.7314e-02,  1.5332e-02,  1.9288e-02,\n",
            "        -4.4661e-02,  5.0577e-02,  4.3391e-03,  4.6164e-02,  9.1339e-02,\n",
            "        -7.0876e-02,  1.4182e-02,  6.9297e-02,  2.5107e-02,  1.1028e-02,\n",
            "         1.1613e-02, -2.3474e-02,  5.2265e-02, -8.3276e-02, -5.3834e-02,\n",
            "         2.0681e-02, -4.4166e-02, -7.8381e-02, -1.0387e-01,  6.1494e-02,\n",
            "        -1.0916e-03, -1.0033e-01, -5.9673e-03, -1.2240e-01,  7.2777e-02,\n",
            "        -1.3664e-03, -2.1094e-02,  5.3261e-02,  3.9425e-02,  7.4221e-02,\n",
            "        -1.0072e-01, -5.2677e-03,  7.9972e-03, -1.0101e-01,  9.7247e-03,\n",
            "         5.5794e-02,  5.6504e-03,  6.9240e-02, -1.9926e-02,  2.8815e-02,\n",
            "         2.9216e-03,  3.9610e-02, -3.8277e-02, -6.3773e-04,  4.7707e-02,\n",
            "         2.9635e-02, -1.7754e-02, -1.4412e-02,  2.1079e-02, -6.5434e-02,\n",
            "         1.3963e-02, -3.9299e-02, -9.8476e-02, -3.1367e-02,  6.4941e-02,\n",
            "         4.9521e-02, -3.4118e-02, -1.1359e-01,  6.3753e-02,  1.1327e-01,\n",
            "         9.6434e-02, -2.0002e-02,  1.3592e-01, -5.7111e-02,  5.4583e-02,\n",
            "         4.1030e-02, -3.1982e-02, -1.4250e-01,  2.9209e-03,  2.8458e-02,\n",
            "        -6.5953e-02, -8.2555e-02,  3.8716e-02,  4.6521e-02, -1.3202e-01,\n",
            "        -2.0601e-02, -3.0716e-02, -7.8835e-03,  1.1429e-01,  1.9816e-02,\n",
            "        -3.5246e-02,  4.1230e-03, -1.2560e-02, -2.3459e-02,  3.2273e-02,\n",
            "        -1.0036e-01,  2.8796e-02, -9.8177e-03, -3.3467e-02, -1.5711e-02,\n",
            "        -2.2253e-02,  5.2515e-02,  5.8488e-02,  2.6783e-02, -8.9727e-02,\n",
            "         2.2690e-03,  3.5415e-02,  9.4395e-02,  1.0781e-01,  7.2728e-02,\n",
            "         2.0170e-02,  6.9750e-02,  8.3771e-02, -2.3631e-02,  3.4260e-02,\n",
            "        -1.1197e-01, -1.6004e-02,  1.0702e-03,  1.0086e-02,  5.8172e-02,\n",
            "        -3.3162e-02, -7.7963e-02,  4.7717e-02, -8.1169e-02,  4.0368e-02,\n",
            "        -8.1358e-02,  5.6003e-02, -1.2977e-02, -2.1311e-02, -1.3564e-02,\n",
            "         2.3265e-03,  4.1486e-02, -6.4469e-03, -7.5590e-02, -2.8563e-02,\n",
            "        -2.4962e-02, -2.4490e-02,  9.6827e-02,  5.2611e-02, -8.8839e-04,\n",
            "         1.2166e-01,  1.0377e-02,  1.8699e-02,  3.7634e-02, -4.4151e-02,\n",
            "         5.9885e-03, -4.9503e-02,  5.0008e-03,  7.5156e-03, -1.3069e-02,\n",
            "         8.4939e-02, -1.4025e-02,  2.3019e-02, -2.3995e-02,  3.9396e-02,\n",
            "        -1.8565e-02, -6.2489e-02, -8.8125e-02, -2.5595e-03, -3.5875e-02,\n",
            "        -6.1910e-03,  4.1591e-02,  8.2742e-04, -7.3422e-02, -1.0874e-01,\n",
            "        -9.8309e-02, -7.3192e-02,  1.9531e-02, -3.2941e-02, -2.6022e-02,\n",
            "         6.1804e-02, -6.8107e-02, -4.1176e-02,  4.6871e-02, -1.7228e-02,\n",
            "         4.4314e-03,  4.1636e-02, -8.0972e-02,  2.5002e-02, -2.5467e-02,\n",
            "        -1.2325e-02, -2.2053e-02, -6.6179e-02,  7.7994e-03,  3.9427e-02,\n",
            "         2.0380e-02,  4.1481e-02,  5.1393e-02,  2.2905e-02,  8.8838e-04,\n",
            "        -5.3705e-02, -1.8338e-03, -4.4247e-03,  2.4446e-02, -1.2022e-02,\n",
            "        -4.6568e-02, -8.5222e-03,  8.3215e-02, -3.6686e-04, -1.0893e-01,\n",
            "        -6.4653e-03,  8.3765e-02,  1.0889e-02, -5.4431e-02, -5.5643e-02,\n",
            "         2.9851e-02,  3.9627e-02,  1.9525e-02,  7.9224e-02,  4.2602e-02,\n",
            "        -2.3108e-02,  1.1474e-01, -3.2830e-02, -3.6787e-02, -1.9404e-02,\n",
            "        -5.8872e-02, -4.3331e-02, -4.9852e-02,  9.2552e-02, -4.5496e-02,\n",
            "         1.9915e-02,  8.1433e-02,  5.4503e-02,  4.1145e-02,  2.2515e-02,\n",
            "        -5.7644e-02, -3.6328e-02,  2.2937e-02, -4.2606e-02, -3.3826e-02,\n",
            "         3.5877e-02,  3.2692e-02, -1.2586e-01,  3.9427e-02, -6.6019e-02,\n",
            "         1.0822e-02, -3.5595e-02,  4.2867e-02,  8.3187e-02, -8.4326e-02,\n",
            "        -5.4280e-02, -8.8386e-02, -4.2226e-02,  2.4748e-02, -2.8106e-02,\n",
            "        -2.1720e-02,  1.6657e-03,  1.0468e-01, -6.3366e-02, -8.8841e-03,\n",
            "         4.8828e-02,  9.8149e-02, -3.5133e-02,  4.3391e-02,  5.0050e-02,\n",
            "        -3.2300e-02,  2.3210e-02, -8.7485e-02,  6.4873e-03,  1.1352e-01,\n",
            "        -6.6110e-02, -6.1094e-02, -6.6483e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.9.attention.output.LayerNorm.weight', Parameter containing:\n",
            "tensor([0.8510, 0.8257, 0.7928, 0.7660, 0.8913, 0.8281, 0.8445, 0.8589, 0.8335,\n",
            "        0.8318, 0.7619, 0.7584, 0.7800, 0.7981, 0.8150, 0.8331, 0.8130, 0.7681,\n",
            "        0.8971, 0.8065, 0.8500, 0.7828, 0.8105, 0.8420, 0.7675, 0.8296, 0.7695,\n",
            "        0.8437, 0.7941, 0.7906, 0.8720, 0.7707, 0.8293, 0.8442, 0.8368, 0.8053,\n",
            "        0.8500, 0.8071, 0.8720, 0.8020, 0.8177, 0.8587, 0.8598, 0.8547, 0.7876,\n",
            "        0.7641, 0.8262, 0.8296, 0.8399, 0.8436, 0.8437, 0.8228, 0.8427, 0.7761,\n",
            "        0.7994, 0.8011, 0.7898, 0.7825, 0.7900, 0.7748, 0.8243, 0.7830, 0.7415,\n",
            "        0.8101, 0.8179, 0.8152, 0.8171, 0.8370, 0.8315, 0.8333, 0.8294, 0.8526,\n",
            "        0.8107, 0.7847, 0.8414, 0.7678, 0.8014, 0.7884, 0.7709, 0.7842, 0.7995,\n",
            "        0.8541, 0.8626, 0.7935, 0.7421, 0.7844, 0.8210, 0.8014, 0.8193, 0.8523,\n",
            "        0.8044, 0.7955, 0.7774, 0.7874, 0.8264, 0.7873, 0.7921, 0.7779, 0.8087,\n",
            "        0.8809, 0.8706, 0.8710, 0.8334, 0.8159, 0.7753, 0.7403, 0.7870, 0.8629,\n",
            "        0.8031, 0.8052, 0.8132, 0.7908, 0.7825, 0.8076, 0.7931, 0.7665, 0.7964,\n",
            "        0.8168, 0.8174, 0.7986, 0.7972, 0.7374, 0.7908, 1.5736, 0.7438, 0.8114,\n",
            "        0.8175, 0.7729, 0.8241, 0.7774, 0.8176, 0.7696, 0.7618, 0.7936, 0.7794,\n",
            "        0.8420, 0.8064, 0.7897, 0.8098, 0.7232, 0.8446, 0.8544, 0.8349, 0.9250,\n",
            "        0.7950, 0.7515, 0.8202, 0.8211, 0.8191, 0.8058, 0.7866, 0.8170, 0.7736,\n",
            "        0.8333, 0.8139, 0.7869, 0.7910, 0.7895, 0.8322, 0.7710, 0.8215, 0.8464,\n",
            "        0.7851, 0.8645, 0.8320, 0.8428, 0.8414, 0.8195, 0.7995, 0.8295, 0.7840,\n",
            "        0.8225, 0.7822, 0.7974, 0.8703, 0.7734, 0.7586, 0.7917, 0.7643, 0.7986,\n",
            "        1.1452, 0.8621, 0.7994, 0.8423, 0.8737, 0.8176, 0.8991, 0.8092, 0.8371,\n",
            "        0.8004, 0.8279, 0.8419, 0.8627, 0.7956, 0.8486, 0.8536, 0.8331, 0.7776,\n",
            "        0.8259, 0.7628, 0.8196, 0.8029, 0.7470, 0.7574, 0.7778, 0.8815, 0.7860,\n",
            "        0.7648, 0.8167, 0.7715, 0.7660, 0.8203, 0.7959, 0.8472, 0.7968, 0.7602,\n",
            "        0.7818, 0.8744, 0.7962, 0.7917, 0.8197, 0.8365, 0.8365, 0.7572, 0.7966,\n",
            "        1.2110, 0.8171, 0.7959, 0.8326, 0.8835, 0.8224, 0.8375, 0.8334, 0.8147,\n",
            "        0.8302, 0.7990, 0.8172, 0.7980, 0.8131, 0.7872, 0.7525, 0.8322, 0.8247,\n",
            "        0.8071, 0.7800, 0.7475, 0.7993, 0.8181, 0.7804, 0.7601, 0.8367, 0.8880,\n",
            "        0.7911, 0.7993, 0.8491, 0.7950, 0.7475, 0.8268, 0.7655, 0.7931, 0.8672,\n",
            "        0.7960, 0.8240, 0.7914, 0.8019, 0.8155, 0.8145, 0.7911, 0.7961, 0.8460,\n",
            "        0.8038, 0.8091, 0.7963, 0.7739, 0.8235, 0.8404, 0.7353, 0.7868, 0.7713,\n",
            "        0.8257, 0.7760, 0.8650, 0.8396, 0.7547, 0.8330, 0.7613, 0.8619, 0.8795,\n",
            "        0.7843, 0.8174, 0.7860, 0.8342, 0.8418, 0.8442, 0.8352, 0.7675, 0.7637,\n",
            "        0.7934, 0.8304, 0.8057, 0.8740, 0.8356, 0.8312, 0.7938, 0.9066, 0.8161,\n",
            "        0.7882, 0.8436, 0.6058, 0.7900, 0.8079, 0.7783, 0.8306, 0.7776, 0.8318,\n",
            "        0.7522, 0.8328, 0.7851, 0.8060, 0.7278, 0.7904, 0.8447, 0.7845, 0.8577,\n",
            "        0.8094, 0.8080, 0.7544, 0.8361, 0.7872, 0.7987, 0.7649, 0.7356, 0.7803,\n",
            "        0.8844, 0.8541, 0.7947, 0.8072, 0.8383, 0.8543, 0.7873, 0.7751, 0.9416,\n",
            "        0.7676, 0.7911, 0.7594, 0.7603, 0.8573, 0.8636, 0.8009, 0.9210, 0.8014,\n",
            "        0.8343, 0.8710, 0.7355, 0.8167, 0.8286, 0.8040, 0.7774, 0.7924, 0.7905,\n",
            "        0.8173, 0.7930, 0.7894, 0.7766, 0.8055, 0.8762, 0.7797, 0.7965, 0.8487,\n",
            "        0.7506, 0.8332, 0.7838, 0.8682, 0.8132, 0.9272, 0.8566, 0.8289, 0.8118,\n",
            "        0.8008, 0.8701, 0.8313, 2.5250, 0.8217, 0.8654, 0.8540, 0.8199, 0.8196,\n",
            "        0.8356, 0.8108, 0.8126, 0.7997, 0.7916, 0.8042, 0.7916, 0.8182, 0.8213,\n",
            "        0.8095, 0.7978, 0.8538, 0.8172, 0.7845, 0.7694, 0.7891, 0.7885, 0.7536,\n",
            "        0.7715, 0.8031, 0.8020, 0.8340, 0.8361, 0.8003, 0.8129, 0.8456, 0.8083,\n",
            "        0.7897, 0.7797, 0.7836, 0.8082, 0.7998, 0.8000, 0.8122, 0.7390, 0.8369,\n",
            "        0.8250, 0.7879, 0.7934, 0.7731, 0.7736, 0.7948, 0.7981, 0.8248, 0.8291,\n",
            "        0.7916, 0.7704, 0.8154, 0.8876, 0.7966, 0.7762, 0.7982, 0.8266, 0.8194,\n",
            "        0.8384, 0.8074, 0.7418, 0.8020, 0.9121, 0.8985, 0.8177, 0.8144, 0.7862,\n",
            "        0.8073, 0.8366, 0.8987, 0.7970, 0.8768, 0.8282, 0.7598, 0.8120, 0.8513,\n",
            "        0.8381, 0.8151, 0.7820, 0.8012, 0.8504, 0.8743, 0.7752, 0.8129, 0.8677,\n",
            "        0.7503, 0.7722, 0.9080, 0.7513, 0.8448, 0.8314, 0.7648, 0.8037, 0.7693,\n",
            "        0.8952, 0.8161, 0.8136, 0.8226, 0.7862, 0.7451, 0.8143, 0.8198, 0.8029,\n",
            "        0.8108, 0.8793, 0.7652, 0.7910, 0.8355, 0.8227, 0.8080, 0.9047, 0.8200,\n",
            "        0.7819, 0.7874, 0.7532, 0.9005, 0.8115, 0.8480, 0.8377, 0.7895, 0.8139,\n",
            "        0.7743, 0.8517, 0.7912, 0.8046, 0.8307, 0.8291, 0.7865, 0.7627, 0.7951,\n",
            "        0.8157, 0.7955, 0.8474, 0.8280, 0.7328, 0.8517, 0.7856, 0.8500, 0.7728,\n",
            "        0.8499, 0.7929, 0.7896, 0.8106, 1.0271, 0.7854, 0.8490, 0.7472, 0.7939,\n",
            "        0.7746, 0.8279, 0.7994, 0.8574, 0.8042, 0.8021, 0.7965, 0.7551, 0.9254,\n",
            "        0.8000, 0.7857, 0.8344, 0.8255, 0.8428, 0.8358, 0.7970, 0.7681, 0.8200,\n",
            "        0.7992, 0.8324, 0.7910, 0.8328, 0.8405, 0.8329, 0.8147, 0.8015, 0.8139,\n",
            "        0.8428, 0.7922, 0.8250, 0.7718, 0.7867, 0.8736, 0.8075, 0.8417, 0.7727,\n",
            "        0.8572, 0.7758, 0.8489, 0.7720, 0.8072, 0.8162, 0.7464, 0.8102, 0.7856,\n",
            "        0.7961, 0.7762, 0.8932, 0.8287, 0.7932, 0.8443, 0.8429, 0.7838, 0.8267,\n",
            "        0.8288, 0.9089, 0.8545, 0.8156, 0.8476, 0.8470, 0.8044, 0.8675, 0.7878,\n",
            "        0.7617, 0.8100, 0.8019, 0.8035, 0.8220, 0.8211, 0.7597, 0.8634, 0.8445,\n",
            "        0.8229, 0.8500, 0.7857, 0.8870, 0.7757, 0.8299, 0.7883, 0.7862, 0.7882,\n",
            "        0.8250, 0.7542, 0.7470, 0.7905, 0.7932, 0.8666, 0.8015, 0.8408, 0.8210,\n",
            "        0.8140, 0.8361, 0.7913, 0.8215, 0.8108, 0.8295, 0.8067, 0.7671, 0.8479,\n",
            "        0.8202, 0.7591, 0.8208, 0.8396, 0.7866, 0.7910, 0.8478, 0.8291, 0.7592,\n",
            "        0.8228, 0.8095, 0.8147, 0.7912, 0.7899, 0.8279, 0.8742, 0.7988, 0.8531,\n",
            "        0.7806, 0.8327, 0.8789, 0.8030, 0.8163, 0.7886, 0.7544, 0.7862, 0.8216,\n",
            "        0.8141, 0.8043, 0.8063, 0.8437, 0.8568, 0.8603, 0.8814, 0.8053, 0.7855,\n",
            "        0.8483, 0.8329, 0.8150, 0.7926, 0.8178, 0.8195, 0.8148, 0.8055, 0.8319,\n",
            "        0.8338, 0.8059, 0.7993, 0.8197, 0.8854, 0.8378, 0.8840, 0.8034, 0.8227,\n",
            "        0.8283, 0.8310, 0.7845, 0.8204, 0.8798, 0.8211, 0.7838, 0.7893, 0.7812,\n",
            "        0.8488, 0.8126, 0.8101, 0.8248, 0.7681, 0.7743, 0.7819, 0.9110, 0.8200,\n",
            "        0.7569, 0.7792, 0.8366, 0.7883, 0.8131, 0.8047, 0.8415, 0.8265, 0.7610,\n",
            "        0.7898, 0.8392, 0.7941, 0.7533, 0.8616, 0.8420, 0.7869, 0.8216, 0.8024,\n",
            "        1.2443, 0.8168, 0.8529, 0.7907, 0.8274, 0.8539, 0.7878, 0.8237, 0.8074,\n",
            "        0.8214, 0.7938, 0.8045, 0.8205, 0.9494, 0.8140, 0.7813, 0.8519, 0.8056,\n",
            "        0.8402, 0.7720, 0.7907, 0.8242, 0.8478, 0.8229, 0.7814, 0.8257, 0.7962,\n",
            "        0.8264, 0.8280, 0.8258, 0.8627, 0.8229, 0.7925, 0.8172, 0.8139, 0.7995,\n",
            "        0.8773, 0.8333, 0.8293, 0.7715, 0.8145, 0.7740, 0.7940, 0.8213, 0.8117,\n",
            "        0.7642, 0.7931, 0.8569], device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.9.attention.output.LayerNorm.bias', Parameter containing:\n",
            "tensor([-8.9634e-02, -6.3622e-02,  6.7667e-02, -9.8957e-02,  8.3189e-02,\n",
            "         9.3695e-02,  1.6163e-01,  1.6849e-01, -1.2009e-01, -1.6283e-01,\n",
            "         1.2824e-02,  1.3595e-02, -2.0002e-01,  6.5053e-02,  3.3547e-03,\n",
            "         2.4297e-01,  2.0000e-01,  6.8249e-02, -8.8831e-02, -4.7633e-02,\n",
            "         1.9888e-01, -2.2325e-02, -3.4413e-02,  9.2640e-02,  3.4294e-03,\n",
            "         8.9293e-02,  6.9186e-02, -1.2650e-01, -9.4288e-02, -3.3893e-02,\n",
            "         1.3517e-01, -4.3623e-04,  1.3734e-01, -1.4416e-01, -8.7846e-02,\n",
            "         5.1723e-02, -1.6302e-01, -8.8349e-02, -4.6989e-02, -1.6174e-01,\n",
            "        -9.8409e-02, -1.7043e-01, -8.8724e-02,  8.7026e-02,  3.2200e-02,\n",
            "        -1.7585e-02,  1.5776e-01,  8.5899e-02, -3.0859e-02, -2.2616e-02,\n",
            "        -2.9177e-01, -3.1198e-02,  1.9825e-01, -7.7385e-02,  9.8072e-02,\n",
            "         1.2666e-01, -5.6339e-02, -1.3905e-01, -1.5887e-01, -1.5905e-01,\n",
            "        -6.4038e-02,  8.5224e-02, -3.4700e-02, -1.0963e-01,  1.3282e-02,\n",
            "         1.4645e-01, -3.2465e-02,  1.9734e-02, -1.8064e-01, -5.0230e-02,\n",
            "        -9.6340e-02,  6.3890e-02, -9.1495e-02, -6.3184e-02,  1.0972e-01,\n",
            "        -2.0805e-02, -1.2959e-01,  6.1818e-02,  4.8875e-02,  3.1672e-02,\n",
            "        -3.2338e-02,  9.3389e-02, -3.8628e-02,  8.0285e-02, -1.3373e-02,\n",
            "        -1.1157e-01,  6.8738e-02, -1.0489e-01, -1.8389e-01,  1.1481e-01,\n",
            "        -1.2556e-01, -1.4697e-01,  8.5729e-02, -8.7324e-02,  7.8940e-02,\n",
            "        -1.0211e-02,  8.6786e-02, -5.9400e-02,  5.5322e-02, -1.2584e-01,\n",
            "        -1.1083e-01, -8.0483e-02,  1.0781e-01, -7.5506e-02, -8.0584e-02,\n",
            "         3.6466e-02, -8.5659e-02, -1.7817e-01, -1.0515e-01,  3.6775e-02,\n",
            "        -7.7444e-02, -1.4709e-02, -6.7841e-02, -1.7277e-01, -7.6749e-02,\n",
            "         4.1880e-02, -3.5353e-02, -1.0811e-01, -7.8314e-02, -5.6127e-02,\n",
            "        -5.6506e-02, -7.9387e-02,  2.3372e-03,  1.0449e+00,  2.5767e-02,\n",
            "        -2.7539e-01, -3.5457e-02,  6.5390e-02, -1.1383e-01,  2.4931e-02,\n",
            "        -8.7080e-03,  2.5219e-02, -7.1286e-02, -7.8882e-02, -1.6375e-01,\n",
            "        -2.0500e-02,  2.7776e-02, -6.5971e-02, -7.1774e-02, -8.0701e-02,\n",
            "        -1.0329e-01,  6.0130e-03, -9.9378e-03,  1.0897e-01,  1.4376e-01,\n",
            "        -5.2824e-02, -8.3945e-02, -2.7456e-01, -1.0571e-01,  8.9165e-02,\n",
            "        -6.2144e-02,  1.1374e-01, -9.9546e-02, -5.9904e-02, -1.2443e-01,\n",
            "         5.3822e-02, -1.3767e-02,  2.7864e-02,  2.8030e-02, -1.6972e-01,\n",
            "        -1.9637e-02, -1.0432e-01, -4.1808e-02, -1.2087e-02,  1.8580e-01,\n",
            "         4.5931e-02,  7.6061e-02,  1.0496e-01, -1.2660e-01, -4.2139e-02,\n",
            "        -8.2277e-02, -1.4728e-01, -6.5631e-02, -8.0154e-02, -3.5748e-02,\n",
            "         5.3958e-02,  9.5627e-02, -2.0282e-02,  4.9823e-02,  5.4665e-02,\n",
            "        -4.5130e-01,  1.0649e-01, -3.7907e-02, -4.4066e-02,  2.2208e-02,\n",
            "        -5.1547e-03,  2.8113e-01, -6.6522e-02,  9.2171e-02,  1.7494e-02,\n",
            "        -1.4836e-01, -3.4533e-03,  1.4114e-01, -1.4549e-01,  2.6293e-02,\n",
            "        -1.0252e-01, -9.1711e-02, -4.4933e-02, -1.8692e-02, -2.1993e-02,\n",
            "         3.2264e-03,  1.3967e-02, -2.8481e-02, -1.4913e-01, -9.1051e-02,\n",
            "         3.9888e-02, -4.8912e-02, -9.4883e-02,  1.3721e-02,  3.3163e-02,\n",
            "        -1.6499e-01, -3.9790e-02, -4.5493e-03,  1.1105e-01, -2.9148e-02,\n",
            "        -4.1659e-02, -2.0134e-01, -4.9231e-02,  1.1906e-01, -1.3337e-01,\n",
            "         1.0928e-02,  1.5222e-01, -7.5770e-02,  5.1390e-02, -4.5789e-02,\n",
            "         4.1191e-01,  5.1656e-02, -1.7002e-01,  9.6573e-02, -6.4475e-02,\n",
            "        -1.6274e-01, -7.3911e-02,  3.2534e-01, -1.3640e-01, -9.1051e-02,\n",
            "        -3.9113e-02, -5.1964e-02, -1.4728e-02,  9.5567e-02, -3.0453e-01,\n",
            "         4.9397e-02,  7.7281e-02, -2.0980e-01,  1.3360e-01, -5.4493e-02,\n",
            "        -9.2164e-02,  6.1992e-02,  3.9270e-03, -1.0176e-01, -7.1568e-02,\n",
            "        -1.9041e-01,  1.4594e-01, -4.9409e-02, -1.0839e-01,  1.1297e-02,\n",
            "        -9.1030e-02, -6.3892e-02, -1.3601e-01, -4.7126e-02,  1.0431e-01,\n",
            "         1.2021e-01, -7.2285e-03, -4.1923e-02, -3.0529e-02, -2.1581e-02,\n",
            "        -2.4533e-01,  4.8957e-03, -1.7248e-04,  3.1634e-02,  2.7645e-02,\n",
            "        -2.6671e-02,  5.8838e-02, -1.2700e-01,  4.0932e-02, -4.4434e-02,\n",
            "        -8.9510e-02, -8.4061e-05, -1.8936e-03, -7.6442e-02, -8.5867e-02,\n",
            "         3.6410e-03,  1.0914e-01, -3.5020e-02, -1.0355e-01, -2.8205e-01,\n",
            "        -6.2289e-02,  1.5333e-01, -5.0480e-02, -3.2438e-02, -1.1360e-01,\n",
            "        -6.3252e-02, -4.3177e-03, -1.8378e-01, -2.8954e-02,  1.8174e-01,\n",
            "         3.2420e-02, -7.2692e-02,  8.3667e-03,  1.1529e-01, -6.7861e-02,\n",
            "        -1.1008e-01,  1.2941e-01, -1.4846e-02,  7.2553e-02,  8.9079e-02,\n",
            "        -4.6507e-02, -1.0127e-01, -1.3501e-01, -1.0327e+00, -3.1622e-03,\n",
            "        -1.5172e-01, -1.0368e-01,  1.2802e-01,  7.5823e-02, -1.3481e-01,\n",
            "         8.8376e-02, -4.8903e-02, -6.7820e-02, -5.3022e-02, -8.6679e-02,\n",
            "        -2.7180e-02,  1.6691e-01,  6.1938e-02, -2.7889e-01, -7.9135e-02,\n",
            "        -1.1899e-01, -1.4626e-02,  1.8332e-01, -2.5914e-02, -8.6856e-02,\n",
            "         6.8072e-02,  7.8954e-02,  5.3224e-02,  1.7055e-01, -5.1558e-02,\n",
            "        -1.0577e-01, -1.4009e-01, -2.7469e-02,  4.7304e-03, -1.3622e-01,\n",
            "         1.2525e-02,  1.0520e-01,  6.8147e-02,  5.0730e-02, -4.0849e-02,\n",
            "        -1.7330e-01, -1.3342e-02, -2.2979e-01, -9.0664e-02, -2.6308e-01,\n",
            "        -6.1537e-02, -1.0180e-01,  2.2991e-01, -7.1487e-02, -1.7504e-02,\n",
            "        -6.6763e-02, -1.0061e-01,  1.2692e-01,  2.4475e-02,  6.2756e-03,\n",
            "        -1.8025e-02, -2.1105e-01,  1.1078e-01,  1.7901e-02,  5.7091e-02,\n",
            "         1.5526e-02, -1.0141e-01,  5.2689e-03, -1.6347e-01, -1.0707e-01,\n",
            "        -2.7559e-01, -5.2087e-02, -2.8723e-02, -2.0108e-01, -2.6397e-01,\n",
            "        -2.1626e-02,  1.1753e-01, -1.6254e-01, -2.1556e-01,  1.0447e-02,\n",
            "        -6.2737e-02, -1.4833e+00, -1.1046e-01, -1.7192e-01,  2.4766e-01,\n",
            "        -5.0067e-02, -9.2092e-02,  7.8533e-02, -5.4513e-02, -1.7551e-01,\n",
            "        -1.3987e-01, -8.4708e-03,  3.1370e-03, -1.7431e-01, -1.0219e-01,\n",
            "        -6.1529e-02, -1.8624e-01, -1.5332e-01,  1.7929e-01, -7.5322e-02,\n",
            "        -3.3790e-02, -1.2696e-01,  9.9250e-02, -9.6178e-02, -1.3553e-01,\n",
            "        -5.8228e-02,  2.9613e-02, -1.9149e-01,  8.2919e-02, -6.0430e-02,\n",
            "        -5.1517e-02,  7.5239e-03, -1.8235e-02, -8.7784e-02, -4.8428e-02,\n",
            "         5.1432e-02,  2.6579e-02, -1.2016e-01, -9.3306e-03, -2.8217e-02,\n",
            "        -2.0247e-02, -7.8200e-02,  2.2541e-02, -1.4143e-01, -1.0894e-02,\n",
            "         1.6573e-02, -1.0229e-01, -1.9638e-01, -1.4054e-01,  8.7213e-02,\n",
            "        -1.3702e-01, -1.5208e-01, -7.6166e-02,  9.1434e-02, -8.1207e-02,\n",
            "         3.4009e-02,  4.7558e-02, -1.8234e-01, -3.5899e-02, -5.7347e-02,\n",
            "         5.4107e-02,  1.4793e-02,  1.0430e-02, -1.7031e-01, -1.0464e-01,\n",
            "        -3.5042e-01, -1.9158e-01,  9.2418e-02,  3.7559e-02, -9.7405e-02,\n",
            "         6.1124e-02, -1.1003e-01,  1.3475e-01, -1.1100e-01,  1.8244e-01,\n",
            "         1.1706e-01, -4.4486e-02,  5.5285e-02, -3.4761e-01, -2.7539e-01,\n",
            "        -1.7336e-02, -1.7760e-01,  1.8534e-02,  1.2154e-01,  2.1128e-01,\n",
            "        -8.5735e-02,  1.3856e-01, -3.2358e-01, -3.3461e-03, -5.2334e-02,\n",
            "         1.5147e-02, -6.4845e-02, -9.5687e-02,  1.0531e-01, -1.1930e-01,\n",
            "        -1.6826e-01, -6.2170e-02,  3.8616e-02, -1.6144e-01, -7.9383e-02,\n",
            "        -4.7872e-02, -6.3124e-02, -1.1797e-01,  9.5356e-02, -1.5895e-01,\n",
            "        -1.1327e-01, -4.9932e-02,  1.3097e-01, -2.3225e-02, -4.4876e-02,\n",
            "        -1.6668e-01, -1.0864e-01,  2.1689e-03,  1.7966e-02, -4.6334e-02,\n",
            "        -1.0440e-01, -1.7442e-01, -2.9528e-02,  1.8975e-02,  1.1198e-01,\n",
            "         4.1941e-02,  1.1200e-01, -8.8794e-02, -1.4606e-01, -4.5993e-02,\n",
            "        -3.4797e-03, -1.5252e-01,  4.3101e-02, -2.1567e-01, -7.5942e-02,\n",
            "        -1.0493e-01, -6.9546e-02, -3.4403e-02, -9.2701e-02, -3.4303e-02,\n",
            "         1.7153e-01, -2.0395e-02, -1.1249e-01, -1.9277e-01,  1.5488e-02,\n",
            "        -5.1233e-02,  6.1843e-02,  3.2551e-02, -8.7959e-02, -1.8635e-02,\n",
            "        -1.0520e-01, -1.0839e-01, -5.9517e-02,  1.5896e-02,  3.5751e-02,\n",
            "         1.3413e-02, -3.1133e-02, -1.4934e-01, -9.0302e-03,  4.5739e-02,\n",
            "        -1.4792e-01, -2.0007e-02, -1.3493e-01,  1.0787e-02, -3.3816e-01,\n",
            "         3.1175e-02, -4.6899e-02,  4.3312e-02, -6.6313e-02, -1.5312e-01,\n",
            "        -1.0540e-01, -9.8326e-02, -4.7153e-03,  1.3070e-01,  8.9454e-02,\n",
            "        -9.8902e-02, -9.2686e-02,  1.0269e-01, -2.8549e-02,  6.6661e-03,\n",
            "        -5.3918e-02, -7.3896e-02,  1.2809e-01, -1.2009e-02, -8.5162e-02,\n",
            "        -1.2883e-01, -8.3677e-02,  1.4764e-01, -1.0634e-01, -1.3061e-01,\n",
            "         1.0886e-02, -1.9140e-02,  5.7075e-02, -5.2589e-02,  2.6397e-02,\n",
            "        -1.2699e-01, -4.0904e-03,  1.5376e-01, -2.3331e-02, -1.1140e-01,\n",
            "         5.7679e-02,  3.1887e-02, -2.9850e-01,  2.0528e-02, -2.6509e-01,\n",
            "        -7.3115e-02,  5.6351e-02,  6.2625e-02,  2.8202e-02, -1.5173e-01,\n",
            "        -1.3854e-01,  1.9960e-01, -7.8035e-02, -1.3321e-01, -6.6651e-02,\n",
            "        -6.6570e-02, -1.0854e-01,  2.9537e-02, -6.8222e-02, -4.1412e-02,\n",
            "         5.0937e-02, -1.3486e-01, -3.0162e-02,  2.1738e-02,  6.1227e-02,\n",
            "        -1.3544e-04, -5.1528e-02, -1.2112e-01,  1.0918e-01,  5.4059e-02,\n",
            "         1.4152e-01, -2.2390e-02,  1.9856e-02, -2.0093e-01,  7.6524e-03,\n",
            "         5.3075e-02, -1.1348e-01, -5.5725e-02,  2.0508e-02, -3.6909e-02,\n",
            "        -1.3771e-01, -1.8553e-01, -4.8380e-02, -1.0147e-01, -1.7881e-01,\n",
            "         1.9155e-01, -1.5777e-02,  4.4207e-02, -2.8531e-02,  3.4955e-03,\n",
            "         6.8470e-02,  1.5250e-01, -6.3887e-02,  1.6214e-02,  1.7061e-02,\n",
            "         1.2607e-01, -5.5609e-03,  1.2198e-01,  2.5293e-01, -3.7000e-02,\n",
            "         1.1397e-02,  1.0501e-01,  3.7040e-02,  1.0137e-01, -1.6220e-02,\n",
            "        -1.1710e-01,  2.0133e-01, -1.5687e-01, -2.4239e-01, -4.3420e-02,\n",
            "        -3.1539e-01,  8.1076e-03,  2.2814e-01,  7.4322e-02,  1.9125e-01,\n",
            "         1.0501e-01,  7.0834e-02,  1.4475e-01,  8.2203e-02, -6.9037e-02,\n",
            "        -1.0078e-01,  8.2504e-02,  1.0292e-01, -9.9707e-03, -1.0356e-01,\n",
            "         2.0269e-02, -8.9988e-02,  9.3030e-02, -7.7161e-02, -9.7605e-03,\n",
            "         1.4462e-02, -3.4608e-02,  2.0389e-01,  1.3810e-01,  1.7598e-01,\n",
            "         1.2853e-01, -5.7037e-02, -1.6307e-01, -6.9622e-02,  1.8726e-01,\n",
            "        -9.3597e-02,  7.7707e-02,  2.6425e-02, -3.1967e-02, -6.2175e-02,\n",
            "         2.8677e-02, -8.3913e-02,  7.2417e-02, -6.8797e-02, -3.4197e-02,\n",
            "        -4.2399e-02,  3.0505e-02,  1.4990e-01, -1.6313e-01, -3.1918e-02,\n",
            "        -8.8834e-02, -9.3050e-02, -5.1629e-02,  9.4387e-05,  4.2220e-02,\n",
            "        -2.3819e-02, -2.0130e-01, -6.7952e-02, -6.0981e-02,  6.2493e-02,\n",
            "         1.7185e-01, -8.5870e-03, -1.0499e-01, -7.3440e-03,  1.7131e-01,\n",
            "         3.7926e-02,  2.7036e-01,  7.5689e-02, -1.9814e-02,  3.0447e-02,\n",
            "        -9.0346e-02,  7.0207e-02,  5.3655e-02,  4.0548e-02, -9.5520e-02,\n",
            "        -1.1084e-02, -2.1027e-01,  2.4161e-02,  4.7274e-02, -4.2653e-02,\n",
            "        -6.7483e-01, -1.2850e-02, -9.2719e-02,  5.6161e-02,  9.8158e-02,\n",
            "        -1.5418e-01, -8.3470e-02, -1.3492e-01, -8.9307e-02, -6.8725e-02,\n",
            "         4.8085e-02,  1.1932e-02, -8.5937e-02,  7.0960e-02,  2.3639e-01,\n",
            "        -6.3271e-02, -1.0931e-01,  2.1265e-01, -1.1311e-01,  5.1169e-02,\n",
            "        -3.5789e-02, -2.7773e-02, -2.3312e-01, -1.4190e-01,  4.1386e-02,\n",
            "         7.7668e-02,  1.8668e-02, -2.0945e-01,  6.8989e-02,  7.8100e-02,\n",
            "        -8.4330e-02, -8.3940e-02, -7.9275e-02,  1.5135e-01, -1.6493e-01,\n",
            "        -4.6172e-02, -2.9566e-01,  6.8822e-02, -8.8592e-02, -1.5561e-01,\n",
            "         6.2503e-02, -1.3773e-01,  5.5092e-02, -1.6108e-01, -1.6780e-01,\n",
            "        -5.0354e-02, -7.1745e-02, -6.7072e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.9.intermediate.dense.weight', Parameter containing:\n",
            "tensor([[ 0.0629, -0.0076,  0.0022,  ...,  0.0212,  0.0408, -0.0592],\n",
            "        [ 0.0203, -0.0268,  0.0203,  ..., -0.0099, -0.0123, -0.0222],\n",
            "        [-0.0538, -0.0307,  0.0282,  ...,  0.0060,  0.0669, -0.0058],\n",
            "        ...,\n",
            "        [ 0.0031,  0.0109,  0.0724,  ...,  0.0184, -0.0549, -0.0085],\n",
            "        [ 0.0004,  0.0598, -0.0426,  ...,  0.0170,  0.0828, -0.0586],\n",
            "        [ 0.0307, -0.0489, -0.0273,  ..., -0.1159,  0.0398, -0.0008]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.9.intermediate.dense.bias', Parameter containing:\n",
            "tensor([-0.1082, -0.0017, -0.0896,  ..., -0.0263, -0.0708, -0.0997],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.9.output.dense.weight', Parameter containing:\n",
            "tensor([[ 0.0549,  0.0073,  0.0138,  ..., -0.0440, -0.0059, -0.0271],\n",
            "        [ 0.0164,  0.0396,  0.0088,  ..., -0.0044, -0.0283, -0.0266],\n",
            "        [ 0.0044, -0.0237, -0.0002,  ...,  0.0162, -0.0186, -0.0064],\n",
            "        ...,\n",
            "        [ 0.0046, -0.0307, -0.0117,  ...,  0.0189, -0.0245, -0.0089],\n",
            "        [-0.0031, -0.0036, -0.0307,  ..., -0.0121,  0.0417, -0.0260],\n",
            "        [-0.0341, -0.0096, -0.0177,  ..., -0.0648,  0.0017,  0.0063]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.9.output.dense.bias', Parameter containing:\n",
            "tensor([-8.7552e-02, -1.9101e-02, -2.9883e-02, -8.1504e-02,  9.8980e-02,\n",
            "         5.9764e-02,  7.0469e-02,  6.3483e-02, -5.0608e-02, -6.9417e-02,\n",
            "         2.0236e-02,  4.9168e-02, -5.7219e-02,  1.2250e-01, -1.7906e-02,\n",
            "         1.4715e-01,  1.7219e-01,  2.1932e-02,  4.7961e-03, -2.1743e-02,\n",
            "         6.5337e-02,  1.5524e-02,  4.8866e-02,  2.4456e-02, -3.7374e-02,\n",
            "         5.5888e-02,  9.7395e-02,  5.0426e-02, -1.0072e-01,  3.8435e-02,\n",
            "         8.8411e-02, -3.3628e-02,  6.0645e-02, -1.0744e-01, -7.4975e-02,\n",
            "         2.6858e-02, -8.6133e-02, -2.6558e-02, -7.0204e-02, -3.9223e-02,\n",
            "        -1.5491e-02, -1.1075e-01,  1.9014e-02,  1.0081e-01,  6.8838e-02,\n",
            "         1.5397e-02,  1.4781e-02,  2.6420e-02, -2.1303e-02,  4.2291e-03,\n",
            "        -1.4659e-01,  7.6633e-02,  1.2531e-01,  2.2555e-02,  1.2799e-01,\n",
            "         9.7054e-02,  1.4199e-02, -2.5208e-02, -5.2480e-02, -1.5332e-01,\n",
            "         2.9439e-02,  6.6671e-02,  7.7241e-02, -1.1008e-01,  4.2143e-02,\n",
            "         8.1816e-02, -8.6334e-02,  8.0345e-02, -8.2716e-02, -6.8623e-02,\n",
            "         1.4669e-02,  5.3713e-02,  4.5828e-03, -5.0369e-02,  1.0560e-02,\n",
            "        -1.1561e-01, -5.1248e-02,  1.2057e-01,  7.7503e-02,  8.8792e-02,\n",
            "         5.3211e-03,  7.5784e-03,  6.2266e-03,  1.0384e-01, -3.4989e-02,\n",
            "        -1.6219e-01, -6.1267e-02, -3.6482e-03, -6.5597e-02,  9.7629e-02,\n",
            "        -1.2195e-01, -4.4889e-02,  1.0313e-01,  5.4797e-03,  3.3794e-02,\n",
            "        -1.1611e-02,  1.1351e-01, -2.3649e-02,  6.6286e-02, -9.9045e-02,\n",
            "         5.5303e-02, -3.9737e-02,  6.1537e-02, -4.0833e-02, -8.6907e-02,\n",
            "         6.6727e-02, -2.6189e-02, -8.0558e-02, -3.3461e-02, -4.3184e-02,\n",
            "        -3.9677e-02,  1.5538e-01, -2.5149e-02, -5.2311e-02, -1.6817e-02,\n",
            "         7.4012e-02, -1.7712e-02,  5.4394e-02, -2.4889e-02, -3.9993e-03,\n",
            "         2.4664e-02,  9.0225e-02, -1.0348e-02,  3.4622e-01, -9.4902e-03,\n",
            "        -6.2517e-02, -2.2002e-02,  6.9746e-02, -9.6318e-02, -8.6096e-03,\n",
            "        -2.6965e-03,  6.8363e-02, -2.4802e-02, -1.5015e-02, -7.2702e-02,\n",
            "        -5.5987e-02,  3.7600e-02,  1.6591e-02,  9.1095e-03,  6.1895e-02,\n",
            "        -6.5435e-02,  1.6176e-02, -3.7755e-02,  5.1033e-02,  1.5009e-01,\n",
            "        -2.4141e-02,  5.9902e-03, -1.7313e-01, -1.1148e-01,  5.6002e-02,\n",
            "        -1.2592e-01,  5.2785e-03, -9.1512e-02,  2.4099e-02, -8.7852e-02,\n",
            "         6.0496e-02,  3.2715e-02,  6.1635e-02, -6.3099e-02, -3.8216e-02,\n",
            "         2.9694e-02, -2.8703e-02,  4.4855e-02, -4.2044e-02,  1.0165e-01,\n",
            "         8.9768e-02,  2.7094e-02,  9.6900e-02, -5.8014e-02,  3.7607e-02,\n",
            "        -2.3233e-02, -1.3521e-02, -9.0145e-03, -6.2598e-02, -6.0940e-02,\n",
            "        -8.2780e-02,  5.0186e-02,  2.4054e-02,  1.2223e-01,  5.3364e-02,\n",
            "        -6.3819e-01,  6.2707e-02, -3.3669e-03, -4.2087e-02,  2.3676e-02,\n",
            "        -2.5632e-02,  1.3470e-01, -1.1527e-02,  6.4680e-02,  1.5122e-02,\n",
            "        -6.1214e-02,  5.9024e-02,  6.8468e-02, -7.7565e-02,  6.8911e-02,\n",
            "        -1.0017e-02, -9.0518e-02,  4.3546e-02,  4.4535e-03, -1.7598e-03,\n",
            "        -6.2536e-02, -4.9217e-02,  4.0797e-02, -9.4278e-02, -7.4907e-02,\n",
            "         1.1984e-01, -4.2156e-02,  4.6366e-02, -1.9191e-03,  1.1976e-01,\n",
            "        -1.2034e-01, -4.5352e-02,  4.6730e-02,  1.6635e-01,  4.0956e-02,\n",
            "        -8.6741e-03, -8.1010e-02, -5.4985e-02,  3.5872e-02, -1.2873e-01,\n",
            "         6.9693e-02,  4.7547e-02, -7.7888e-02,  6.0655e-02, -1.5256e-02,\n",
            "         1.3536e-01,  7.8470e-02, -5.3165e-02,  9.4824e-05, -1.7653e-02,\n",
            "        -1.0447e-01,  1.1193e-02,  6.3987e-02, -7.1466e-02, -3.6802e-02,\n",
            "         5.2816e-03,  2.6962e-02,  1.9735e-02,  7.9609e-02, -8.0390e-02,\n",
            "         1.2347e-01,  6.7812e-02, -3.1846e-02,  1.1534e-01, -2.6287e-02,\n",
            "        -5.9748e-02,  1.0363e-02, -2.5639e-02, -4.0211e-02, -1.1744e-02,\n",
            "        -1.5721e-01,  4.3972e-03,  3.0167e-02, -9.7805e-03,  5.6758e-02,\n",
            "        -9.3214e-02,  6.9306e-03, -4.4406e-02,  8.5561e-02,  1.1771e-01,\n",
            "         6.9305e-02,  4.3670e-02, -3.6307e-02,  1.9562e-03, -8.3519e-03,\n",
            "        -9.0759e-02,  3.0296e-02, -7.7617e-02,  7.8057e-02,  6.1967e-02,\n",
            "         1.0569e-01,  1.4329e-01, -5.7834e-02,  6.0752e-02, -1.4316e-02,\n",
            "        -4.2428e-02,  4.6467e-02, -1.7290e-02, -4.2118e-02, -4.1163e-02,\n",
            "        -2.2674e-02,  1.2416e-01, -6.1690e-02, -1.2397e-02, -1.3567e-01,\n",
            "        -1.0550e-01,  8.9255e-02, -7.9626e-02, -5.2635e-02, -4.7508e-03,\n",
            "        -4.7494e-02,  5.1953e-02, -8.8919e-02, -1.0175e-01,  1.0324e-01,\n",
            "         3.6963e-02, -4.2933e-03, -4.7262e-02,  2.0356e-02, -2.3087e-02,\n",
            "        -5.9691e-02,  5.1103e-02,  6.3223e-04,  7.3352e-02,  6.9537e-02,\n",
            "        -4.8742e-02,  2.5387e-02, -5.6810e-02, -6.1066e-01, -4.4211e-02,\n",
            "        -8.0503e-02, -6.0467e-02,  7.0444e-02,  6.9231e-02,  1.0150e-02,\n",
            "        -2.6286e-02, -7.8187e-02,  1.2135e-02, -6.2621e-03,  3.2202e-02,\n",
            "        -9.3170e-03,  7.1725e-02,  7.2442e-02, -1.4227e-01,  1.8552e-02,\n",
            "        -7.8338e-02,  1.0542e-02,  1.1239e-01,  5.2885e-02,  3.8779e-02,\n",
            "        -1.0015e-01,  1.2903e-02, -1.8343e-03,  3.9987e-02, -6.7529e-02,\n",
            "        -2.5000e-03, -4.7019e-02, -3.9251e-02, -4.1427e-02, -6.3993e-02,\n",
            "         8.6111e-02,  9.2680e-02,  4.1254e-02,  2.9180e-02, -9.1338e-03,\n",
            "        -6.2895e-02,  2.6644e-03, -8.6475e-02,  1.6118e-02, -1.2145e-01,\n",
            "        -8.3768e-02, -4.5096e-02,  1.2818e-01, -5.7325e-02, -1.4708e-02,\n",
            "        -4.9054e-02, -9.2094e-02,  8.5005e-02,  2.7500e-02,  2.4341e-02,\n",
            "         8.2859e-02, -9.8056e-02,  1.0937e-01,  3.2294e-02,  4.2069e-02,\n",
            "         1.0627e-01, -1.2262e-01,  3.5071e-02, -4.8184e-02, -3.3970e-02,\n",
            "        -1.4408e-01,  7.2304e-03, -1.8278e-02, -6.9799e-02, -8.7174e-02,\n",
            "        -4.4147e-02, -1.9730e-02, -4.3730e-02, -1.6056e-01,  1.7131e-02,\n",
            "         3.1927e-02, -2.9129e-01, -1.3208e-04, -8.6744e-02,  1.0072e-01,\n",
            "        -9.7853e-02, -5.3415e-02,  1.0111e-03, -5.5046e-02, -5.0510e-02,\n",
            "        -2.0119e-02,  1.0338e-01,  3.3506e-02, -5.2398e-02, -5.2287e-02,\n",
            "         1.7510e-02, -9.9062e-02, -5.0860e-02,  1.0784e-01, -9.4566e-03,\n",
            "        -4.0615e-02, -1.1739e-01,  8.4467e-02,  4.2408e-03, -7.0628e-02,\n",
            "         1.5794e-02, -8.7676e-02, -6.4637e-02,  3.0169e-02, -4.5232e-02,\n",
            "        -4.5618e-02,  3.8786e-02, -6.4020e-02, -3.1999e-03, -4.4916e-02,\n",
            "         3.0444e-02,  5.7500e-02, -3.8595e-02, -2.4118e-02, -1.5788e-02,\n",
            "         1.9027e-02, -5.1826e-02,  6.0551e-02, -1.3605e-01,  3.3981e-02,\n",
            "         1.0574e-01,  1.5876e-02, -1.1774e-01,  4.7213e-03,  1.0500e-01,\n",
            "        -1.0522e-01, -6.5394e-02, -4.6684e-03,  1.1568e-01, -2.9537e-02,\n",
            "         1.9302e-02,  8.0413e-02, -4.7058e-02, -3.3123e-02, -2.8264e-02,\n",
            "         1.2967e-01,  3.9024e-02,  6.2766e-03, -8.3924e-02, -2.0221e-02,\n",
            "        -1.0369e-01, -5.6690e-02,  6.1575e-02, -3.5458e-02, -2.4787e-02,\n",
            "         4.7428e-02, -4.3019e-02,  6.3370e-02, -1.2462e-02,  8.3076e-02,\n",
            "        -1.3625e-02,  9.2603e-03,  4.2029e-03, -1.8970e-01, -1.6745e-02,\n",
            "        -2.1018e-02, -1.1468e-01, -3.5212e-02,  1.1852e-01,  8.4228e-02,\n",
            "         5.1111e-03,  1.1418e-01, -1.5040e-01,  6.2058e-02, -3.2656e-02,\n",
            "         1.8308e-02, -2.1581e-02, -3.2736e-02,  7.3150e-02, -6.4752e-02,\n",
            "        -9.4181e-02, -4.8331e-02,  6.4193e-03, -6.7217e-02, -1.8256e-02,\n",
            "         3.1565e-05, -4.0711e-02, -1.1345e-01,  3.9847e-02, -4.3403e-02,\n",
            "        -8.2811e-04, -1.5399e-02,  9.3750e-02, -8.5563e-03,  1.3749e-02,\n",
            "        -4.4913e-02,  4.7170e-02,  4.2243e-02,  4.4624e-02, -2.9499e-02,\n",
            "        -8.8034e-03, -2.2389e-02,  3.9218e-02, -2.6770e-02, -4.7181e-02,\n",
            "         3.1094e-02,  6.1592e-02, -9.8796e-02, -9.6118e-02, -4.7970e-02,\n",
            "         5.7782e-02, -9.1554e-02,  9.4851e-02, -1.2530e-01, -3.6643e-03,\n",
            "        -4.5754e-02, -8.6513e-02, -2.7181e-02, -4.1280e-02,  4.2433e-02,\n",
            "         1.3953e-01, -2.1563e-02, -3.9561e-03, -8.6442e-02,  3.1682e-02,\n",
            "        -1.7148e-02,  5.3783e-02,  2.4924e-03, -7.1851e-02, -4.1573e-02,\n",
            "        -2.9374e-02,  2.6773e-02, -4.9041e-02,  2.4384e-02, -1.5951e-04,\n",
            "         5.3462e-02,  9.5581e-03, -4.2903e-02,  5.2717e-02,  1.1375e-02,\n",
            "        -6.6682e-02,  5.7971e-02, -1.4347e-02,  6.3449e-02, -2.0303e-01,\n",
            "         2.6150e-02,  4.1828e-02, -7.0530e-03, -1.1210e-02, -3.0129e-02,\n",
            "        -7.7546e-02,  4.1267e-02,  9.3949e-02,  6.9542e-02,  2.3388e-02,\n",
            "         1.7832e-02,  1.6107e-02,  3.9901e-03,  1.2365e-01,  2.4412e-02,\n",
            "        -4.0933e-02,  3.1260e-02,  2.4695e-02, -1.8128e-02, -6.2454e-02,\n",
            "         6.7982e-03,  6.7770e-02,  5.1695e-02, -5.8211e-02, -1.2121e-01,\n",
            "         3.5418e-02, -4.1471e-02,  1.3869e-01,  7.7779e-02,  2.8041e-02,\n",
            "        -7.1986e-02,  2.6992e-02,  1.4375e-01, -6.9353e-02,  2.0755e-02,\n",
            "        -1.3017e-02,  2.4441e-02, -1.5003e-01,  1.0360e-01, -1.2508e-01,\n",
            "        -4.2380e-02,  9.7494e-03,  1.0667e-01,  1.7276e-01, -2.7205e-02,\n",
            "        -5.9563e-02,  5.6052e-02, -9.3633e-02, -9.9569e-02,  2.8048e-02,\n",
            "        -2.3351e-02, -8.9287e-02,  2.0996e-02, -6.4035e-02,  9.4746e-03,\n",
            "         7.0486e-02, -9.0936e-02, -1.2638e-02, -2.5624e-04,  1.2731e-01,\n",
            "        -7.6902e-03,  6.8181e-04, -8.9530e-02,  8.1624e-02,  1.0716e-01,\n",
            "         8.8180e-02,  3.3903e-02,  4.8081e-02, -7.4646e-02,  2.5266e-02,\n",
            "         1.1099e-01, -1.2067e-02,  7.8247e-02,  5.0167e-02,  9.4191e-02,\n",
            "        -1.2335e-01, -5.1383e-02, -3.9435e-02,  3.2673e-02, -5.8408e-02,\n",
            "         1.1142e-01, -4.1838e-02, -2.9445e-02,  9.4614e-03,  5.3572e-02,\n",
            "         1.4995e-02,  9.3793e-02, -1.8687e-03,  5.4938e-02,  1.5869e-02,\n",
            "         1.2725e-01,  3.5434e-02,  1.2855e-01,  8.4082e-02, -8.6476e-03,\n",
            "         7.8199e-02,  7.4972e-02,  1.1008e-01,  1.1953e-01,  4.1692e-03,\n",
            "        -7.4996e-02,  9.5510e-02, -1.0597e-01, -6.3848e-02, -1.2091e-02,\n",
            "        -1.0671e-01,  2.2532e-02,  1.2704e-01,  4.6429e-02,  9.3594e-02,\n",
            "         7.9082e-02,  7.3825e-02,  4.2154e-02,  1.5092e-01, -3.8896e-02,\n",
            "        -3.8002e-02,  1.1316e-01,  6.6065e-02,  9.3564e-02, -4.6570e-02,\n",
            "         7.6666e-02, -2.3078e-02,  5.8144e-02, -4.0367e-02,  9.0807e-02,\n",
            "         1.4236e-02, -1.3126e-02,  4.3617e-02,  9.0530e-02,  1.5598e-01,\n",
            "         1.9056e-01, -2.4029e-02, -5.2486e-02, -6.4968e-02,  1.6059e-01,\n",
            "        -3.9003e-02,  6.0146e-02,  3.2064e-02, -6.1824e-02, -4.3470e-02,\n",
            "         1.6267e-02, -7.3185e-02,  1.4688e-01, -6.1569e-02,  3.5383e-02,\n",
            "         1.5175e-02, -1.1097e-02,  7.9761e-02, -2.5666e-02, -1.8623e-02,\n",
            "        -5.2326e-02, -1.2878e-01,  2.3762e-02,  2.8581e-02,  1.0265e-01,\n",
            "        -9.5077e-03, -1.1973e-01, -4.7809e-02,  1.9155e-02, -6.8521e-02,\n",
            "         1.4069e-01,  5.0390e-02, -6.8261e-02,  8.0132e-02,  1.0216e-01,\n",
            "         1.2100e-02,  8.4435e-02,  7.3140e-02,  3.1655e-03,  6.0415e-02,\n",
            "        -1.2637e-01,  3.2786e-02, -2.2860e-02,  2.8518e-02, -2.1612e-02,\n",
            "         9.4717e-03, -1.0841e-01,  3.2258e-02,  5.6320e-02,  1.5290e-02,\n",
            "        -5.5774e-01,  2.8587e-02, -1.8576e-02, -1.3262e-02,  4.4602e-02,\n",
            "        -6.2596e-02, -1.1419e-02, -1.2759e-01, -1.0925e-02, -3.6998e-02,\n",
            "         2.8758e-02,  5.6271e-02, -4.1738e-02, -1.2156e-02,  1.6054e-01,\n",
            "        -1.8478e-02, -4.9370e-03,  1.4750e-01, -1.0961e-01,  2.0658e-02,\n",
            "        -5.6119e-02,  4.6679e-02, -9.2555e-02, -6.0768e-02,  7.0134e-02,\n",
            "         3.5825e-02,  7.8854e-03, -2.3229e-02,  1.4131e-02,  1.1417e-01,\n",
            "        -9.1527e-02, -4.8586e-02, -3.8156e-02,  8.4667e-02, -7.0317e-02,\n",
            "        -8.5190e-02, -1.5605e-01,  8.2730e-02,  5.1901e-02, -2.9160e-02,\n",
            "         3.1027e-02, -4.4396e-02,  1.3495e-01, -1.4035e-02, -5.6577e-02,\n",
            "         3.4225e-02,  2.1248e-02,  4.5314e-03], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.9.output.LayerNorm.weight', Parameter containing:\n",
            "tensor([0.8251, 0.8348, 0.7794, 0.8126, 0.7828, 0.8165, 0.7776, 0.7747, 0.8210,\n",
            "        0.7964, 0.7777, 0.7842, 0.7933, 0.8016, 0.8251, 0.7820, 0.7734, 0.7917,\n",
            "        0.8153, 0.8294, 0.7857, 0.7943, 0.8350, 0.8044, 0.8106, 0.7977, 0.7718,\n",
            "        0.8028, 0.7701, 0.8530, 0.8197, 0.7987, 0.8022, 0.7990, 0.8531, 0.8301,\n",
            "        0.7703, 0.8147, 0.7926, 0.7936, 0.8294, 0.8227, 0.7952, 0.8105, 0.8121,\n",
            "        0.7895, 0.8224, 0.8203, 0.8467, 0.8441, 0.7715, 0.8163, 0.8154, 0.7807,\n",
            "        0.7886, 0.7995, 0.8315, 0.7771, 0.8254, 0.8022, 0.8009, 0.7931, 0.8045,\n",
            "        0.7798, 0.8309, 0.8031, 0.8369, 0.8285, 0.8119, 0.8138, 0.8305, 0.8246,\n",
            "        0.8315, 0.7902, 0.8092, 0.7675, 0.7957, 0.7714, 0.8129, 0.8209, 0.8283,\n",
            "        0.8230, 0.8065, 0.8160, 0.7973, 0.8186, 0.7803, 0.8248, 0.8407, 0.7930,\n",
            "        0.8030, 0.8242, 0.7747, 0.7540, 0.8093, 0.8211, 0.7576, 0.8064, 0.8067,\n",
            "        0.8141, 0.8427, 0.8502, 0.7863, 0.8153, 0.7770, 0.7758, 0.8113, 0.8116,\n",
            "        0.8187, 0.7715, 0.8011, 0.7986, 0.8053, 0.8180, 0.8365, 0.8390, 0.8062,\n",
            "        0.8318, 0.7999, 0.8233, 0.7957, 0.7907, 0.8353, 0.1663, 0.8098, 0.7777,\n",
            "        0.7882, 0.7617, 0.8072, 0.8148, 0.8258, 0.8303, 0.7876, 0.8151, 0.7999,\n",
            "        0.8330, 0.8135, 0.7883, 0.8148, 0.7804, 0.8275, 0.8257, 0.8150, 0.7447,\n",
            "        0.7917, 0.8153, 0.8149, 0.7527, 0.8124, 0.7785, 0.7845, 0.7735, 0.7998,\n",
            "        0.8182, 0.8317, 0.7964, 0.8098, 0.7814, 0.8062, 0.7777, 0.8044, 0.7984,\n",
            "        0.8277, 0.8313, 0.7588, 0.8208, 0.7930, 0.7996, 0.7547, 0.8110, 0.8197,\n",
            "        0.8066, 0.8038, 0.8155, 0.8202, 0.8340, 0.7884, 0.8155, 0.8113, 0.7976,\n",
            "        0.1416, 0.8115, 0.8415, 0.8499, 0.8355, 0.8290, 0.7574, 0.8101, 0.7923,\n",
            "        0.8234, 0.8126, 0.8331, 0.7698, 0.7824, 0.7892, 0.8021, 0.8207, 0.8018,\n",
            "        0.8180, 0.8023, 0.8192, 0.8107, 0.8279, 0.7781, 0.7852, 0.6966, 0.8377,\n",
            "        0.8110, 0.8203, 0.7936, 0.7912, 0.8254, 0.8101, 0.7749, 0.8121, 0.7943,\n",
            "        0.7995, 0.7963, 0.7826, 0.8149, 0.8187, 0.7952, 0.8192, 0.7814, 0.8082,\n",
            "        0.3372, 0.8132, 0.7895, 0.8107, 0.7660, 0.7589, 0.8285, 0.8014, 0.8318,\n",
            "        0.8236, 0.7872, 0.7850, 0.8034, 0.7982, 0.7600, 0.8229, 0.8231, 0.7998,\n",
            "        0.7718, 0.8273, 0.7904, 0.8195, 0.8088, 0.7735, 0.8079, 0.8067, 0.8565,\n",
            "        0.8043, 0.8117, 0.8019, 0.8084, 0.8137, 0.7587, 0.8066, 0.7961, 0.8284,\n",
            "        0.8194, 0.8553, 0.8281, 0.7880, 0.7507, 0.7752, 0.8030, 0.7949, 0.8124,\n",
            "        0.8038, 0.8077, 0.8146, 0.8063, 0.8237, 0.8176, 0.7756, 0.8122, 0.8022,\n",
            "        0.7971, 0.7880, 0.7773, 0.8359, 0.7711, 0.7809, 0.8221, 0.7846, 0.8284,\n",
            "        0.8062, 0.8281, 0.7902, 0.7822, 0.8118, 0.7755, 0.7783, 0.7924, 0.8119,\n",
            "        0.8184, 0.7972, 0.8094, 0.8189, 0.7841, 0.8178, 0.8380, 0.8079, 0.8106,\n",
            "        0.8015, 0.8089, 1.4368, 0.8324, 0.7814, 0.7687, 0.7665, 0.8197, 0.8279,\n",
            "        0.7889, 0.7996, 0.8173, 0.8145, 0.8092, 0.8309, 0.7942, 0.8039, 0.7998,\n",
            "        0.8222, 0.8200, 0.8132, 0.7628, 0.8252, 0.8232, 0.7979, 0.7718, 0.8213,\n",
            "        0.8053, 0.7994, 0.8032, 0.7628, 0.8216, 0.8292, 0.7952, 0.8085, 0.8388,\n",
            "        0.8063, 0.8085, 0.8071, 0.7865, 0.8005, 0.7313, 0.8029, 0.7600, 0.8055,\n",
            "        0.8069, 0.7975, 0.7985, 0.8311, 0.7962, 0.7773, 0.7786, 0.7927, 0.8052,\n",
            "        0.7948, 0.7855, 0.8078, 0.8031, 0.8176, 0.7887, 0.7931, 0.8140, 0.8100,\n",
            "        0.8133, 0.7698, 0.7932, 0.8300, 0.7928, 0.7503, 0.8270, 0.8106, 0.8048,\n",
            "        0.7845, 0.8155, 0.8155, 0.2536, 0.8103, 0.7783, 0.7538, 0.8259, 0.8319,\n",
            "        0.8387, 0.8335, 0.7858, 0.8124, 0.8057, 0.8129, 0.7896, 0.8417, 0.8427,\n",
            "        0.7926, 0.8139, 0.8051, 0.8048, 0.7916, 0.7862, 0.7843, 0.8062, 0.8166,\n",
            "        0.7787, 0.8195, 0.8162, 0.7995, 0.8251, 0.8179, 0.7671, 0.8214, 0.8367,\n",
            "        0.8063, 0.8083, 0.8073, 0.7929, 0.8413, 0.7784, 0.8074, 0.8027, 0.7906,\n",
            "        0.7903, 0.8173, 0.8231, 0.8217, 0.8307, 0.8203, 0.7904, 0.7832, 0.7669,\n",
            "        0.8237, 0.7877, 0.8320, 0.8314, 0.7896, 0.8090, 0.8206, 0.8156, 0.7982,\n",
            "        0.8292, 0.7918, 0.7818, 0.8250, 0.7518, 0.7979, 0.8007, 0.8207, 0.7739,\n",
            "        0.8172, 0.8052, 0.8029, 0.7933, 0.8297, 0.8332, 0.7969, 0.8215, 0.7459,\n",
            "        0.7839, 0.8077, 0.7897, 0.8376, 0.8132, 0.7691, 0.8116, 0.7727, 0.7511,\n",
            "        0.8192, 0.8161, 0.8058, 0.8114, 0.8242, 0.8138, 0.7840, 0.7908, 0.8491,\n",
            "        0.8326, 0.7811, 0.8120, 0.8192, 0.8116, 0.8013, 0.8118, 0.8114, 0.8063,\n",
            "        0.8085, 0.7608, 0.8015, 0.8005, 0.8123, 0.8252, 0.8310, 0.7786, 0.8160,\n",
            "        0.8289, 0.8081, 0.8032, 0.7974, 0.8163, 0.7769, 0.7992, 0.7744, 0.7747,\n",
            "        0.8085, 0.8226, 0.8117, 0.7959, 0.8174, 0.8196, 0.7900, 0.8211, 0.8295,\n",
            "        0.8148, 0.7874, 0.8066, 0.8196, 0.7752, 0.8119, 0.7833, 0.8285, 0.8109,\n",
            "        0.8055, 0.8034, 0.8000, 0.8093, 0.3332, 0.8061, 0.8162, 0.7930, 0.8070,\n",
            "        0.8315, 0.8332, 0.8237, 0.7884, 0.7710, 0.8421, 0.8047, 0.7888, 0.8172,\n",
            "        0.8012, 0.7979, 0.8478, 0.8192, 0.8070, 0.7819, 0.7937, 0.8314, 0.7808,\n",
            "        0.8165, 0.8397, 0.7882, 0.7909, 0.8080, 0.8026, 0.8206, 0.8128, 0.7738,\n",
            "        0.8048, 0.8282, 0.8260, 0.8000, 0.8183, 0.8027, 0.8006, 0.8062, 0.8043,\n",
            "        0.7940, 0.8297, 0.8014, 0.7936, 0.8204, 0.7846, 0.7910, 0.7954, 0.8107,\n",
            "        0.8333, 0.7498, 0.7903, 0.7935, 0.7991, 0.8034, 0.8218, 0.8086, 0.8293,\n",
            "        0.7960, 0.7717, 0.8300, 0.7787, 0.8352, 0.8340, 0.8228, 0.8190, 0.8202,\n",
            "        0.8161, 0.7932, 0.7973, 0.8293, 0.8166, 0.8183, 0.8218, 0.8302, 0.8113,\n",
            "        0.7656, 0.8109, 0.7740, 0.8187, 0.8191, 0.7948, 0.8216, 0.8025, 0.7904,\n",
            "        0.8239, 0.8056, 0.7725, 0.8176, 0.7805, 0.8300, 0.8249, 0.7939, 0.7410,\n",
            "        0.7963, 0.8527, 0.8381, 0.8244, 0.8064, 0.7934, 0.7839, 0.8046, 0.8128,\n",
            "        0.7808, 0.8215, 0.7960, 0.7845, 0.8408, 0.7729, 0.7967, 0.8378, 0.7826,\n",
            "        0.7836, 0.7964, 0.7529, 0.7975, 0.7618, 0.8192, 0.7779, 0.7923, 0.7806,\n",
            "        0.7817, 0.7693, 0.8327, 0.8074, 0.7919, 0.8352, 0.7803, 0.8068, 0.7745,\n",
            "        0.7950, 0.8178, 0.8105, 0.8176, 0.8124, 0.8136, 0.8095, 0.8352, 0.7797,\n",
            "        0.8233, 0.7475, 0.7883, 0.7931, 0.7495, 0.8081, 0.8022, 0.7882, 0.8159,\n",
            "        0.7981, 0.7914, 0.8121, 0.8196, 0.8087, 0.8475, 0.8177, 0.7951, 0.8115,\n",
            "        0.8258, 0.8063, 0.7977, 0.8071, 0.7969, 0.8173, 0.8449, 0.7921, 0.8083,\n",
            "        0.8310, 0.8090, 0.8157, 0.7836, 0.7831, 0.7968, 0.7787, 0.8153, 0.8320,\n",
            "        0.7884, 0.8291, 0.7977, 0.7856, 0.7711, 0.7059, 0.8316, 0.8299, 0.7995,\n",
            "        0.8171, 0.7885, 0.7962, 0.7963, 0.8245, 0.7558, 0.8110, 0.8218, 0.7989,\n",
            "        0.1384, 0.8331, 0.8168, 0.7994, 0.8275, 0.8207, 0.8063, 0.7935, 0.7783,\n",
            "        0.7950, 0.8143, 0.8227, 0.8296, 0.8086, 0.7636, 0.8048, 0.8131, 0.7752,\n",
            "        0.8219, 0.7901, 0.8131, 0.8068, 0.8072, 0.7957, 0.7979, 0.8126, 0.8238,\n",
            "        0.8145, 0.8305, 0.7915, 0.8464, 0.8283, 0.7804, 0.7945, 0.8148, 0.7782,\n",
            "        0.6472, 0.8075, 0.8154, 0.7994, 0.8202, 0.8150, 0.8094, 0.7835, 0.7966,\n",
            "        0.7961, 0.8114, 0.8266], device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.9.output.LayerNorm.bias', Parameter containing:\n",
            "tensor([-4.1362e-02, -2.1139e-02, -7.5811e-02, -8.0979e-03, -9.7905e-02,\n",
            "        -6.4240e-02, -1.1137e-01, -8.8885e-02,  4.2872e-03,  1.0240e-02,\n",
            "        -6.5854e-02, -8.0492e-02,  2.9897e-02, -6.2206e-02, -4.8446e-02,\n",
            "        -1.0729e-01, -1.0719e-01, -6.6133e-02,  1.2595e-02, -4.8516e-02,\n",
            "        -1.1542e-01, -2.8713e-02, -8.3221e-02, -1.6142e-01, -5.5506e-02,\n",
            "        -8.8782e-02, -6.1462e-02, -5.6109e-03,  2.6672e-03, -5.5647e-02,\n",
            "        -7.5772e-02, -4.6467e-02, -1.1253e-01,  4.3458e-04, -1.2891e-02,\n",
            "        -1.8985e-02,  1.8273e-02, -1.0369e-02, -6.9231e-02,  4.4604e-02,\n",
            "        -3.7813e-03,  5.2488e-02, -1.1338e-02, -3.9193e-02, -3.9279e-02,\n",
            "        -2.4902e-02, -7.4043e-02, -4.3355e-02, -2.8837e-02, -2.3977e-02,\n",
            "         7.8040e-02, -5.5550e-02, -1.1675e-01, -6.7734e-04, -8.5435e-02,\n",
            "        -8.8475e-02, -5.6222e-02,  2.1747e-02,  1.2484e-02,  2.7954e-02,\n",
            "         9.6641e-03, -9.6321e-02, -4.1679e-02, -3.9307e-02, -2.6504e-02,\n",
            "        -8.5320e-02, -4.7608e-02, -6.0109e-02,  2.2232e-02,  1.1919e-02,\n",
            "         4.3061e-02, -3.8129e-02,  1.1741e-02, -4.4939e-02, -6.3287e-02,\n",
            "        -2.3888e-02, -3.0544e-02, -3.1179e-02, -6.8521e-02, -1.0757e-01,\n",
            "        -5.8121e-02, -7.9392e-02, -5.0252e-02, -8.0455e-02, -5.2742e-02,\n",
            "        -5.2701e-02, -5.7354e-02, -2.9069e-02,  2.4157e-02, -1.2187e-01,\n",
            "         5.8234e-03,  3.1140e-03, -1.2781e-01, -3.3020e-02, -7.0746e-02,\n",
            "         2.8580e-02, -9.6201e-02,  1.4824e-02, -7.1823e-02, -5.7498e-03,\n",
            "         2.6578e-03, -4.0516e-02, -9.2663e-02, -1.9174e-02, -2.9189e-02,\n",
            "        -4.1956e-02, -1.1289e-02,  1.0686e-02,  1.2404e-03, -3.6887e-02,\n",
            "        -1.7767e-03, -7.6929e-03, -1.9454e-02,  1.5242e-02, -2.4329e-02,\n",
            "        -4.3963e-02,  4.1254e-03,  4.0596e-03, -2.0910e-02, -4.1460e-02,\n",
            "        -1.0514e-02, -2.5199e-02, -3.9839e-02, -3.7927e-01, -5.3645e-02,\n",
            "         3.2562e-02, -4.4924e-02, -7.3146e-02, -1.2187e-02, -3.3652e-02,\n",
            "        -2.1251e-02, -8.9068e-02, -8.9228e-04,  5.0278e-03,  7.5714e-03,\n",
            "        -1.9635e-02, -4.8386e-02,  3.7448e-03, -3.8368e-03,  1.1127e-03,\n",
            "         1.9572e-02, -3.6723e-02, -4.0977e-02, -1.1455e-01, -8.5390e-02,\n",
            "        -1.6222e-02, -9.9858e-03,  5.8480e-02, -1.5125e-02, -1.2109e-01,\n",
            "         1.6111e-02, -1.0311e-01,  4.8411e-02, -5.3147e-02, -1.3798e-02,\n",
            "        -7.1730e-02, -4.0161e-02, -3.3945e-02, -3.8025e-02, -7.8987e-03,\n",
            "        -6.0108e-02,  9.9554e-03,  5.7799e-03, -6.7667e-02, -1.0783e-01,\n",
            "        -2.7267e-02, -1.3513e-01, -5.5996e-02,  4.8432e-02, -4.2904e-02,\n",
            "        -1.5741e-03,  2.0023e-02, -5.2984e-02, -7.2217e-02, -4.1452e-02,\n",
            "        -3.8336e-02, -1.0602e-01, -3.8518e-03, -3.3556e-02, -8.4335e-02,\n",
            "        -1.9414e-01, -1.0884e-01,  3.8163e-02, -3.4392e-02, -7.1212e-02,\n",
            "        -3.3847e-02, -1.5423e-01, -5.1650e-02, -1.0664e-01, -1.5648e-02,\n",
            "        -8.8657e-03,  4.4095e-02, -9.4535e-02, -1.7994e-02, -8.6558e-02,\n",
            "        -4.4600e-02, -3.6234e-02, -1.8038e-02,  7.7291e-03, -2.1728e-02,\n",
            "         5.1011e-03, -3.8045e-02, -4.8390e-02, -1.4892e-02, -1.2812e-02,\n",
            "        -1.1820e-01, -4.9538e-02,  2.2665e-02, -5.9120e-02, -5.1434e-02,\n",
            "         1.5802e-03, -2.3590e-02, -9.8635e-02, -9.1213e-02, -3.5988e-02,\n",
            "        -3.7719e-02,  3.5263e-02,  7.8793e-03, -1.0256e-01, -1.1728e-02,\n",
            "        -6.5399e-02, -8.2471e-02, -3.8857e-02, -6.5676e-02, -1.2306e-02,\n",
            "         4.3430e-01, -6.2846e-02,  3.0290e-02, -1.0564e-01, -2.7240e-02,\n",
            "         1.1110e-02, -4.3579e-02, -1.5262e-01,  5.8040e-03, -1.5194e-02,\n",
            "        -2.8451e-02, -3.2047e-02, -3.3394e-02, -9.8465e-02,  7.7444e-02,\n",
            "        -3.9825e-02, -6.5182e-02,  6.2047e-02, -9.2174e-02, -3.6456e-02,\n",
            "        -3.1621e-03, -5.8628e-02, -2.2820e-02,  2.5846e-02,  3.5630e-02,\n",
            "         9.8710e-04, -1.1307e-01, -1.3313e-02,  1.7162e-02, -7.5754e-02,\n",
            "        -9.8890e-03, -5.1323e-03,  4.0096e-02, -2.6852e-02, -7.1099e-02,\n",
            "        -6.3135e-02, -1.7889e-02, -2.4470e-02, -4.3215e-03, -2.5126e-02,\n",
            "         8.1877e-02, -4.7567e-02, -4.6521e-02, -7.1673e-02, -2.2439e-02,\n",
            "        -8.7681e-03, -4.7087e-02,  1.4264e-02, -4.8559e-02, -1.2411e-02,\n",
            "        -3.3777e-02, -5.1161e-02, -4.4873e-02, -4.8985e-03,  9.3010e-03,\n",
            "        -5.4595e-02, -1.0051e-01, -5.2337e-02, -1.6020e-03,  7.2563e-02,\n",
            "        -4.4614e-02, -1.3594e-01, -5.9025e-02, -4.3979e-02, -1.7385e-02,\n",
            "        -3.3880e-02, -4.9312e-02,  2.7752e-02, -3.7881e-02, -9.5431e-02,\n",
            "        -5.4052e-02, -2.1057e-02, -8.0697e-02, -9.2981e-02,  1.1323e-02,\n",
            "        -1.9936e-02, -6.2868e-02, -2.3075e-02, -7.0953e-02, -1.3957e-01,\n",
            "        -2.8137e-02, -1.6601e-02,  9.3149e-03, -4.1541e-01, -8.8792e-03,\n",
            "        -8.1566e-03,  5.3763e-03, -8.6175e-02, -3.5735e-02, -1.6793e-02,\n",
            "        -8.2754e-02, -5.3099e-02, -2.7819e-02, -3.3768e-02,  3.1374e-03,\n",
            "        -8.3585e-03, -1.1262e-01, -6.6105e-02,  8.8857e-02, -1.3025e-02,\n",
            "        -1.0040e-02, -5.3616e-02, -9.9734e-02,  6.4510e-03,  3.3691e-02,\n",
            "        -8.8595e-02, -7.2246e-02, -7.5322e-02, -1.2594e-01, -2.3822e-02,\n",
            "         2.0735e-03,  1.3872e-02, -2.7440e-02, -2.0588e-02, -5.4746e-03,\n",
            "        -2.8957e-02, -7.3782e-02, -5.3082e-02, -7.3429e-02, -4.4103e-02,\n",
            "         2.5730e-03,  1.1183e-02,  1.2541e-01, -5.3598e-03,  6.3142e-02,\n",
            "        -2.0548e-02, -2.2290e-02, -1.4678e-01, -2.7653e-02, -1.8403e-02,\n",
            "        -5.6971e-03, -5.1638e-02, -7.1981e-02, -5.9321e-03, -4.1501e-02,\n",
            "        -1.7628e-02,  6.8465e-03, -7.9499e-02, -2.9736e-02, -6.9518e-02,\n",
            "        -2.3201e-02,  4.0839e-04, -2.5406e-02,  1.6187e-02, -1.6235e-02,\n",
            "         7.2862e-02, -1.9922e-02, -4.6751e-02, -2.7472e-03,  4.8653e-02,\n",
            "        -2.7372e-02, -9.0514e-02, -2.6969e-02,  4.2322e-02, -4.6600e-02,\n",
            "        -2.7588e-02,  2.5768e-01,  7.3218e-03,  3.6501e-02, -1.4214e-01,\n",
            "         1.9775e-03,  2.6063e-03, -8.5092e-02, -3.9450e-02,  3.2309e-02,\n",
            "        -3.1081e-02, -3.9204e-02, -4.0745e-02,  4.4976e-02, -3.5110e-02,\n",
            "        -2.9960e-02,  3.7239e-02,  2.4120e-02, -1.1123e-01,  2.0888e-02,\n",
            "         1.0318e-02, -2.9862e-02, -7.1648e-02,  9.5027e-03,  1.7330e-02,\n",
            "         5.9332e-02, -7.5450e-02,  5.0240e-02, -1.1350e-01, -3.4397e-03,\n",
            "        -4.1896e-02,  2.5440e-03, -2.2849e-02,  7.7766e-03, -3.0966e-02,\n",
            "        -7.3168e-02, -4.5329e-02,  6.6936e-03, -5.9718e-02, -4.3592e-02,\n",
            "        -3.6960e-02,  2.2038e-02, -6.0747e-02,  1.1733e-02,  2.1222e-02,\n",
            "        -6.0428e-02,  2.9012e-02,  1.1516e-02,  6.7814e-02, -8.9485e-02,\n",
            "         1.7257e-03,  1.7998e-02,  1.3386e-02, -6.4914e-02, -4.2015e-03,\n",
            "        -7.0684e-02, -3.2654e-02,  1.6183e-02, -2.4565e-02, -2.1684e-02,\n",
            "        -5.0920e-02, -5.4181e-02, -9.0532e-02,  3.4629e-02, -1.7536e-02,\n",
            "         2.8171e-02,  7.9664e-02, -6.9862e-03, -3.9849e-02, -7.6986e-04,\n",
            "        -1.0305e-01, -1.7982e-02, -1.0644e-01,  3.3085e-02, -1.0057e-01,\n",
            "        -1.1000e-01,  1.8379e-02, -7.5758e-02,  6.9268e-02,  9.0715e-02,\n",
            "        -5.1281e-02,  2.9102e-02, -3.7505e-02, -7.9620e-02, -1.3072e-01,\n",
            "         9.0839e-03, -8.3967e-02,  4.9114e-02, -4.9140e-02, -5.2445e-02,\n",
            "        -6.2678e-02, -6.3389e-02, -2.3650e-02, -1.1224e-01, -1.0199e-02,\n",
            "         1.7565e-02, -2.2788e-02, -7.4345e-02,  2.6721e-02,  3.4486e-02,\n",
            "        -1.9545e-02, -3.0934e-02,  9.9406e-03, -6.9302e-02, -8.4427e-03,\n",
            "         2.0471e-02, -6.0629e-02, -1.2793e-01,  1.3341e-02, -4.0084e-02,\n",
            "         4.4260e-03, -2.9441e-03, -5.2052e-02, -3.7910e-02, -7.0367e-02,\n",
            "        -1.7955e-02,  3.0577e-02, -5.3963e-02, -8.4500e-02, -1.0929e-01,\n",
            "        -2.1843e-02, -6.7049e-02, -1.7152e-02,  1.1544e-02, -3.6903e-02,\n",
            "        -4.1726e-02,  1.7800e-02, -6.0652e-02,  8.4222e-03, -6.5603e-03,\n",
            "         2.0868e-02, -1.0623e-02, -2.9781e-02,  2.0752e-02, -4.4583e-02,\n",
            "        -8.6989e-02, -1.3837e-02, -1.1138e-02,  5.2651e-02, -3.5046e-02,\n",
            "        -1.3963e-02, -6.2696e-02, -8.8634e-02, -1.3488e-02, -7.9394e-04,\n",
            "        -1.1459e-02, -4.1579e-01, -2.9550e-03, -7.5688e-02, -5.7328e-02,\n",
            "        -1.3988e-02, -3.2999e-02, -1.6646e-02, -6.0204e-02, -5.6912e-02,\n",
            "         4.9636e-02, -4.6067e-02,  1.4898e-03, -7.1886e-02,  1.3885e-01,\n",
            "        -7.0987e-02, -1.2024e-02, -4.3055e-02, -1.7132e-02,  4.4137e-02,\n",
            "         8.6884e-02, -3.8044e-03, -1.3646e-02, -1.2310e-01, -9.4855e-02,\n",
            "         4.3715e-03,  4.0915e-02, -6.6918e-02, -2.9787e-02, -2.3023e-02,\n",
            "        -3.7373e-03, -2.5459e-02, -1.0892e-01, -4.6046e-02,  7.2752e-03,\n",
            "         1.7191e-02,  2.6607e-02, -1.1708e-01,  1.0959e-02,  1.0620e-02,\n",
            "        -3.5897e-02, -5.3323e-02, -5.8725e-02, -1.0378e-02, -4.7706e-02,\n",
            "         1.8659e-03, -6.0888e-02, -1.0792e-01, -3.3674e-02,  1.8794e-02,\n",
            "        -6.7407e-02, -5.3001e-02,  7.0236e-02, -6.1790e-02,  4.3988e-02,\n",
            "         8.6426e-03, -8.1173e-02, -4.3926e-02, -2.9863e-02, -9.4102e-04,\n",
            "        -1.0606e-02, -2.0545e-01,  8.4482e-03, -1.6001e-02, -2.0171e-02,\n",
            "        -3.7645e-02, -2.5153e-02, -5.8750e-02,  1.0159e-02, -3.9517e-02,\n",
            "        -3.4803e-02, -2.8174e-02, -4.9856e-02, -5.0199e-02, -2.9427e-02,\n",
            "        -4.1125e-02, -1.2681e-02,  2.4346e-02, -6.0683e-02, -8.4800e-02,\n",
            "        -9.0528e-02, -3.9378e-02, -4.6350e-02,  2.7161e-03, -1.5785e-02,\n",
            "        -8.0687e-02,  2.2171e-02, -4.7229e-03, -5.2977e-02, -1.8089e-02,\n",
            "         1.1580e-02,  9.4699e-03, -1.2036e-02,  2.1710e-02,  3.5645e-02,\n",
            "        -1.2094e-01, -4.0135e-02, -7.4306e-02, -9.7870e-03, -5.3679e-02,\n",
            "        -4.7410e-02, -1.1074e-01,  3.4376e-02, -7.2576e-02, -5.7748e-02,\n",
            "        -1.0441e-01, -4.5554e-02, -6.4857e-02, -1.4859e-01, -3.1018e-02,\n",
            "        -4.7611e-02, -6.7080e-02, -6.3054e-02, -8.6482e-02, -5.7229e-02,\n",
            "         4.1857e-02, -1.1226e-01,  2.6263e-03,  5.0956e-02, -2.2680e-02,\n",
            "         7.5209e-02, -3.6766e-02, -1.1250e-01, -4.2424e-02, -1.0365e-01,\n",
            "        -7.9786e-02, -6.1874e-02, -1.0176e-01, -7.9025e-02, -1.5940e-02,\n",
            "        -2.2612e-02, -9.9498e-02, -8.6026e-02, -3.7958e-02, -2.1595e-03,\n",
            "        -6.3151e-02,  3.0186e-02, -8.7673e-02, -2.3391e-02, -3.7365e-02,\n",
            "        -5.6762e-02, -3.4761e-02, -1.2715e-01, -1.0902e-01, -9.3632e-02,\n",
            "        -1.0694e-01, -1.4135e-02,  2.1508e-02, -1.0193e-02, -7.1002e-02,\n",
            "         1.5634e-02, -9.1708e-02, -7.8840e-02, -6.2213e-02, -1.4059e-03,\n",
            "        -2.5708e-02, -1.9438e-02, -5.8914e-02, -2.4784e-04,  3.0568e-03,\n",
            "        -2.8721e-02, -4.9874e-02, -9.1331e-02,  3.1717e-02, -4.1207e-02,\n",
            "         4.6977e-03, -1.8336e-02, -3.8194e-02, -3.4620e-02, -1.6672e-02,\n",
            "        -6.0389e-02,  4.9552e-02, -4.2689e-02,  1.7019e-02, -7.2453e-02,\n",
            "        -7.7890e-02, -1.2995e-02,  2.8752e-02, -1.9292e-02, -9.7709e-02,\n",
            "        -6.3426e-02, -1.6326e-01, -6.6448e-02, -4.3279e-02, -1.0640e-01,\n",
            "        -3.5923e-02, -3.9645e-02, -1.0021e-01, -8.0595e-02, -7.7919e-03,\n",
            "        -5.2596e-02,  5.7838e-02, -6.7250e-02, -4.0111e-02, -1.5037e-02,\n",
            "         2.5015e-01, -2.8034e-02, -1.3051e-02, -3.6914e-02, -8.1649e-02,\n",
            "        -6.9879e-03,  1.7801e-02, -9.0037e-03,  3.4770e-02,  2.9013e-02,\n",
            "        -4.7406e-02, -2.4532e-02, -2.6549e-02, -1.0197e-01, -1.3016e-01,\n",
            "        -1.9337e-02, -2.1702e-02, -1.0323e-01, -1.7879e-03, -5.0249e-02,\n",
            "        -4.0157e-02, -5.4023e-02,  2.9882e-02,  3.2381e-02, -3.8132e-02,\n",
            "        -1.0330e-01, -6.2905e-02,  2.3009e-02, -5.4649e-02, -5.7878e-02,\n",
            "        -2.2109e-02, -1.4713e-02, -8.6525e-03, -7.3654e-02,  1.3044e-02,\n",
            "        -4.2456e-02,  1.4466e-01, -6.9343e-02, -2.6265e-02,  3.4474e-02,\n",
            "        -5.8037e-02, -2.6273e-02, -7.6483e-02,  4.9269e-02,  2.7771e-02,\n",
            "        -1.5530e-02, -2.2757e-02, -2.3138e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.10.attention.self.query.weight', Parameter containing:\n",
            "tensor([[ 7.1212e-04,  3.7436e-02, -8.1037e-03,  ..., -6.3732e-02,\n",
            "         -9.5217e-04,  9.4325e-03],\n",
            "        [-8.5306e-03,  2.1923e-02,  3.7563e-02,  ..., -2.5715e-02,\n",
            "          4.9945e-02,  1.4635e-02],\n",
            "        [ 1.7770e-02, -4.4673e-02,  2.7443e-02,  ...,  5.2510e-05,\n",
            "         -2.2302e-02, -2.1553e-02],\n",
            "        ...,\n",
            "        [-2.2598e-03, -1.3400e-02,  1.9143e-02,  ..., -7.8677e-02,\n",
            "          3.4674e-02, -2.0308e-02],\n",
            "        [-3.6355e-02,  2.1299e-02,  9.1749e-03,  ..., -2.3351e-02,\n",
            "          4.4595e-02, -1.0835e-01],\n",
            "        [ 7.1196e-02, -2.0205e-02,  5.8498e-02,  ...,  1.0116e-01,\n",
            "          1.8773e-02, -6.1368e-02]], device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.10.attention.self.query.bias', Parameter containing:\n",
            "tensor([-3.6676e-02, -1.4497e-02, -3.8229e-02,  1.1834e-03, -5.4898e-02,\n",
            "        -6.6086e-02, -1.0283e-02,  6.3277e-02, -1.7615e-02, -4.0525e-02,\n",
            "        -4.5876e-03,  2.2352e-02, -2.2756e-02,  7.4079e-02,  9.4136e-03,\n",
            "        -3.4196e-02,  6.0986e-02,  2.0791e-02,  3.0050e-02, -2.4763e-02,\n",
            "         1.1391e-01,  5.7480e-02,  4.4814e-02,  9.5734e-03,  8.2847e-02,\n",
            "        -3.6637e-02, -4.6617e-02, -7.7708e-02, -6.1253e-02, -1.6416e-02,\n",
            "         6.0051e-02, -1.6581e-02,  1.3533e-02, -3.4358e-02,  3.3707e-02,\n",
            "        -2.6728e-02, -5.1416e-02, -7.5241e-04,  1.1595e-01,  1.1904e-02,\n",
            "        -2.1282e-02,  6.6300e-02, -1.8813e-02,  1.2436e-02, -2.3964e-02,\n",
            "         3.9917e-02, -3.8244e-02,  9.8298e-03,  2.5534e-02, -4.9707e-02,\n",
            "         2.4500e-02,  1.7851e-02, -1.1556e-02, -3.3669e-03, -4.9233e-02,\n",
            "        -2.7483e-02,  2.7150e-02, -2.5129e-02, -5.5339e-02, -3.6421e-02,\n",
            "        -8.4919e-02,  4.4152e-02,  2.5176e-02,  5.0405e-02, -3.2279e-02,\n",
            "         5.1725e-01,  3.1538e-01,  9.1517e-02, -7.7288e-02, -4.9325e-02,\n",
            "        -3.4406e-01, -4.2674e-01, -3.9943e-01,  2.6362e-01, -5.3500e-02,\n",
            "        -3.4239e-02,  2.7550e-01, -8.8158e-02, -1.0656e-01,  4.0385e-02,\n",
            "        -5.9642e-01,  4.1534e-01, -2.4191e-01,  2.4182e-01, -1.0772e-01,\n",
            "        -2.1232e-01, -1.9627e-01, -9.7312e-01, -5.6273e-01, -5.6076e-01,\n",
            "         6.5617e-02,  6.9254e-01,  2.9777e-01,  8.4928e-01,  1.2793e-01,\n",
            "        -4.3920e-01, -3.8617e-01,  2.0038e-01,  1.2056e-02, -1.2885e-01,\n",
            "        -6.0275e-01,  1.8397e-01, -1.2694e-01, -2.1938e-01,  6.0315e-02,\n",
            "         1.6924e-01,  2.8922e-02,  8.8135e-01, -5.2385e-01,  3.6702e-01,\n",
            "        -1.4968e-01, -2.7139e-01,  2.0185e-01, -8.5037e-01, -4.2382e-01,\n",
            "        -2.4636e-01, -6.7376e-01, -1.9075e-01,  2.3266e-01,  6.5202e-01,\n",
            "        -2.5832e-01,  2.5141e-01,  8.3373e-02, -3.6621e-01, -1.8366e-01,\n",
            "         9.2316e-03,  3.5688e-01, -5.1173e-01, -3.9082e-02,  2.7961e-02,\n",
            "        -5.4518e-03, -1.5936e-01,  7.4766e-03,  9.2788e-02, -9.5824e-02,\n",
            "        -7.4753e-02, -8.9883e-02, -1.8775e-01, -2.6499e-01,  5.0678e-02,\n",
            "        -6.9718e-02,  1.0717e-02,  1.7259e-01,  9.3599e-02,  1.1339e-01,\n",
            "        -1.2010e-01,  6.8740e-02,  1.7185e-02, -3.3607e-02, -4.4961e-02,\n",
            "        -1.6610e-01, -9.8654e-02, -9.1762e-02, -3.5900e-01,  8.3867e-02,\n",
            "         2.5942e-02, -3.1374e-01,  7.3607e-02, -2.8122e-02, -1.6737e-02,\n",
            "         7.7106e-02,  3.4752e-02,  1.4944e-01, -9.4777e-02,  3.7560e-01,\n",
            "         9.0495e-02, -1.7742e-02,  6.4571e-02,  7.2401e-02,  2.3713e-02,\n",
            "        -9.8992e-02, -2.6287e-01,  1.8729e-01,  2.5469e-02,  4.0502e-02,\n",
            "        -2.6387e-01,  1.4363e-01,  2.2296e-01,  3.6160e-01, -4.0661e-02,\n",
            "        -9.3315e-02, -1.4177e-01, -9.6329e-02, -1.1685e-01, -1.6268e-01,\n",
            "        -1.0495e-01, -7.6147e-02, -5.0305e-02,  1.8191e-01,  1.3517e-01,\n",
            "        -9.8461e-02, -1.4344e-01,  2.6707e-02, -5.7844e-03,  1.6136e-02,\n",
            "         3.8577e-02,  6.7770e-03,  1.6096e-02, -1.6162e-01, -3.2251e-02,\n",
            "         7.4246e-03, -3.2815e-02, -2.3777e-02,  2.0309e-02, -5.0988e-03,\n",
            "         3.7856e-02,  1.2099e-02,  2.8584e-02,  7.1429e-03,  4.6537e-02,\n",
            "        -2.7939e-02,  2.6535e-03,  1.5291e-04,  1.6220e-03,  3.5484e-04,\n",
            "        -1.2698e-03, -5.1362e-03,  4.8101e-02, -5.1189e-02, -1.1553e-02,\n",
            "        -3.9562e-02,  1.2229e-02,  1.9334e-02, -1.8756e-02, -3.4825e-02,\n",
            "         5.6503e-03, -1.0227e-02,  4.6868e-02, -5.2604e-02, -1.9529e-03,\n",
            "        -7.9708e-02,  6.5829e-03,  1.1274e-02,  1.3924e-02, -3.2394e-04,\n",
            "         1.9719e-02, -3.0671e-02,  1.5449e-02, -7.6544e-03, -1.0178e-02,\n",
            "        -6.1795e-04, -4.7540e-02,  2.7070e-02,  5.0625e-03, -2.4552e-02,\n",
            "         1.5775e-02, -9.4018e-03,  9.3730e-03, -2.1154e-02,  2.2448e-01,\n",
            "        -7.5260e-02,  1.9674e-02,  2.8595e-02,  1.2666e-01, -1.8409e-02,\n",
            "        -1.4414e-02, -3.2591e-02,  2.2882e-02,  6.1680e-02,  2.7876e-04,\n",
            "        -8.5153e-02, -1.4246e-01, -4.9553e-01, -1.8867e-03, -6.1740e-02,\n",
            "        -6.6917e-02, -1.7902e-02,  1.7867e-01,  7.0403e-02,  5.3641e-02,\n",
            "        -4.7796e-01, -4.1694e-02,  9.0810e-02,  1.0040e-01,  1.8740e-01,\n",
            "         1.9844e-01, -1.9279e-01, -5.5545e-02,  6.1532e-03, -1.8530e-01,\n",
            "        -5.2766e-01, -4.9558e-01, -4.3685e-02, -1.5085e-02,  1.2192e-01,\n",
            "        -1.1756e-02, -5.9354e-02,  4.8515e-02,  1.1279e-01, -2.6360e-01,\n",
            "        -3.5071e-02, -1.3980e-02, -1.5387e-03, -3.5413e-03, -3.9907e-02,\n",
            "        -2.8938e-01,  4.3386e-02, -2.9567e-01,  1.1038e-01,  9.2879e-02,\n",
            "        -3.4024e-01,  1.6324e-02, -1.9134e-01,  6.8955e-02,  1.1309e-01,\n",
            "         5.4174e-02,  3.5320e-02,  1.0750e-01,  2.7687e-01,  2.8393e-02,\n",
            "        -1.8880e-01,  2.2631e-01, -9.1082e-02,  8.3804e-03, -3.2280e-02,\n",
            "         1.9409e-01,  2.3351e-01,  7.2624e-02, -7.0471e-02,  1.9323e-01,\n",
            "        -5.4754e-01, -1.7651e-01,  6.0117e-01,  2.0444e-01,  1.6887e-02,\n",
            "        -1.7484e-01,  7.7730e-02, -3.0719e-01,  2.8470e-01, -2.4057e-01,\n",
            "        -4.2552e-02, -1.8666e-01,  8.2095e-02, -2.3497e-01,  1.5527e-01,\n",
            "        -9.2018e-03,  2.2057e-02, -4.7594e-01, -8.1566e-01,  7.5547e-01,\n",
            "         1.0632e-01, -8.3864e-01,  8.0297e-01, -5.3292e-03, -6.0082e-01,\n",
            "        -3.2864e-02, -1.2063e-01, -3.3728e-01,  1.7570e-01, -7.3240e-02,\n",
            "        -3.4682e-01, -3.2228e-01, -1.8842e-01,  1.8825e-01,  3.2367e-01,\n",
            "        -1.0805e-01, -3.3671e-01, -2.3307e-01, -2.5989e-01,  6.2850e-02,\n",
            "         1.0372e-01, -3.9301e-01,  1.0514e+00,  1.5401e-01, -1.8647e-01,\n",
            "         4.9909e-02,  3.5124e-02,  1.6945e-01, -2.7831e-01,  1.0684e+00,\n",
            "        -6.0452e-02, -1.1925e-01, -2.3353e-01,  3.1287e-01, -4.5386e-01,\n",
            "        -4.6662e-01, -7.0648e-01, -2.4313e-01, -6.1058e-01,  2.8846e-01,\n",
            "        -3.2776e-01,  1.9120e-01, -5.6931e-02,  3.1565e-01,  1.5460e-01,\n",
            "        -9.3033e-02,  5.4597e-01,  1.5567e-01, -2.7285e-02, -3.2044e-02,\n",
            "         2.2920e-02,  8.2734e-02, -5.9834e-03, -7.3332e-02,  7.0241e-01,\n",
            "        -1.1788e-01,  2.1087e-02,  3.5833e-01, -3.9852e-02, -1.8313e-01,\n",
            "        -1.0973e-01,  6.7136e-02, -1.2771e-01, -1.1298e-01, -2.1797e-01,\n",
            "         2.0707e-01, -8.0772e-03, -3.2468e-01, -1.1986e-01, -2.5774e-01,\n",
            "        -2.5874e-02,  8.5947e-03,  7.8453e-02, -4.3006e-02, -1.4729e-01,\n",
            "         4.3664e-01,  6.8956e-02, -1.5234e-01,  4.9087e-01,  4.5011e-02,\n",
            "         7.0767e-02,  2.7688e-02, -1.7070e-01, -1.3992e-01, -2.1632e-01,\n",
            "        -1.1516e-01,  2.5437e-01, -1.5697e-02,  9.7330e-02,  1.4378e-01,\n",
            "        -2.9128e-01, -9.4858e-02,  7.3918e-03,  9.6405e-02, -2.6798e-01,\n",
            "         5.8345e-03,  3.8105e-01,  4.3693e-01, -1.5936e-01,  3.1014e-01,\n",
            "        -2.3732e-02,  2.5146e-01, -6.9307e-01,  3.4516e-01, -1.5148e-01,\n",
            "         2.3852e-01,  2.5131e-01,  1.1839e-01, -7.6520e-02,  5.6796e-02,\n",
            "         2.2740e-01, -6.8586e-03,  1.1995e-02, -4.7338e-02,  2.0103e-02,\n",
            "        -7.6994e-02, -2.9013e-02,  8.2418e-03,  8.0422e-02, -8.3503e-02,\n",
            "         5.2074e-02, -4.3157e-02, -1.7998e-02, -2.9773e-02, -8.7704e-02,\n",
            "         4.1017e-03, -2.6376e-02, -2.6145e-02, -1.0665e-01,  2.7705e-03,\n",
            "        -5.9064e-02, -3.9609e-02,  1.2494e-01,  2.3574e-02, -2.0911e-02,\n",
            "         1.9897e-01, -1.0499e-01,  7.8466e-02,  5.0771e-02,  6.2194e-02,\n",
            "         6.2985e-02,  3.9984e-02, -4.6588e-02, -6.1262e-02, -4.0192e-02,\n",
            "        -5.3037e-02, -7.7313e-02,  4.6824e-02,  2.7598e-02, -6.4732e-02,\n",
            "        -7.0918e-02, -6.1746e-02, -7.4184e-03,  5.1934e-02,  1.6011e-01,\n",
            "         3.7397e-02, -8.2230e-03, -2.5685e-03, -1.1432e-01, -1.4145e-02,\n",
            "         1.9198e-02,  1.7893e-03, -4.0012e-02,  7.2151e-02, -3.3220e-02,\n",
            "        -3.9576e-02, -4.0817e-03,  3.2345e-03,  3.9684e-03, -1.1273e-01,\n",
            "         1.8606e-02, -2.5588e-03,  5.1042e-02,  8.8128e-02,  4.9168e-02,\n",
            "        -6.8063e-02,  1.4073e-02, -9.9500e-02, -1.4290e-01, -9.3147e-02,\n",
            "        -7.5217e-02,  8.4148e-02,  3.4342e-02, -4.8259e-02,  5.5812e-02,\n",
            "        -1.0971e-01, -2.8572e-02,  7.9434e-02, -4.4745e-02,  1.5693e-01,\n",
            "        -3.6602e-02, -1.5493e-02,  3.4104e-02,  2.0368e-01,  4.0578e-02,\n",
            "        -1.1849e-01, -1.7429e-01, -7.3193e-02,  5.9164e-03,  8.1119e-02,\n",
            "         9.3226e-02,  1.4431e-01,  1.0619e-01,  4.8593e-04,  9.5823e-02,\n",
            "         2.4659e-01, -7.0603e-02, -1.2534e-01, -7.7356e-02,  2.5577e-02,\n",
            "         2.4145e-01,  1.3097e-01, -1.9513e-01,  2.5259e-02, -8.4881e-02,\n",
            "         3.5848e-02,  2.9840e-01, -9.3579e-02, -3.1677e-02,  2.4896e-02,\n",
            "        -1.0908e-01, -2.8209e-01, -1.9974e-01,  2.3098e-02, -1.1324e-01,\n",
            "         1.9289e-02,  1.5003e-01, -2.7591e-01, -6.2483e-03,  1.0658e-01,\n",
            "        -7.6996e-02,  8.4920e-03, -2.9820e-03, -1.4032e-01,  1.2744e-01,\n",
            "        -2.3254e-02,  3.5013e-02,  3.8396e-01, -4.7234e-02, -3.4301e-02,\n",
            "         6.6341e-01,  6.6387e-02,  1.4546e-01, -2.3940e-01,  1.0763e+00,\n",
            "        -6.9414e-01,  4.6188e-01,  3.7988e-01,  4.8126e-01, -5.3142e-02,\n",
            "         7.2297e-01, -7.4483e-01,  4.2585e-02, -2.6484e-01, -3.1656e-01,\n",
            "         2.1765e-02, -5.7106e-01, -1.6304e-01,  3.6541e-01, -7.3599e-01,\n",
            "        -4.5916e-01, -4.5169e-01,  8.9293e-01,  1.5114e-02,  4.6301e-01,\n",
            "         9.4393e-01,  3.2961e-01, -2.4394e-01,  1.0680e-01,  8.8579e-02,\n",
            "         3.4309e-01,  1.7438e-02,  6.8438e-03,  4.1239e-01,  7.3337e-02,\n",
            "         1.0671e-01, -3.4730e-01, -1.1039e-01, -3.9782e-01,  1.8350e-01,\n",
            "         1.1450e-01, -5.9134e-01,  1.0044e+00,  1.9876e-02, -6.1755e-01,\n",
            "        -1.0018e-01,  9.6574e-02,  5.5285e-01, -5.5048e-01, -3.9156e-01,\n",
            "         4.7010e-01,  5.2039e-01,  1.9557e-02, -2.2252e-01,  2.8641e-01,\n",
            "        -7.5915e-01, -6.9513e-02,  6.5966e-02, -3.0074e-01, -1.6357e-01,\n",
            "         2.3629e-02,  4.1242e-02,  4.0894e-02, -6.7828e-03, -2.3744e-02,\n",
            "         3.0827e-02, -2.0887e-02,  3.4592e-02,  3.9809e-02,  1.4730e-02,\n",
            "         1.1941e-02,  5.2029e-02, -3.8518e-02,  1.7108e-02,  2.7907e-02,\n",
            "         1.6488e-02, -1.1684e-03, -1.4101e-02,  2.8589e-02,  2.9665e-02,\n",
            "         3.0325e-02,  8.2166e-03,  2.7454e-02,  2.8607e-03,  5.2948e-02,\n",
            "        -6.7410e-03, -4.1444e-03, -4.5906e-03, -3.0016e-02, -3.9101e-03,\n",
            "        -2.7200e-02,  2.4937e-02, -4.3848e-02,  6.1882e-03,  2.5957e-02,\n",
            "        -2.1802e-02,  3.6864e-02, -8.4708e-04,  1.3786e-04, -2.7332e-03,\n",
            "        -5.6039e-02, -2.7593e-02, -4.1676e-03, -1.1120e-02, -5.0939e-03,\n",
            "        -1.1879e-01,  2.3142e-02,  1.4584e-02,  1.2228e-03,  9.3503e-04,\n",
            "        -4.6743e-03,  1.3130e-01, -2.2952e-02,  5.3496e-02, -2.3156e-02,\n",
            "        -2.1183e-02,  6.5988e-03,  4.6882e-03,  3.9598e-02, -2.5615e-01,\n",
            "         1.9471e-02, -5.7348e-05, -1.6107e-01, -2.3666e-03,  6.1859e-02,\n",
            "         5.2104e-02,  1.1598e-01, -6.9523e-03,  2.6113e-02,  1.8232e-03,\n",
            "        -3.7961e-02, -2.5008e-01, -3.0902e-02,  2.9389e-02,  4.5719e-02,\n",
            "        -3.8667e-02,  3.8771e-02, -7.0226e-02,  1.4924e-01,  1.7715e-01,\n",
            "        -3.2035e-02, -4.0235e-02, -3.7918e-02, -4.8772e-02,  5.3331e-02,\n",
            "        -2.9140e-01, -1.1118e-01,  1.3034e-02,  1.0191e-01, -2.4598e-02,\n",
            "         3.4927e-02,  2.8932e-02, -5.8911e-02, -8.0538e-02,  1.9185e-02,\n",
            "        -8.9750e-03, -4.6281e-02,  7.3386e-02, -4.7614e-02, -4.3294e-02,\n",
            "        -4.9458e-01,  4.3920e-02, -9.7529e-02,  5.1268e-02, -2.0372e-01,\n",
            "        -5.4626e-02, -1.1206e-02,  2.0399e-02,  5.9959e-02, -1.4725e-02,\n",
            "         1.6513e-01,  1.2975e-01, -5.0578e-02, -9.2773e-02,  1.0862e-02,\n",
            "        -4.3261e-01, -6.4384e-02, -7.1346e-02, -1.1462e-02,  3.6257e-02,\n",
            "        -1.5810e-02,  9.4803e-03, -1.1244e-02, -1.3266e-01, -3.8921e-02,\n",
            "        -2.6781e-01,  4.4253e-03, -9.8164e-03], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.10.attention.self.key.weight', Parameter containing:\n",
            "tensor([[-0.0494, -0.0304,  0.0180,  ..., -0.0675, -0.0281,  0.0717],\n",
            "        [ 0.0553, -0.0459,  0.0137,  ..., -0.0548,  0.0217,  0.0575],\n",
            "        [-0.0246, -0.0288, -0.0607,  ...,  0.0337, -0.1069,  0.0231],\n",
            "        ...,\n",
            "        [-0.0458,  0.0602,  0.0168,  ...,  0.0462, -0.0164, -0.0458],\n",
            "        [-0.0871,  0.0105,  0.0944,  ..., -0.0401,  0.0398, -0.0676],\n",
            "        [-0.0317, -0.0725, -0.0396,  ...,  0.0228, -0.0564, -0.0245]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.10.attention.self.key.bias', Parameter containing:\n",
            "tensor([-6.9300e-03,  8.5589e-03,  6.7078e-03, -5.2475e-03, -1.7074e-04,\n",
            "         5.0674e-03,  4.1815e-03,  1.8580e-03, -1.0160e-03,  1.1964e-03,\n",
            "         2.7083e-04, -2.4458e-04, -5.4050e-04, -2.5650e-03, -4.7394e-03,\n",
            "         4.8531e-04, -1.8098e-02,  4.8317e-03, -4.4570e-03, -5.3863e-03,\n",
            "         3.1918e-03,  1.9480e-03,  4.9041e-03,  7.7739e-03,  2.8508e-03,\n",
            "        -1.3875e-03,  1.9202e-03, -1.5556e-02, -1.5886e-03,  1.2212e-02,\n",
            "        -1.1864e-02,  2.5167e-04, -5.7444e-03,  4.7266e-03,  4.8462e-03,\n",
            "        -3.6997e-03,  1.6192e-03, -1.2002e-02, -1.1027e-02, -9.1347e-03,\n",
            "         1.5765e-03,  4.0555e-03,  3.5739e-04, -1.6339e-03,  1.0962e-02,\n",
            "        -8.4997e-03, -2.8029e-03, -3.4114e-03, -8.8147e-05,  6.0323e-03,\n",
            "         6.8531e-03, -4.1245e-03,  4.4613e-03,  8.4199e-03, -3.1771e-03,\n",
            "         1.6702e-03,  3.4850e-05, -7.3424e-03, -1.5101e-03, -4.7782e-03,\n",
            "         9.2995e-03, -4.0258e-03,  6.3561e-04, -6.4632e-03, -2.0197e-03,\n",
            "        -9.8148e-04, -1.2540e-02, -7.2752e-03,  1.4507e-03,  1.2296e-04,\n",
            "        -2.4770e-03, -6.0837e-03,  5.7567e-03,  1.4083e-03,  3.1109e-04,\n",
            "        -7.3145e-03, -5.0330e-03, -1.2406e-03,  6.2154e-03, -4.2917e-03,\n",
            "         2.1385e-03,  6.0483e-03,  1.5663e-04, -8.1126e-03,  2.5341e-04,\n",
            "         1.2467e-03, -3.5925e-04,  3.3632e-03,  2.0393e-03,  7.3646e-03,\n",
            "         2.1375e-03, -2.7026e-03, -4.4974e-03,  7.0134e-03,  1.1709e-02,\n",
            "         3.3522e-03,  1.0950e-02,  3.8271e-03, -4.0832e-04,  1.7266e-03,\n",
            "         4.3481e-03, -9.0409e-03, -9.4867e-04, -7.6617e-03, -6.6592e-03,\n",
            "        -6.1604e-03,  1.4187e-03,  2.6609e-03, -1.7144e-03,  3.7914e-03,\n",
            "         3.0868e-03,  5.0570e-03,  8.5467e-03, -9.9423e-03, -2.0796e-03,\n",
            "        -3.2258e-04,  4.1050e-03,  2.7862e-03,  1.2309e-02, -7.2912e-03,\n",
            "         5.8101e-03, -5.7256e-03, -1.5843e-02,  8.8699e-03,  7.5747e-04,\n",
            "        -6.7110e-03, -5.5266e-05,  6.6545e-03,  7.0998e-04, -2.3847e-03,\n",
            "        -3.4828e-03,  9.0933e-03, -3.4571e-03,  1.0093e-03, -1.5860e-03,\n",
            "        -3.0404e-03,  6.4610e-04,  3.7303e-03, -8.0115e-03, -2.7465e-03,\n",
            "        -8.7046e-04,  1.2836e-04, -4.4670e-03,  3.0893e-03,  1.3451e-04,\n",
            "        -7.2371e-03,  4.6840e-03, -4.0207e-04,  5.1145e-03, -2.1060e-04,\n",
            "         1.0095e-02,  8.6286e-03, -6.7095e-03,  2.5879e-03, -2.8491e-04,\n",
            "         8.5179e-03,  1.1921e-03,  9.9142e-04,  4.6654e-04,  5.1127e-03,\n",
            "         6.8374e-03,  3.5552e-04, -1.8421e-03,  4.5094e-03, -1.5912e-03,\n",
            "        -7.3022e-03,  7.6686e-04,  6.0547e-03,  1.0770e-03, -4.5268e-03,\n",
            "         1.1596e-03,  1.6065e-05,  8.2845e-04, -2.2180e-03, -6.7698e-03,\n",
            "        -5.1597e-03, -3.8464e-03, -2.9985e-03, -3.5815e-03,  1.8866e-03,\n",
            "         3.5590e-03,  1.6332e-04,  6.9980e-03,  1.9937e-03,  1.8188e-03,\n",
            "        -1.4298e-02, -2.1265e-03,  1.1429e-02,  1.9449e-05, -2.9942e-04,\n",
            "        -9.0682e-04,  5.7061e-03,  8.6897e-03, -1.2316e-02,  9.2947e-03,\n",
            "         8.1251e-03,  7.0416e-03,  1.7093e-02,  1.4056e-02, -4.1692e-03,\n",
            "        -1.2083e-03, -6.5084e-03,  8.4934e-04, -1.2930e-02, -6.5551e-04,\n",
            "        -9.5311e-03, -4.2694e-03, -4.9882e-03,  8.3678e-03, -1.0326e-02,\n",
            "         2.7191e-03,  3.6803e-03,  5.4991e-03, -2.8498e-03, -9.1039e-03,\n",
            "        -3.6083e-03,  1.2725e-02,  5.0432e-03,  4.9373e-03,  8.4427e-04,\n",
            "         4.8043e-03, -7.2875e-03, -1.1513e-03,  1.4376e-02, -1.9139e-02,\n",
            "        -5.7019e-03,  1.2708e-02, -1.8130e-03, -1.4101e-03,  8.7945e-03,\n",
            "         1.8720e-03,  1.9557e-02, -1.8691e-03,  5.2001e-03,  1.4023e-02,\n",
            "         3.9722e-03,  8.8082e-04, -1.2820e-02,  1.4084e-03,  1.5000e-02,\n",
            "         2.5960e-03, -1.5960e-02, -9.8676e-03,  1.1860e-03, -2.2861e-03,\n",
            "         1.2104e-03,  1.1771e-02, -1.9730e-03, -1.1499e-02, -1.3481e-02,\n",
            "        -1.4296e-02, -2.5724e-04, -4.5529e-03, -3.7688e-03, -5.7932e-03,\n",
            "         7.7856e-03, -3.8337e-03, -1.9320e-03,  6.8045e-03, -4.4586e-03,\n",
            "         1.6528e-03,  3.7164e-03, -1.5865e-02, -5.3465e-03,  3.9016e-04,\n",
            "        -1.2832e-03,  1.4900e-04,  1.5609e-03,  1.0342e-03,  5.5661e-04,\n",
            "         2.0919e-04, -1.2588e-03,  1.7375e-02,  8.1706e-03, -6.6738e-04,\n",
            "         1.0252e-02, -8.8026e-03,  1.3686e-03,  8.6173e-04, -9.1880e-03,\n",
            "         4.0729e-04, -7.5691e-03,  5.0836e-06, -1.6615e-03, -5.0922e-03,\n",
            "        -7.5183e-03, -3.7821e-03, -9.2778e-04,  6.7328e-03, -4.3813e-03,\n",
            "        -1.0221e-03, -1.2368e-02, -1.0867e-02,  5.5248e-03,  4.5581e-03,\n",
            "        -1.0131e-02, -1.6451e-04, -6.0702e-03,  1.8299e-03,  2.7157e-03,\n",
            "        -6.8185e-04,  4.8931e-03,  1.4355e-02,  5.9589e-03, -1.6657e-03,\n",
            "        -8.5590e-03, -2.3917e-03,  2.6150e-03, -3.4086e-03,  3.6453e-03,\n",
            "        -4.2691e-04, -3.9505e-03,  2.1323e-03,  5.6997e-03,  9.1721e-04,\n",
            "         1.9600e-04,  1.0139e-03, -6.3644e-03,  7.4493e-03, -8.3878e-04,\n",
            "         1.6466e-03,  6.4495e-03, -7.1808e-03,  9.9231e-04, -9.3418e-03,\n",
            "         4.9754e-03, -4.9856e-04, -5.6552e-04, -7.2044e-03,  1.5831e-03,\n",
            "         4.8036e-03, -3.9309e-03,  3.7005e-03, -7.0703e-03,  3.9998e-03,\n",
            "        -1.0022e-03, -1.6003e-03, -2.9830e-03, -6.9749e-04,  2.6021e-03,\n",
            "         7.6296e-03, -3.1171e-03,  7.4728e-04, -1.4076e-03, -3.7539e-03,\n",
            "         3.1295e-03, -3.0072e-03, -4.0572e-04, -3.8275e-03,  8.0638e-03,\n",
            "         1.0790e-03, -8.0044e-03, -5.9716e-03, -4.9287e-04,  5.2971e-03,\n",
            "        -3.4904e-03,  5.0437e-03, -6.7597e-03,  5.2795e-03, -6.2792e-04,\n",
            "         1.5572e-03, -1.0748e-02,  8.6681e-03,  5.2530e-03, -7.9259e-04,\n",
            "         4.1247e-03,  5.7562e-03,  1.6704e-03,  1.8165e-03,  1.0173e-02,\n",
            "        -1.2128e-02, -5.2427e-03,  2.0409e-03, -7.1521e-03, -1.9720e-03,\n",
            "         3.4334e-04,  2.2246e-03,  4.6589e-03, -9.2280e-03,  4.1184e-03,\n",
            "         5.7624e-03,  4.5186e-03,  1.1339e-02,  2.3686e-03, -2.6594e-03,\n",
            "        -3.5295e-03,  3.1483e-03, -2.2285e-03,  1.7733e-03,  6.0390e-03,\n",
            "        -4.5070e-03, -6.6122e-04,  2.6287e-04, -4.0881e-03, -2.6016e-03,\n",
            "        -1.5992e-03,  1.2946e-02, -5.1533e-03,  1.7519e-03,  8.5993e-03,\n",
            "        -1.3457e-03,  5.4211e-03,  6.8997e-03, -1.5737e-03,  4.3691e-03,\n",
            "         4.4413e-03, -5.0651e-03, -3.0544e-03, -3.7347e-04, -4.5827e-03,\n",
            "        -8.3630e-04, -2.5575e-03, -4.9778e-03,  9.3016e-03, -4.1158e-03,\n",
            "         5.2344e-03, -3.3605e-03, -6.4183e-03, -3.1329e-03, -4.4335e-03,\n",
            "         6.0293e-03, -7.9370e-03, -2.2736e-03, -1.2955e-03,  2.2187e-03,\n",
            "        -2.2986e-03,  6.5840e-03, -4.0940e-03, -1.4016e-03,  2.6759e-03,\n",
            "        -3.1392e-03,  4.8249e-03, -1.8981e-03,  2.0993e-04, -4.8740e-03,\n",
            "         1.3249e-04,  7.8641e-04,  3.4153e-03,  2.8487e-03, -6.0631e-04,\n",
            "         4.5089e-03,  1.1371e-03, -4.7340e-03, -9.1283e-03, -2.4335e-03,\n",
            "        -6.6363e-03, -3.6126e-03, -3.6649e-03,  4.0616e-03,  6.0948e-03,\n",
            "         1.0095e-02,  2.2510e-03,  4.5820e-03, -3.5534e-03,  1.2988e-03,\n",
            "         2.6603e-03,  5.1994e-03, -2.5577e-03, -3.2454e-03,  5.5617e-03,\n",
            "         3.7364e-03, -2.2635e-03, -4.7702e-03, -5.4390e-04, -3.6517e-03,\n",
            "         4.2101e-03,  1.0642e-03,  7.9035e-03, -1.1293e-03, -2.7839e-03,\n",
            "        -5.6322e-03,  2.6490e-04,  2.1525e-03,  1.4891e-03,  1.0607e-02,\n",
            "        -4.4227e-03, -4.2261e-03, -1.6154e-02, -1.4040e-03,  6.5024e-03,\n",
            "         5.5152e-03, -7.0980e-03,  2.2842e-04, -5.2333e-03,  5.5853e-03,\n",
            "        -1.3156e-03,  4.0778e-03, -8.5845e-03,  3.2263e-04, -4.0613e-03,\n",
            "         1.0848e-02,  4.0395e-03,  3.0994e-03, -7.1049e-04,  6.6042e-03,\n",
            "         2.1754e-03, -1.1331e-02, -1.4605e-02,  1.1207e-03,  9.2770e-03,\n",
            "        -7.9655e-03,  6.8227e-03, -3.4564e-03, -6.8266e-03, -1.1763e-02,\n",
            "         1.1995e-02, -8.9833e-03,  3.5921e-03,  2.0283e-03,  8.9450e-03,\n",
            "         1.5300e-03,  2.6868e-03,  3.9229e-03, -1.8556e-03, -2.2035e-02,\n",
            "        -1.6732e-02,  1.1018e-02,  9.4328e-04, -2.9249e-03,  2.7719e-03,\n",
            "         1.0992e-02, -4.4166e-03, -3.0738e-03,  9.1286e-03, -1.9481e-03,\n",
            "        -2.2179e-03,  1.8511e-03,  9.2610e-04, -1.4791e-02, -5.8868e-03,\n",
            "        -7.4093e-04,  6.3656e-03, -6.6316e-03, -1.0364e-02,  2.5863e-03,\n",
            "        -5.8151e-03,  2.9192e-03, -6.3398e-03, -1.9645e-03, -6.9300e-03,\n",
            "         1.4866e-02, -2.5841e-03,  1.9477e-02,  4.8696e-03,  9.5582e-03,\n",
            "         1.2214e-02,  7.4713e-03,  2.3840e-03,  5.1540e-03, -6.9205e-03,\n",
            "         1.1135e-02,  4.5974e-03, -7.1432e-03, -3.0160e-03,  8.3152e-03,\n",
            "        -1.4420e-02,  2.9595e-03, -6.3967e-03,  4.0334e-03, -4.9541e-03,\n",
            "        -4.3201e-03, -1.8934e-03, -2.1565e-02, -7.3732e-03, -3.3527e-03,\n",
            "         1.7263e-03,  4.6471e-03, -7.3171e-03, -3.7515e-03,  6.9501e-03,\n",
            "        -6.6552e-03,  2.2093e-04, -5.7853e-03, -7.0962e-03,  6.4529e-03,\n",
            "        -7.6430e-03,  6.5924e-03, -2.4543e-03, -6.7676e-04, -4.4349e-03,\n",
            "         1.4539e-02,  3.5131e-04,  1.6586e-03,  4.4028e-03,  3.2808e-03,\n",
            "         6.9976e-03, -7.2592e-03, -6.4432e-03,  1.4150e-03, -7.7481e-03,\n",
            "         1.2922e-03, -1.5928e-03, -5.0782e-03,  1.7602e-04, -5.5456e-03,\n",
            "         1.5327e-03, -9.5769e-03,  2.9619e-03,  7.6047e-03, -4.6032e-03,\n",
            "        -4.4523e-03, -5.2178e-03,  1.4846e-02,  9.7620e-04,  2.3608e-03,\n",
            "        -1.4634e-03, -2.4513e-03,  6.4284e-03, -1.6828e-03, -2.5102e-03,\n",
            "         1.2151e-03,  4.3027e-03,  2.0908e-03,  1.5445e-04, -8.4323e-03,\n",
            "         7.4257e-03, -2.8628e-03,  4.7550e-03, -2.9127e-03,  4.9662e-03,\n",
            "        -6.3313e-03, -1.0529e-02,  7.6072e-03, -7.6190e-03, -1.0754e-02,\n",
            "         6.8550e-03,  5.7422e-03,  3.1198e-05, -7.7430e-03, -8.1641e-03,\n",
            "         3.9294e-03, -7.4940e-04, -5.8984e-04, -6.9939e-03, -1.9477e-03,\n",
            "        -7.3148e-03,  4.0242e-03, -4.6901e-04,  3.6592e-03, -5.9084e-03,\n",
            "         5.7921e-03,  2.3480e-02, -1.1452e-02, -1.0659e-02,  3.8100e-03,\n",
            "        -1.2034e-02, -9.7815e-03, -1.7632e-02, -3.3302e-03,  1.6782e-03,\n",
            "         7.0468e-03, -4.0884e-03,  7.5114e-03,  1.3719e-02,  5.8457e-03,\n",
            "        -4.6209e-03, -1.0341e-03, -2.8693e-03, -5.3472e-03,  6.6903e-03,\n",
            "         7.2242e-03,  7.0865e-03, -1.2820e-03,  1.7918e-02, -8.8557e-03,\n",
            "         4.6244e-03,  4.7741e-03,  9.4395e-04,  1.2661e-03,  1.6341e-02,\n",
            "         1.0434e-02,  1.3533e-02,  2.1026e-03,  1.8231e-03, -3.7463e-03,\n",
            "         1.5056e-03,  1.2786e-02, -1.2236e-02,  1.1722e-02, -5.6654e-03,\n",
            "        -1.0062e-02,  1.6478e-03, -5.7537e-03,  5.5735e-03,  3.6610e-03,\n",
            "        -3.2811e-03, -8.9671e-03,  9.4958e-03,  8.1540e-04, -1.0753e-02,\n",
            "         8.1474e-03, -2.3672e-03,  1.2399e-02, -1.0714e-02, -2.3229e-03,\n",
            "         4.1645e-03,  8.7689e-03, -1.8756e-03, -3.3093e-03, -1.0230e-02,\n",
            "         1.1990e-02, -4.9874e-03, -1.1005e-02, -4.6603e-04,  2.9095e-03,\n",
            "        -2.4737e-03,  1.0091e-03, -3.5922e-03, -3.8659e-03,  9.9002e-03,\n",
            "         1.6129e-04,  1.8465e-03, -4.1638e-03,  2.2505e-03, -1.7540e-03,\n",
            "         7.1755e-03, -1.4535e-03,  1.2793e-04,  1.0095e-03,  7.5493e-03,\n",
            "        -4.2152e-03, -4.7085e-03,  2.0163e-03,  3.1765e-03,  5.4303e-03,\n",
            "         4.1548e-03, -2.3893e-03, -1.6012e-03, -1.9725e-03, -5.7064e-03,\n",
            "         7.5328e-04,  2.3737e-03,  1.1296e-03,  2.1506e-03, -1.9043e-04,\n",
            "        -7.1720e-05, -3.3824e-03, -1.1684e-03, -4.7213e-03, -1.2236e-02,\n",
            "        -6.1418e-04,  2.1929e-03,  1.6242e-03,  3.5141e-03, -3.0162e-03,\n",
            "         1.4377e-03,  2.2871e-03, -3.4077e-03, -5.0907e-03, -1.6790e-03,\n",
            "        -7.6882e-03,  5.4499e-03,  2.6872e-03,  2.1042e-03,  5.8136e-03,\n",
            "        -7.7307e-03,  2.9835e-03, -2.8981e-03, -1.0854e-03,  5.6781e-03,\n",
            "        -1.1336e-03,  5.1574e-03,  9.9243e-03, -1.0162e-02, -1.6332e-03,\n",
            "        -2.0570e-03, -2.1694e-03,  4.1583e-03], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.10.attention.self.value.weight', Parameter containing:\n",
            "tensor([[ 0.0191,  0.0250, -0.0788,  ...,  0.0007, -0.0136, -0.1021],\n",
            "        [ 0.0486,  0.0593, -0.0428,  ..., -0.0052,  0.0004,  0.0854],\n",
            "        [-0.0261, -0.0245, -0.0730,  ...,  0.0181,  0.0102,  0.0124],\n",
            "        ...,\n",
            "        [ 0.0522, -0.0128, -0.0081,  ...,  0.0222,  0.0348, -0.0141],\n",
            "        [-0.0154, -0.0016, -0.0025,  ...,  0.0110, -0.0844,  0.0150],\n",
            "        [-0.0844, -0.0115, -0.0088,  ...,  0.0191, -0.0089, -0.0265]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.10.attention.self.value.bias', Parameter containing:\n",
            "tensor([-1.3350e-02, -1.2241e-02, -5.1834e-03, -2.3253e-03,  1.4861e-03,\n",
            "         2.4383e-02, -4.7029e-03,  6.1049e-03,  1.6735e-02, -2.8792e-02,\n",
            "        -1.6313e-02, -3.6831e-02,  6.2654e-04,  3.4270e-03,  1.6318e-03,\n",
            "        -4.7612e-03, -3.6793e-03, -1.4869e-02, -1.9048e-02, -5.8014e-03,\n",
            "        -1.2619e-02,  5.6017e-03, -5.6833e-03, -5.0335e-03, -6.2400e-03,\n",
            "         3.1298e-03, -1.5889e-02, -2.4703e-03, -3.7229e-03, -6.2011e-05,\n",
            "        -2.4154e-02, -8.4109e-03, -1.9645e-02,  1.0948e-02, -2.9129e-03,\n",
            "         1.0527e-02,  1.1648e-02, -1.1574e-02,  4.0845e-03,  1.2072e-02,\n",
            "         2.0569e-03,  2.8987e-03,  2.9153e-02, -3.9385e-04,  2.3153e-02,\n",
            "        -9.0909e-04,  4.7908e-03,  3.4791e-03,  1.1528e-02,  5.4078e-03,\n",
            "        -8.7105e-03, -1.1281e-02, -2.9433e-03,  4.6126e-03, -9.6811e-03,\n",
            "        -8.9111e-03,  3.1745e-03, -2.2809e-02, -8.3955e-03, -1.2113e-02,\n",
            "        -1.1042e-02, -2.8629e-03,  5.8608e-03,  2.0203e-02,  2.6668e-04,\n",
            "         1.9300e-02, -1.1342e-03, -2.2901e-02, -6.4364e-02, -3.3167e-04,\n",
            "        -2.8583e-02,  1.5488e-04,  9.3850e-03,  1.6857e-02, -5.6285e-02,\n",
            "         1.1561e-02,  6.7098e-03, -3.8285e-02,  2.4469e-02,  5.0906e-04,\n",
            "        -1.3732e-02, -3.2334e-02, -5.6922e-03, -9.0804e-03, -2.1825e-03,\n",
            "         7.3159e-03, -1.7027e-02,  1.0637e-02, -1.9983e-02, -1.3757e-02,\n",
            "         2.3911e-02,  1.8017e-03,  4.4301e-03, -3.0852e-04, -3.3880e-03,\n",
            "         1.3912e-03,  2.8033e-02,  1.1536e-02,  9.2376e-04, -2.7181e-02,\n",
            "         1.7142e-02, -1.7371e-02,  9.5090e-03,  6.5369e-04,  1.1548e-02,\n",
            "         3.7717e-02,  4.0399e-02,  1.6682e-02, -4.0748e-02,  4.7523e-03,\n",
            "        -2.4130e-03,  1.5916e-02, -1.4287e-03, -3.4652e-02, -1.2213e-02,\n",
            "        -3.7741e-02, -3.3402e-02,  2.4532e-02, -1.0940e-02, -8.0124e-03,\n",
            "        -1.4722e-02,  1.7045e-02, -1.7671e-02, -2.1977e-02,  2.0841e-03,\n",
            "         9.9653e-03,  2.3066e-03, -1.2506e-02,  3.1778e-02, -9.0424e-03,\n",
            "        -2.9688e-02,  1.7219e-02, -3.6268e-02,  3.3124e-02, -1.9548e-02,\n",
            "        -1.4086e-02,  4.6025e-04,  4.3928e-03,  2.7029e-02, -1.1392e-02,\n",
            "        -6.4204e-03, -2.2280e-02,  6.3656e-04,  3.3273e-03, -1.3519e-02,\n",
            "         3.0293e-02, -1.4113e-03,  2.2032e-02,  1.7443e-02,  9.8677e-03,\n",
            "         1.4074e-02,  1.9211e-02, -4.0963e-03, -4.5703e-03,  3.9457e-03,\n",
            "        -2.7532e-03,  4.1846e-02, -7.6351e-03,  7.2277e-04, -3.0766e-02,\n",
            "        -1.0882e-02, -1.5692e-02,  1.4736e-02, -1.6786e-02,  4.0584e-03,\n",
            "        -4.6403e-03, -2.2499e-02,  3.1056e-02,  2.2250e-02, -2.0192e-02,\n",
            "         6.5615e-03,  1.8398e-02,  5.5014e-03, -8.5460e-03,  8.8002e-03,\n",
            "         1.7165e-02,  1.2593e-02,  2.3666e-03, -2.8021e-02,  3.7259e-03,\n",
            "         1.2558e-02,  1.0195e-02,  2.2687e-02,  3.8066e-03, -3.1303e-02,\n",
            "        -2.0709e-02,  5.7161e-03,  2.0312e-02, -1.5804e-02,  1.9810e-02,\n",
            "        -2.4154e-03, -2.0503e-02,  1.7852e-02, -2.5597e-02,  9.0242e-04,\n",
            "        -3.3668e-02, -3.6108e-03,  4.2357e-04, -3.4844e-03, -2.9928e-02,\n",
            "         3.3270e-03, -3.8828e-02, -3.4625e-03, -1.0708e-02,  5.3690e-02,\n",
            "        -1.1361e-02,  1.3236e-02, -5.9697e-03,  2.3299e-02, -2.2907e-03,\n",
            "        -1.1507e-02, -1.5089e-02,  2.3709e-03,  3.6263e-03, -2.1374e-03,\n",
            "         8.8470e-03, -2.3885e-02, -5.0407e-03, -4.2979e-02, -4.9424e-03,\n",
            "         1.1059e-02, -2.1954e-02,  3.4234e-03, -1.9271e-02, -1.3620e-02,\n",
            "         4.3213e-03,  1.5615e-02,  1.3805e-03,  5.1108e-02, -9.8081e-03,\n",
            "        -9.4055e-03,  7.5806e-03, -1.2106e-02,  1.8464e-02,  6.3217e-03,\n",
            "         1.2373e-03,  6.1405e-02, -2.4513e-03, -1.6544e-03, -2.1221e-02,\n",
            "         3.2381e-03, -1.2077e-02, -1.1435e-02, -1.1793e-02,  2.4057e-04,\n",
            "        -1.7033e-02,  1.0072e-02, -1.1155e-02,  5.4494e-02,  1.2625e-02,\n",
            "         4.2714e-03, -5.5552e-03,  5.1022e-02, -6.3836e-03,  1.1153e-02,\n",
            "         8.4355e-03, -1.2189e-02,  1.0992e-02, -9.7256e-04, -2.2125e-04,\n",
            "        -2.4779e-02,  4.4409e-02,  1.2747e-02,  1.6033e-03, -2.0174e-02,\n",
            "        -7.9164e-03, -7.6956e-04, -2.8494e-02, -1.0824e-02, -2.1369e-03,\n",
            "        -1.9371e-02,  1.3256e-02,  2.7234e-02, -1.6288e-02,  3.3539e-02,\n",
            "         6.6427e-03,  2.3452e-03,  2.1147e-02, -3.7630e-02, -2.3931e-02,\n",
            "        -2.6740e-02,  9.9008e-04, -2.7346e-02, -2.3854e-02,  8.1214e-03,\n",
            "         1.0071e-02,  1.8965e-02, -3.5950e-02, -7.5781e-03, -8.6459e-03,\n",
            "        -1.8457e-02,  9.8906e-03, -2.4546e-02,  2.2530e-02,  4.9579e-03,\n",
            "         3.5609e-03, -4.3660e-03,  3.9109e-03, -2.6452e-03,  1.5502e-02,\n",
            "        -1.0225e-02, -2.8940e-02, -3.3159e-02,  3.8494e-03, -8.6620e-03,\n",
            "        -8.2316e-03, -2.6441e-02, -2.1966e-02,  2.0637e-03,  2.9648e-02,\n",
            "         1.2465e-02, -1.1413e-02,  3.5648e-03,  1.7074e-02,  5.8724e-03,\n",
            "         3.3044e-03,  3.3953e-02,  3.3598e-03, -2.3500e-02, -3.7845e-02,\n",
            "        -7.3611e-03, -1.1997e-02,  5.1268e-03, -4.8742e-03, -4.6476e-03,\n",
            "         4.1273e-03,  1.4692e-03,  2.2300e-02, -6.4788e-03,  3.5085e-02,\n",
            "        -1.1673e-02,  2.7537e-02,  2.2329e-02, -7.8740e-03,  2.7090e-02,\n",
            "         3.8967e-02, -6.5245e-03, -2.5078e-02, -5.4266e-02, -7.2810e-03,\n",
            "         2.2777e-02, -4.1299e-02, -2.3387e-02, -3.6107e-03, -1.0980e-02,\n",
            "        -1.7981e-03, -4.0899e-03,  3.2738e-02,  3.7847e-03, -3.9175e-03,\n",
            "         1.4053e-02,  2.2559e-02,  5.4679e-03,  1.4589e-02, -8.9432e-03,\n",
            "         5.9774e-03, -3.9494e-02,  3.7443e-02, -1.5772e-02, -4.9468e-02,\n",
            "        -4.0126e-02,  3.6735e-02, -3.1785e-02,  7.3212e-03,  3.5100e-02,\n",
            "         3.5659e-02, -5.3211e-03, -1.4792e-02, -2.4156e-02,  4.7360e-02,\n",
            "        -6.0169e-03,  4.6755e-02,  1.6771e-02,  1.8297e-02, -3.0379e-02,\n",
            "        -3.2600e-03, -1.6084e-02, -3.2488e-02,  1.1818e-02,  6.6291e-03,\n",
            "        -3.5752e-02, -1.4779e-02,  2.8962e-02,  2.3775e-02, -3.3525e-03,\n",
            "         3.5023e-02,  2.0849e-02,  4.6085e-03,  1.2365e-02, -1.0929e-02,\n",
            "         1.3178e-02,  1.4053e-02, -8.0204e-03, -2.1090e-02, -6.8489e-04,\n",
            "        -5.3866e-04, -2.7979e-02,  1.1940e-02,  4.5396e-03, -1.4660e-02,\n",
            "         7.4962e-03, -1.0116e-02,  1.9435e-02,  1.9432e-02, -1.3252e-02,\n",
            "         9.2515e-03, -1.5563e-03, -1.5905e-02, -4.1010e-03, -5.9906e-03,\n",
            "         1.0174e-02,  2.2275e-02, -2.6958e-03,  1.8835e-02,  3.9279e-03,\n",
            "        -2.0716e-03, -1.5407e-02,  2.1015e-03,  2.7672e-02,  6.2282e-03,\n",
            "        -3.3398e-02, -1.8011e-02,  5.2867e-03, -1.4992e-03,  7.9753e-03,\n",
            "         1.6238e-02,  3.2268e-02, -1.0238e-02,  6.3883e-03, -3.7444e-02,\n",
            "         7.4858e-03, -2.5848e-02,  8.0738e-03, -2.1964e-03,  7.1654e-03,\n",
            "        -1.1526e-02,  9.9002e-03, -1.3438e-02, -1.0952e-02, -3.8715e-03,\n",
            "         1.2619e-02, -1.3798e-02,  1.7650e-02, -3.3050e-03,  1.0870e-02,\n",
            "         3.4690e-03,  2.6517e-02,  8.8734e-03,  1.9271e-02,  7.3693e-04,\n",
            "         4.1614e-03, -2.5147e-03,  3.9508e-02, -4.7983e-03, -5.4129e-04,\n",
            "         5.4280e-03, -1.1971e-02,  8.5920e-03, -1.2053e-02,  1.9232e-02,\n",
            "        -7.3664e-05, -1.7381e-02,  7.5337e-03,  2.4862e-02,  2.9259e-03,\n",
            "         1.2019e-03, -9.8528e-03,  1.6792e-02, -2.6678e-03, -1.5878e-02,\n",
            "        -2.1041e-02, -6.3805e-03, -6.9587e-03, -2.1100e-02, -1.0129e-04,\n",
            "        -5.4150e-03, -7.0084e-03, -9.3394e-03, -7.5228e-03,  8.7705e-03,\n",
            "         6.0878e-03, -1.9423e-04,  1.9369e-03, -1.2755e-02,  1.2534e-02,\n",
            "        -1.1064e-03, -3.3061e-03, -1.1506e-02,  2.5097e-02, -4.4739e-03,\n",
            "         8.2340e-03,  3.8356e-03, -1.0637e-02,  1.0431e-02, -7.6940e-03,\n",
            "         3.4094e-02, -1.9006e-02, -1.4190e-02, -9.4534e-03,  1.5433e-02,\n",
            "         2.2639e-02, -8.4075e-03,  4.1798e-03, -2.7385e-03, -1.4367e-02,\n",
            "         8.3901e-04, -1.5439e-02,  4.2996e-03, -1.2589e-02, -2.4563e-02,\n",
            "        -6.3441e-03,  1.9945e-02, -5.8661e-03, -1.5402e-03, -2.0664e-02,\n",
            "         3.9834e-02, -3.1669e-02, -1.9658e-02, -1.1395e-02,  2.0143e-02,\n",
            "         3.2890e-03, -1.3892e-02, -1.0237e-02, -2.6989e-03,  2.5444e-02,\n",
            "        -1.8715e-02, -8.0069e-03,  1.4096e-02, -1.5161e-02,  3.0440e-02,\n",
            "        -3.0488e-04, -2.3883e-02,  7.0899e-03,  4.7084e-03,  2.2949e-02,\n",
            "         8.6598e-03,  4.1352e-04,  2.6105e-02,  9.0844e-03,  1.3375e-02,\n",
            "        -7.4720e-03,  7.3067e-03, -1.3735e-02,  2.7024e-02, -1.7361e-02,\n",
            "         1.8375e-03,  1.1599e-02, -1.1105e-02,  3.7823e-03, -9.2205e-03,\n",
            "         4.6223e-02, -1.9127e-02,  1.9189e-02,  1.0097e-02, -3.3858e-03,\n",
            "        -1.0670e-02,  5.7493e-03,  3.8072e-02,  6.9636e-03,  5.7692e-03,\n",
            "         1.8001e-02,  7.4425e-03,  1.7608e-02,  9.7360e-03, -2.7646e-02,\n",
            "        -2.2686e-03,  2.9473e-02, -3.3328e-02, -1.2251e-02, -1.1919e-02,\n",
            "        -5.6746e-02,  1.7907e-03, -4.4066e-03, -7.9811e-03,  1.1460e-02,\n",
            "        -5.3590e-03,  1.7584e-02,  1.5756e-02, -1.1133e-02,  3.1208e-01,\n",
            "        -2.4096e-02,  2.8343e-02, -3.0667e-02, -4.8228e-03, -1.6304e-02,\n",
            "        -1.9903e-02, -4.8650e-03,  2.0380e-02, -2.1612e-02,  9.0872e-03,\n",
            "        -3.1869e-02, -9.8240e-03, -7.4756e-03,  2.6078e-02,  1.0598e-02,\n",
            "         1.2212e-04,  4.0564e-02, -4.1712e-03,  4.2271e-02, -4.7329e-03,\n",
            "        -2.3388e-03, -8.0900e-03, -5.5501e-03, -4.7973e-02,  3.3132e-02,\n",
            "        -1.7225e-02, -3.2194e-02,  2.7837e-02,  2.6286e-02, -1.6921e-02,\n",
            "         2.7160e-02,  2.0210e-02,  6.0572e-03, -2.5566e-02, -1.9623e-02,\n",
            "        -6.1731e-02, -8.2240e-03,  1.0292e-01, -1.8236e-03, -1.0839e-02,\n",
            "        -3.0355e-02,  2.5934e-02, -5.8327e-03,  8.0007e-02,  2.7361e-03,\n",
            "        -5.9362e-02, -1.9896e-02,  1.7799e-03,  5.1742e-02, -1.7130e-02,\n",
            "        -5.8778e-03, -5.7954e-03, -6.4293e-04,  1.6602e-03, -2.0990e-03,\n",
            "         6.0152e-03,  4.4852e-03,  1.7768e-02,  4.4185e-02, -7.0068e-03,\n",
            "         6.1747e-02,  3.7667e-03,  3.3048e-02, -5.7623e-02,  5.1362e-02,\n",
            "         5.7330e-02,  8.5334e-03,  9.9657e-03, -7.3546e-02,  3.5630e-03,\n",
            "        -6.1687e-02,  6.6658e-03, -4.8955e-02,  5.3323e-02, -3.5615e-02,\n",
            "         9.5301e-02,  4.8338e-02,  6.3920e-03,  2.8649e-01,  1.7050e-02,\n",
            "        -5.9446e-03,  1.8573e-02,  4.7815e-02, -1.8460e-02, -2.9367e-02,\n",
            "         1.4993e-02,  1.7570e-02,  2.4561e-02, -3.2951e-02,  1.3119e-02,\n",
            "         5.0560e-02,  2.0162e-02, -2.7622e-02,  2.5742e-02,  2.8182e-02,\n",
            "         1.2996e-02, -4.2638e-02, -7.5663e-03,  3.5703e-02, -6.0723e-02,\n",
            "         2.1300e-02,  3.4943e-02, -6.2974e-04, -3.7361e-02,  5.2900e-02,\n",
            "        -6.1594e-02,  3.2781e-02, -2.0503e-01, -2.5231e-02, -1.0963e-03,\n",
            "         1.2461e-02,  5.9430e-02,  8.4552e-03,  2.7990e-02,  4.2736e-02,\n",
            "        -1.2145e-02,  9.0404e-02,  3.8170e-03, -2.7388e-02, -3.7823e-02,\n",
            "        -1.2776e-02, -4.0615e-02, -7.6104e-02,  1.8005e-02,  4.7835e-02,\n",
            "         3.8667e-02, -4.6616e-03,  7.9669e-03, -1.3632e-02, -1.3153e-02,\n",
            "        -8.3480e-03,  1.6806e-02,  2.2840e-03, -7.7260e-02,  1.1119e-02,\n",
            "         1.9525e-02, -8.9720e-03, -2.6748e-02,  1.0277e-02,  3.2629e-02,\n",
            "         4.8386e-04,  2.5005e-02, -8.8469e-03,  1.1591e-04,  2.8634e-02,\n",
            "         9.2881e-04, -3.1760e-04,  4.3160e-02,  5.9586e-02,  1.2318e-02,\n",
            "         5.0292e-03,  1.9436e-02,  3.6148e-02, -1.9931e-02,  1.8233e-02,\n",
            "         1.7020e-02, -4.1668e-02, -6.2599e-03, -2.9462e-02, -1.9060e-02,\n",
            "         1.9530e-02,  2.2143e-02, -2.4947e-02, -1.1921e-02,  4.1980e-02,\n",
            "        -3.8685e-02, -1.6653e-02, -2.3361e-02,  1.9460e-02,  2.4227e-03,\n",
            "        -1.1087e-02, -1.9763e-02,  6.8009e-03, -3.4660e-02, -4.3289e-02,\n",
            "         2.7568e-02, -1.1938e-02,  2.6142e-02,  3.4023e-02, -2.6506e-02,\n",
            "        -1.2851e-02,  4.0560e-02, -1.3137e-03,  8.5345e-03,  5.7082e-02,\n",
            "         2.4541e-02, -2.3473e-03, -1.3259e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.10.attention.output.dense.weight', Parameter containing:\n",
            "tensor([[-5.9047e-02, -3.7083e-02,  3.7058e-03,  ...,  2.4455e-02,\n",
            "          2.0699e-02,  2.7833e-05],\n",
            "        [ 2.6160e-02, -2.7335e-02,  1.3541e-02,  ..., -2.3658e-02,\n",
            "         -1.2176e-02, -4.3452e-02],\n",
            "        [ 7.9410e-03,  2.1259e-02,  5.7120e-03,  ..., -3.2796e-02,\n",
            "         -6.1931e-03, -1.5916e-03],\n",
            "        ...,\n",
            "        [-3.4431e-02, -2.3966e-02,  4.5142e-02,  ..., -4.2678e-02,\n",
            "          2.4218e-02, -1.4621e-02],\n",
            "        [-1.5359e-02, -6.7253e-03,  6.2526e-02,  ..., -4.8974e-03,\n",
            "          2.9175e-02, -4.1532e-03],\n",
            "        [ 1.2038e-02, -8.7690e-02,  3.4373e-02,  ..., -2.8485e-02,\n",
            "         -2.3826e-02, -6.8662e-03]], device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.10.attention.output.dense.bias', Parameter containing:\n",
            "tensor([ 2.0892e-02,  1.4581e-02,  5.2178e-02,  3.2427e-02,  9.0755e-03,\n",
            "         1.7235e-02,  2.0646e-02, -6.7698e-02,  3.2617e-02,  6.3936e-02,\n",
            "        -2.9528e-02, -6.8492e-02, -6.2344e-02, -1.7486e-02, -5.9062e-02,\n",
            "        -2.5247e-02,  5.6301e-02,  6.8733e-02,  1.0162e-02, -2.7728e-03,\n",
            "        -2.0946e-02, -1.8208e-03, -2.1162e-02, -4.0140e-04,  3.5422e-02,\n",
            "        -5.5056e-02, -2.2358e-02, -1.0222e-01,  1.3521e-01,  1.6311e-02,\n",
            "        -2.8985e-02,  8.1744e-04, -1.9793e-02, -3.8421e-02,  5.3359e-02,\n",
            "         4.7995e-02,  3.3254e-02, -8.5054e-02,  1.7392e-02,  5.2654e-02,\n",
            "        -3.2327e-02, -5.7849e-02,  1.4949e-02, -1.4949e-02, -6.5015e-02,\n",
            "         4.0730e-02,  9.1089e-04,  5.0172e-02,  2.3568e-02,  5.1360e-02,\n",
            "         3.4443e-02, -5.7045e-02, -3.5161e-02,  7.8529e-02, -1.3442e-02,\n",
            "        -3.6983e-02, -1.4802e-03, -1.1268e-01, -1.1230e-01, -9.6571e-02,\n",
            "        -7.7426e-02,  5.1586e-02, -3.9240e-02, -1.5870e-03,  1.1021e-02,\n",
            "         2.7221e-02, -2.7394e-02,  7.7830e-02, -7.5311e-02,  4.6090e-02,\n",
            "         4.1387e-02,  5.0997e-02, -7.1214e-03,  6.9350e-02, -1.3009e-02,\n",
            "         8.3123e-02, -1.1354e-01, -2.7026e-02,  1.3274e-02, -6.6857e-02,\n",
            "        -5.7783e-03, -1.5848e-02,  2.7606e-02, -2.0850e-02,  4.1069e-02,\n",
            "         9.5667e-02, -2.0576e-02,  5.0873e-02,  8.3961e-03, -1.2340e-01,\n",
            "         7.9257e-02, -3.2330e-02, -4.6895e-02, -5.3333e-02, -3.0789e-02,\n",
            "         8.2158e-02,  7.2134e-02,  1.5064e-02, -5.7358e-02, -2.5067e-02,\n",
            "        -1.8453e-02, -9.2287e-03,  8.1361e-02,  2.8929e-02,  9.3228e-02,\n",
            "        -2.8824e-04, -2.8693e-03,  1.8632e-03, -5.6384e-02,  1.1106e-01,\n",
            "        -3.0974e-02, -3.2911e-02, -5.1712e-04, -4.1277e-02,  7.0639e-02,\n",
            "        -9.0394e-03, -6.7880e-02, -1.3179e-02, -1.0409e-01,  9.2444e-02,\n",
            "        -1.2056e-01, -1.2725e-01,  7.3052e-02, -4.2166e-02,  7.7886e-02,\n",
            "         4.5971e-02, -6.5357e-02, -5.2375e-02, -7.9467e-02,  6.0088e-03,\n",
            "        -3.2759e-02, -2.9180e-02,  2.9531e-02,  3.3492e-02,  1.9331e-02,\n",
            "         5.2533e-03, -2.0435e-02, -1.0799e-01, -2.3617e-02,  1.3732e-01,\n",
            "         4.9843e-02, -5.9347e-02, -1.2239e-02,  9.1236e-02,  3.9073e-02,\n",
            "        -5.0441e-03,  7.5347e-03, -4.2826e-02, -2.5074e-02, -3.0532e-02,\n",
            "         1.4856e-03, -5.0177e-02,  8.0562e-02, -4.2207e-02, -2.8751e-02,\n",
            "         3.3561e-02, -4.8978e-02,  1.0547e-02,  9.3902e-03, -1.9776e-02,\n",
            "        -5.9157e-02,  5.9073e-02,  7.4639e-02,  6.3760e-02,  5.2904e-02,\n",
            "         1.7901e-02,  2.5094e-02,  1.3242e-02, -5.6727e-02, -2.9134e-02,\n",
            "         4.4717e-02,  6.9400e-02, -1.0351e-02, -4.7245e-02, -1.3407e-02,\n",
            "         3.5892e-02,  1.9198e-02,  2.3338e-02, -2.2931e-02, -5.8357e-02,\n",
            "         5.8553e-02, -1.2811e-03, -2.0910e-03,  1.4439e-02, -2.7511e-02,\n",
            "         7.6866e-02, -6.3143e-02, -4.8660e-02,  5.5004e-02, -1.1831e-02,\n",
            "        -2.6315e-02, -6.6688e-03,  2.2683e-02, -1.8516e-02,  6.5799e-02,\n",
            "         1.8756e-02,  1.8666e-03, -2.0149e-03,  4.3476e-03, -1.5934e-01,\n",
            "         3.5778e-02,  8.7093e-02,  3.2087e-02,  6.0228e-02,  1.0112e-01,\n",
            "        -5.2062e-02,  2.7203e-02, -3.5474e-02, -5.5620e-02,  3.0349e-02,\n",
            "         1.1250e-01,  3.0037e-02, -5.1676e-02, -7.3373e-03, -6.5133e-02,\n",
            "         6.0811e-02,  7.1677e-04,  3.6659e-02,  3.1376e-03, -2.9379e-02,\n",
            "        -5.5007e-02,  3.8681e-03, -6.3766e-02,  3.3072e-03,  4.5808e-02,\n",
            "        -3.2994e-02,  1.8079e-02,  5.8690e-02, -1.3152e-02,  7.5456e-02,\n",
            "        -2.2464e-02,  1.3650e-02,  1.6386e-02, -7.0047e-02, -1.7981e-02,\n",
            "        -9.0752e-04, -4.2863e-02,  2.1620e-03, -1.2388e-02, -7.9525e-02,\n",
            "        -7.3754e-02, -4.6685e-02,  5.9819e-02, -5.7041e-02,  9.1596e-02,\n",
            "         9.1880e-02,  1.6060e-02, -3.1605e-02, -4.2121e-02, -6.0814e-03,\n",
            "         7.3915e-02, -2.8001e-02,  5.3165e-02, -7.3249e-02, -1.7641e-02,\n",
            "         5.6498e-02,  7.0367e-02,  2.4056e-02, -3.8064e-02, -1.4537e-01,\n",
            "        -5.2005e-02, -8.4566e-02,  1.4764e-02,  7.3969e-02, -9.6559e-02,\n",
            "         2.6615e-02,  1.4318e-02, -1.0139e-02, -2.9510e-02, -4.9098e-02,\n",
            "        -3.9822e-02, -7.7730e-03, -4.1342e-02,  4.5577e-02, -6.7131e-03,\n",
            "         1.4507e-02,  2.3011e-02,  5.7859e-02, -1.9657e-02, -5.1393e-02,\n",
            "         6.0033e-02, -5.2804e-02,  2.4703e-02,  1.3052e-01,  5.4510e-02,\n",
            "         5.2455e-02, -2.7487e-02,  6.8886e-02,  5.9078e-02, -1.1941e-01,\n",
            "         5.0630e-02, -1.9179e-02, -2.5573e-03,  2.8877e-03,  1.9306e-02,\n",
            "         1.2447e-02,  2.6892e-02, -6.4421e-03,  5.4110e-02, -3.7954e-02,\n",
            "        -8.2794e-02, -1.6445e-02,  5.4632e-02, -3.2233e-02, -3.4849e-02,\n",
            "         2.3452e-03, -4.0024e-02, -7.1535e-02,  1.6524e-01, -4.6903e-02,\n",
            "         2.4021e-02, -2.4586e-02,  3.1408e-02, -3.8794e-02, -1.8283e-02,\n",
            "         4.2810e-02,  5.2866e-02,  1.4086e-02,  6.4905e-02, -7.7743e-02,\n",
            "         6.2260e-03,  2.1117e-02, -4.5459e-03, -4.8708e-03, -3.7715e-02,\n",
            "         4.7391e-02, -1.0938e-01, -6.5166e-02,  4.5260e-02, -9.5262e-02,\n",
            "         9.7321e-02,  5.9654e-02,  8.8644e-02, -3.2164e-02, -1.2933e-02,\n",
            "        -2.9637e-02,  9.3893e-02,  4.0554e-02, -1.0622e-02, -5.7201e-02,\n",
            "         4.2969e-03,  7.0861e-03, -9.6170e-02,  3.7326e-02,  3.3799e-02,\n",
            "        -5.9956e-02, -2.9817e-02, -2.8683e-02,  9.9239e-02,  5.3635e-02,\n",
            "         3.9613e-02, -5.9194e-02, -4.4001e-02, -2.9719e-02, -3.5824e-02,\n",
            "        -3.0731e-02, -4.2629e-03,  1.2197e-02,  6.2746e-02,  8.1705e-02,\n",
            "        -7.2084e-02,  3.4645e-02, -3.8355e-02, -2.8130e-04, -2.6216e-02,\n",
            "        -5.1130e-03,  1.3602e-02, -5.0607e-02, -1.7180e-02,  3.7643e-02,\n",
            "         2.1187e-02, -4.5912e-02,  4.5527e-02, -1.2564e-02,  6.1764e-02,\n",
            "         2.4257e-03,  6.4579e-02,  7.0490e-02,  7.2041e-02, -5.5656e-02,\n",
            "        -3.8987e-02, -6.8683e-02,  2.9099e-02,  7.3828e-02,  3.8526e-02,\n",
            "         1.6351e-03, -1.6925e-02,  4.2856e-02,  1.0524e-01, -7.8026e-02,\n",
            "        -1.2301e-02,  5.7765e-03, -1.3900e-02,  5.2729e-02,  7.8067e-02,\n",
            "        -2.7313e-02,  3.9033e-02, -3.2429e-04,  5.0410e-02,  2.3798e-02,\n",
            "        -7.5117e-02,  4.4745e-02,  2.5193e-03, -5.6575e-02,  4.9092e-03,\n",
            "         8.8590e-02,  7.6156e-02, -1.0246e-02, -5.4952e-03,  5.9665e-02,\n",
            "        -1.6523e-03,  2.7249e-02,  2.5833e-02,  5.0859e-03, -3.9611e-02,\n",
            "        -7.5411e-02, -3.1980e-02, -5.9673e-02, -5.1215e-02,  1.3116e-02,\n",
            "         4.3019e-02,  2.8778e-02,  2.9528e-02,  1.0651e-01,  6.5098e-03,\n",
            "        -8.2781e-03,  3.4962e-02,  8.1117e-02,  5.7154e-02,  7.2445e-02,\n",
            "        -3.3766e-02, -9.1045e-03,  1.8320e-02, -8.2018e-02,  5.8345e-02,\n",
            "         2.1936e-02, -6.4909e-02,  1.8683e-02, -5.8835e-02,  6.0066e-02,\n",
            "        -4.2750e-02, -1.5680e-02, -1.6467e-02, -9.0445e-02, -6.7726e-02,\n",
            "        -1.1713e-02,  3.0684e-02,  1.5820e-01,  1.9098e-02,  7.6610e-02,\n",
            "        -2.4732e-02, -1.5154e-02, -1.5696e-02,  2.2714e-03,  6.1276e-02,\n",
            "        -1.4627e-02,  1.7204e-02,  3.3882e-03, -3.7080e-03, -2.4509e-02,\n",
            "        -5.3459e-02,  2.3667e-02, -3.8822e-02, -4.6559e-02, -6.4898e-02,\n",
            "         3.5302e-02,  1.5338e-02, -3.2683e-02, -2.6005e-02,  1.3505e-01,\n",
            "         1.9067e-03,  2.2332e-02,  1.7037e-02, -2.0748e-02,  6.2439e-02,\n",
            "        -3.1304e-02, -2.6837e-03, -7.9760e-03,  2.6613e-02, -8.0957e-02,\n",
            "        -5.7677e-02,  2.4708e-02, -3.9664e-02, -3.4810e-02,  1.3357e-02,\n",
            "         1.9606e-02,  1.3373e-02,  1.4712e-02, -9.2969e-02, -2.6608e-02,\n",
            "        -5.3597e-02, -2.2018e-02,  3.7111e-02,  4.9822e-02,  3.3201e-02,\n",
            "         3.9872e-02,  6.1973e-02, -8.2073e-03, -7.9960e-02,  6.4323e-02,\n",
            "        -2.3556e-03, -7.1839e-03,  9.2693e-02,  9.5977e-02,  1.0704e-01,\n",
            "         4.9037e-02,  7.5860e-03,  6.7053e-02,  6.6983e-02, -2.8582e-03,\n",
            "         8.9961e-02,  4.0942e-02,  2.3129e-02,  1.0699e-02,  1.4260e-02,\n",
            "        -4.4503e-02,  2.5924e-02, -4.0943e-02, -1.3068e-02, -3.0376e-03,\n",
            "         5.5619e-04,  5.9614e-02,  1.3245e-01, -3.7176e-02,  2.3497e-02,\n",
            "        -4.7790e-02, -3.7709e-02,  7.1525e-02, -5.2302e-02,  3.5674e-03,\n",
            "         7.6931e-02, -8.2258e-02, -8.1030e-02,  1.8713e-02,  1.0855e-01,\n",
            "        -3.6092e-02,  9.0499e-03, -3.5563e-02,  2.9656e-02,  4.4297e-02,\n",
            "         5.8723e-02, -1.0365e-02, -5.9076e-03, -2.3744e-02,  6.6937e-02,\n",
            "        -6.2524e-02, -8.5992e-02, -1.9669e-02, -1.0131e-01,  3.5852e-02,\n",
            "        -4.3871e-02, -5.5250e-02,  1.2461e-02, -3.0062e-02, -3.1245e-02,\n",
            "         2.8605e-02, -2.3917e-02, -8.1261e-02,  1.7011e-02,  2.6904e-03,\n",
            "        -2.9471e-04,  4.4519e-02, -4.7425e-02, -1.8492e-02, -4.2875e-02,\n",
            "        -9.2438e-03,  1.8411e-02, -6.9932e-02, -3.3733e-02,  5.4822e-02,\n",
            "         1.4044e-01, -1.0477e-01, -3.1525e-02,  6.8818e-03,  4.5576e-02,\n",
            "         7.7821e-02, -2.8519e-02,  2.3382e-02,  2.9528e-03, -1.9813e-02,\n",
            "         4.5057e-02, -1.9628e-02, -7.7812e-02, -6.3258e-03, -6.5136e-03,\n",
            "        -8.7933e-02, -1.2530e-02,  6.2591e-02,  9.4201e-02, -1.3093e-01,\n",
            "        -6.2649e-02, -1.2104e-01,  5.3931e-02,  1.1604e-01,  1.8564e-03,\n",
            "        -4.7559e-02, -2.1847e-02, -6.6490e-02, -6.1510e-02,  4.1458e-02,\n",
            "        -4.9599e-02, -3.9659e-02, -6.2780e-02, -8.4143e-03, -4.1884e-02,\n",
            "        -2.0940e-02,  1.0105e-01,  1.7882e-02,  2.9150e-02, -4.0877e-02,\n",
            "         8.3482e-02, -2.7060e-02,  3.5187e-02,  4.1169e-02,  2.0740e-03,\n",
            "         1.8161e-02, -3.1213e-02,  4.6555e-02, -7.9089e-02,  3.2327e-02,\n",
            "        -7.3387e-02, -5.6379e-02,  3.5156e-02, -1.1243e-02,  2.8098e-02,\n",
            "         3.1412e-02, -3.4645e-02,  1.1851e-01, -5.5234e-02,  4.6259e-02,\n",
            "         2.8351e-02, -5.9274e-03,  3.7936e-02, -1.1003e-02,  2.9300e-02,\n",
            "         1.9918e-02,  3.4457e-02,  3.1025e-02, -9.9828e-02, -4.9896e-02,\n",
            "        -5.4747e-02, -5.8652e-02,  3.5006e-02,  7.4691e-02, -7.5890e-02,\n",
            "         1.1201e-01,  3.6170e-02, -4.1594e-02,  1.5267e-02,  6.0655e-02,\n",
            "        -9.1602e-03, -6.9869e-02,  6.4298e-02, -6.5510e-02, -1.7300e-02,\n",
            "        -1.5735e-02,  1.0415e-01, -1.0910e-02, -5.1661e-02,  6.2106e-02,\n",
            "        -3.9600e-02, -1.2046e-01, -1.9290e-02,  1.3820e-02, -3.4411e-02,\n",
            "         6.9219e-02,  6.3657e-02, -5.5638e-02,  3.5550e-02, -2.7452e-02,\n",
            "        -6.1564e-02, -3.3839e-02, -1.1769e-02, -1.1679e-02, -1.6722e-02,\n",
            "        -7.1706e-02, -6.5493e-02, -3.8774e-02, -3.5779e-02, -1.1088e-02,\n",
            "         6.3480e-02,  5.5690e-03, -4.1589e-02, -9.5483e-03, -2.5484e-02,\n",
            "        -2.3603e-02, -3.1703e-02, -3.7608e-02, -1.0983e-01,  6.1890e-02,\n",
            "         4.0341e-02,  4.6595e-02,  1.3212e-02, -1.6557e-02,  1.5432e-02,\n",
            "        -6.6244e-02,  2.0731e-02,  4.1107e-02, -1.4365e-03, -1.4386e-01,\n",
            "        -3.3741e-02, -1.2296e-02,  1.8228e-02, -5.6243e-02, -4.5491e-02,\n",
            "         2.7531e-02,  5.5782e-02,  9.1378e-02, -9.8210e-02, -9.5262e-02,\n",
            "         3.3941e-02,  5.0438e-02,  6.0573e-02,  1.9269e-02,  1.0136e-01,\n",
            "         1.7447e-02,  1.5591e-01,  1.0359e-02, -3.8702e-02, -1.8036e-02,\n",
            "        -4.9068e-02, -7.4784e-03, -1.6350e-05,  3.6517e-02, -3.1697e-03,\n",
            "         2.3160e-02,  8.5653e-02,  3.6728e-02, -5.4213e-02, -9.1739e-03,\n",
            "        -8.6926e-03, -5.3458e-03, -1.8571e-03, -5.0580e-03, -4.9067e-02,\n",
            "         3.6895e-02, -7.1525e-03, -5.3874e-02,  5.9427e-02, -5.6298e-02,\n",
            "        -1.3438e-02,  1.0379e-02, -1.9125e-02,  1.8766e-02, -6.2316e-02,\n",
            "        -1.1205e-02, -6.9145e-02,  1.0397e-02,  2.0669e-02,  7.5851e-02,\n",
            "         2.6743e-03, -3.1027e-02,  5.2728e-02,  8.3897e-03, -3.6187e-02,\n",
            "         4.7871e-02,  5.6607e-02,  3.5422e-02, -9.7401e-03, -5.5995e-03,\n",
            "        -8.6591e-02,  2.7362e-02, -1.5700e-02, -7.7986e-02,  4.0447e-03,\n",
            "         3.6552e-02, -8.8953e-02, -1.3180e-01], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.10.attention.output.LayerNorm.weight', Parameter containing:\n",
            "tensor([0.8516, 0.8139, 0.8367, 0.7804, 0.8809, 0.8266, 0.8254, 0.8704, 0.8311,\n",
            "        0.8326, 0.8139, 0.8068, 0.7919, 0.8126, 0.8157, 0.8379, 0.8228, 0.8021,\n",
            "        0.8737, 0.8193, 0.8451, 0.8182, 0.8029, 0.8129, 0.7986, 0.8149, 0.8223,\n",
            "        0.8579, 0.8327, 0.8090, 0.8529, 0.8110, 0.8050, 0.8320, 0.8345, 0.7915,\n",
            "        0.8518, 0.8148, 0.8619, 0.8186, 0.8246, 0.8251, 0.8375, 0.8504, 0.8012,\n",
            "        0.8117, 1.1885, 0.8286, 0.8332, 0.8408, 0.8715, 0.8231, 0.8083, 0.8043,\n",
            "        0.8448, 0.8442, 0.8043, 0.8106, 0.7972, 0.8112, 0.8506, 0.8161, 0.7982,\n",
            "        0.8390, 0.8310, 0.8446, 0.8066, 0.8471, 0.8720, 0.8347, 0.8236, 0.8442,\n",
            "        0.8164, 0.8162, 0.8356, 0.8548, 0.8220, 0.8554, 0.8124, 0.8086, 0.8203,\n",
            "        0.8478, 0.8427, 0.8122, 0.7985, 0.7930, 0.8720, 0.7804, 0.8037, 0.8763,\n",
            "        0.8309, 0.8152, 0.8255, 0.8356, 0.8150, 0.8157, 0.8542, 0.8154, 0.8086,\n",
            "        0.8437, 0.8366, 0.8480, 0.8255, 0.8181, 0.8310, 0.7936, 0.7778, 0.8487,\n",
            "        0.8150, 0.8810, 0.8325, 0.8273, 0.8007, 0.8074, 0.7856, 0.7914, 0.8108,\n",
            "        0.8064, 0.8252, 0.7947, 0.8152, 0.8136, 0.8196, 2.1988, 0.7953, 0.8158,\n",
            "        0.8449, 0.8231, 0.8200, 0.8187, 0.8287, 0.8035, 0.7919, 0.8154, 0.8176,\n",
            "        0.8215, 0.8216, 0.8111, 0.8236, 0.8094, 0.8398, 0.8436, 0.8508, 0.9312,\n",
            "        0.8243, 0.7902, 0.8260, 0.8784, 0.8314, 0.8245, 0.8200, 0.8517, 0.8160,\n",
            "        0.8384, 0.8111, 0.8152, 0.8347, 0.8506, 0.8516, 0.8480, 0.8463, 0.8275,\n",
            "        0.8108, 0.8466, 0.8684, 0.8373, 0.8337, 0.8474, 0.8418, 0.8237, 0.8073,\n",
            "        0.8386, 0.8017, 0.8128, 0.8366, 0.7916, 0.8101, 0.8203, 0.8014, 0.8276,\n",
            "        2.2593, 0.8438, 0.7983, 0.8251, 0.8337, 0.8015, 0.8900, 0.8267, 0.8513,\n",
            "        0.7960, 0.8199, 0.8095, 0.8462, 0.8228, 0.8278, 0.8558, 0.8596, 0.7976,\n",
            "        0.8492, 0.7893, 0.8185, 0.7881, 0.7980, 0.8282, 0.8181, 1.3081, 0.7967,\n",
            "        0.7993, 0.8166, 0.8408, 0.8113, 0.8258, 0.8352, 0.8712, 0.8399, 0.7994,\n",
            "        0.8216, 0.8576, 0.8240, 0.8173, 0.8217, 0.8454, 0.8357, 0.8063, 0.8231,\n",
            "        1.5342, 0.8137, 0.8222, 0.8210, 0.9655, 0.8404, 0.8097, 0.8240, 0.8170,\n",
            "        0.8348, 0.8265, 0.8302, 0.8378, 0.8204, 0.8553, 0.7878, 0.8218, 0.8350,\n",
            "        0.8566, 0.8354, 0.8035, 0.8040, 0.8193, 0.8325, 0.7953, 0.8337, 0.8443,\n",
            "        0.8021, 0.8238, 0.8603, 0.8051, 0.7853, 0.8646, 0.7991, 0.8277, 0.8433,\n",
            "        0.8164, 0.7853, 0.8163, 0.8234, 0.8473, 0.8588, 0.8047, 0.8205, 0.8385,\n",
            "        0.8216, 0.8512, 0.8250, 0.8227, 0.8381, 0.8374, 0.7989, 0.8114, 0.8323,\n",
            "        0.8334, 0.8393, 0.8753, 0.8273, 0.8021, 0.8497, 0.7771, 0.8626, 0.8884,\n",
            "        0.8147, 0.8230, 0.8180, 0.8737, 0.8015, 0.8688, 0.8634, 0.8222, 0.8147,\n",
            "        0.7858, 0.8370, 0.7966, 0.8638, 0.8275, 0.8445, 0.7997, 0.8477, 0.8180,\n",
            "        0.8131, 0.8466, 0.5300, 0.7921, 0.8668, 0.8326, 0.8596, 0.8008, 0.8337,\n",
            "        0.7762, 0.8375, 0.8009, 0.8420, 0.7761, 0.8216, 0.8668, 0.8108, 0.8748,\n",
            "        0.8364, 0.8154, 0.7936, 0.9133, 0.8168, 0.8020, 0.8219, 0.7865, 0.8064,\n",
            "        0.8714, 0.8580, 0.8162, 0.8419, 0.8091, 0.8386, 0.8272, 0.8200, 0.8527,\n",
            "        0.8080, 0.8012, 0.7897, 0.8253, 0.8327, 0.8740, 0.8075, 0.8821, 0.8000,\n",
            "        0.8234, 0.8885, 0.7922, 0.8337, 0.8499, 0.8347, 0.8196, 0.8143, 0.8178,\n",
            "        0.8547, 0.8037, 0.8078, 0.8129, 0.8160, 0.8777, 0.8296, 0.8319, 0.8401,\n",
            "        0.7785, 0.8852, 0.8079, 0.8408, 0.8573, 0.9176, 0.8365, 0.8049, 0.8349,\n",
            "        0.8222, 0.8514, 0.8362, 3.1994, 0.8310, 0.8773, 0.8864, 0.8246, 0.8234,\n",
            "        0.8371, 0.8331, 0.8531, 0.8445, 0.8116, 0.8156, 0.8169, 0.8032, 0.8072,\n",
            "        0.8374, 0.8342, 0.8453, 0.8442, 0.8173, 0.8155, 0.8378, 0.8092, 0.8058,\n",
            "        0.8317, 0.7968, 0.8162, 0.8266, 0.8346, 0.8269, 0.8537, 0.8465, 0.7964,\n",
            "        0.8174, 0.7943, 0.7900, 0.8370, 0.7915, 0.8274, 0.8434, 0.8009, 0.8588,\n",
            "        0.8175, 0.8153, 0.8234, 0.8027, 0.7989, 0.8172, 0.8170, 0.8407, 0.8841,\n",
            "        0.8255, 0.8080, 0.8183, 0.8504, 0.8317, 0.8106, 0.8048, 0.8322, 0.8469,\n",
            "        0.8296, 0.8161, 0.8067, 0.8128, 0.9662, 0.8350, 0.8515, 0.8332, 0.8418,\n",
            "        0.8191, 0.8680, 0.8824, 0.8296, 0.8565, 0.8265, 0.7991, 0.8029, 0.8814,\n",
            "        0.8646, 0.8364, 0.8145, 0.8410, 0.8604, 0.8705, 0.7852, 0.8511, 0.8877,\n",
            "        0.8133, 0.8167, 0.9911, 0.8181, 0.8459, 0.8313, 0.7976, 0.8256, 0.7844,\n",
            "        0.8758, 0.8412, 0.8192, 0.8440, 0.7821, 0.7770, 0.8157, 0.8390, 0.8312,\n",
            "        0.8428, 0.8897, 0.8142, 0.8153, 0.8233, 0.8222, 0.8055, 0.8962, 0.8279,\n",
            "        0.8096, 0.7981, 0.8129, 0.8920, 0.8296, 0.8829, 0.8344, 0.8296, 0.8655,\n",
            "        0.7928, 0.8444, 0.8144, 0.8047, 0.8177, 0.8316, 0.8234, 0.8078, 0.8300,\n",
            "        0.8294, 0.8389, 0.8289, 0.8332, 0.8000, 0.8540, 0.8194, 0.8425, 0.8334,\n",
            "        0.8388, 0.8237, 0.8154, 0.8075, 1.5364, 0.8095, 0.8266, 0.7928, 0.8374,\n",
            "        0.7862, 0.8404, 0.8142, 0.8454, 0.8615, 0.8225, 0.8244, 0.8117, 0.8681,\n",
            "        0.8166, 0.8321, 0.8287, 0.8350, 0.8878, 0.8556, 0.8362, 0.8222, 0.8547,\n",
            "        0.8017, 0.8317, 0.8326, 0.8374, 0.8742, 0.8102, 0.8213, 0.8039, 0.8718,\n",
            "        0.8637, 0.8106, 0.8033, 0.7914, 0.8150, 0.8669, 0.8319, 0.8549, 0.8010,\n",
            "        0.8615, 0.8012, 0.8388, 0.8147, 0.8004, 0.8225, 0.8036, 0.8079, 0.8040,\n",
            "        0.8066, 0.8645, 0.8635, 0.8290, 0.8484, 0.8457, 0.8401, 0.8158, 0.8141,\n",
            "        0.8270, 0.8721, 0.8536, 0.8339, 0.8179, 0.8317, 0.8162, 0.8355, 0.8383,\n",
            "        0.7929, 0.8359, 0.8111, 0.8062, 0.8061, 0.8139, 0.7818, 0.8455, 0.8310,\n",
            "        0.8546, 0.8446, 0.8277, 0.8750, 0.7991, 0.8450, 0.7979, 0.8168, 0.8426,\n",
            "        0.8186, 0.7913, 0.8123, 0.8163, 0.8479, 0.8612, 0.8139, 0.8641, 0.8820,\n",
            "        0.8241, 0.8044, 0.8284, 0.8351, 0.8440, 0.8401, 0.8465, 0.7994, 0.8624,\n",
            "        0.8297, 0.7926, 0.8416, 0.8594, 0.8046, 0.8307, 0.8443, 0.8361, 0.7991,\n",
            "        0.8373, 0.8148, 0.8511, 0.8440, 0.8393, 0.8328, 0.8824, 0.8366, 0.8598,\n",
            "        0.8273, 0.8226, 0.8240, 0.8333, 0.8207, 0.7816, 0.8238, 0.8272, 0.8575,\n",
            "        0.8498, 0.8221, 0.8422, 0.8259, 0.8493, 0.8334, 0.8514, 0.8092, 0.8383,\n",
            "        0.8381, 0.8798, 0.8294, 0.8112, 0.8698, 0.8403, 0.8144, 0.8294, 0.8296,\n",
            "        0.8507, 0.8324, 0.8246, 0.8201, 0.8782, 0.8429, 0.8603, 0.8141, 0.8261,\n",
            "        0.8285, 0.8405, 0.8011, 0.8276, 0.8736, 0.8315, 0.8129, 0.8327, 0.8386,\n",
            "        0.8210, 0.8268, 0.8239, 0.8434, 0.8195, 0.8146, 0.8368, 0.8611, 0.8269,\n",
            "        0.8236, 0.8026, 0.8456, 0.8388, 0.8736, 0.9052, 0.8264, 0.8228, 0.8041,\n",
            "        0.8032, 0.8736, 0.7961, 0.8114, 0.8553, 0.9220, 0.8062, 0.8310, 0.8195,\n",
            "        2.2278, 0.8226, 0.8401, 0.8217, 0.8251, 0.8460, 0.8280, 0.8745, 0.8515,\n",
            "        0.8174, 0.7941, 0.8202, 0.8214, 0.8997, 0.8288, 0.8202, 0.8498, 0.8283,\n",
            "        0.8155, 0.8155, 0.8343, 0.8463, 0.8305, 0.8457, 0.8121, 0.8282, 0.8091,\n",
            "        0.8278, 0.8238, 0.8573, 0.8414, 0.8250, 0.8370, 0.8395, 0.8216, 0.8565,\n",
            "        1.0024, 0.8248, 0.8308, 0.8249, 0.8481, 0.7920, 0.8152, 0.8485, 0.8238,\n",
            "        0.8056, 0.8108, 0.8425], device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.10.attention.output.LayerNorm.bias', Parameter containing:\n",
            "tensor([-1.0986e-01, -4.3323e-02,  2.6039e-02, -6.2369e-02,  1.4469e-01,\n",
            "         2.3894e-02,  8.5148e-03,  1.4266e-01, -1.0833e-01, -2.4890e-02,\n",
            "         9.4963e-03,  4.1846e-02, -1.2760e-01,  6.9555e-02, -7.1048e-02,\n",
            "         1.3593e-01,  5.3569e-02, -6.0933e-03, -1.6118e-02, -3.3533e-02,\n",
            "         7.2184e-02, -2.3410e-02, -2.8510e-03,  5.5217e-02, -8.6453e-02,\n",
            "         6.9857e-02,  3.2686e-02, -6.1245e-02, -7.7870e-02,  3.0835e-02,\n",
            "         1.2870e-01,  3.8405e-02, -4.6480e-02, -6.3926e-02, -7.3934e-02,\n",
            "        -8.2546e-02, -1.8265e-02,  5.9811e-02, -9.6765e-02, -1.3181e-01,\n",
            "        -1.0285e-01, -1.4961e-01,  1.4476e-02,  1.3553e-01,  5.3121e-02,\n",
            "        -8.5382e-02, -3.9924e-02, -9.4157e-02, -1.5067e-02, -7.8801e-02,\n",
            "        -3.4228e-01,  3.4178e-02, -7.2961e-02, -6.9763e-02,  1.4378e-01,\n",
            "         6.5267e-02, -1.1072e-01,  1.9447e-02, -4.5457e-02, -1.2777e-01,\n",
            "        -2.0564e-03,  3.3590e-02, -4.5370e-02, -1.2036e-01,  4.5831e-02,\n",
            "         8.6247e-02,  3.3824e-02,  9.5974e-02, -2.5458e-01, -4.9390e-02,\n",
            "        -1.4536e-01,  4.9588e-02,  2.2120e-02, -7.0260e-02,  1.3164e-02,\n",
            "        -1.6465e-01, -8.2084e-02,  6.4286e-02,  1.6903e-02,  3.3468e-02,\n",
            "        -9.5631e-03,  4.3854e-02, -6.4052e-02,  1.0266e-01, -3.3576e-02,\n",
            "        -1.0900e-01, -8.6243e-03,  1.3397e-02,  3.6529e-02,  2.3307e-01,\n",
            "        -1.4428e-01, -4.8490e-02, -3.1271e-02, -5.4141e-02, -2.4304e-02,\n",
            "        -1.0987e-01,  1.4673e-01, -9.2204e-02, -1.8399e-02, -9.1738e-02,\n",
            "         1.5773e-02, -9.6264e-02,  5.0307e-03, -1.6426e-02, -8.9977e-02,\n",
            "         3.2157e-02, -3.2686e-02, -7.7668e-02, -1.3247e-02, -4.3428e-02,\n",
            "        -5.8969e-02,  7.2364e-02,  1.8438e-02, -9.3603e-02, -4.2916e-02,\n",
            "        -4.3911e-02,  1.1861e-02, -7.4816e-02, -1.1364e-01, -4.7971e-03,\n",
            "         6.2905e-03, -5.6361e-02,  7.5789e-02,  7.6594e-01, -1.1893e-01,\n",
            "        -7.8478e-02, -1.1712e-01,  3.9348e-02, -7.5822e-02,  5.4312e-03,\n",
            "         5.8176e-02,  6.4310e-02,  3.0069e-02, -6.3156e-02, -8.0370e-02,\n",
            "        -6.2562e-02, -5.1549e-02, -7.0859e-03, -7.0210e-02, -1.5485e-02,\n",
            "        -7.1645e-02,  5.2737e-03,  1.7610e-03,  1.3995e-01, -2.1642e-02,\n",
            "        -6.3398e-04,  7.1803e-03, -2.5995e-01, -8.1443e-02, -5.0951e-02,\n",
            "        -4.2187e-03,  3.2522e-02, -1.4275e-01, -1.5201e-01, -8.1764e-02,\n",
            "         1.5360e-02,  6.0379e-02,  1.0348e-01,  5.9798e-02, -4.7835e-02,\n",
            "         2.3904e-02, -1.0125e-01, -6.1234e-02, -4.0206e-02,  1.8066e-01,\n",
            "        -1.6602e-02, -5.2931e-02,  2.6196e-02, -1.2571e-01,  6.1788e-03,\n",
            "        -1.0078e-02, -4.8021e-02,  1.8180e-01,  2.6755e-03,  6.9794e-02,\n",
            "        -6.1887e-02,  2.9740e-02,  1.5290e-02,  5.7968e-02,  1.7624e-01,\n",
            "        -4.6810e-01,  1.0850e-01, -1.7701e-02,  1.3952e-02, -2.3574e-02,\n",
            "        -1.1344e-02,  1.7689e-01, -1.9464e-02,  3.5528e-02,  5.0650e-02,\n",
            "        -1.0216e-01,  5.9138e-02,  6.1453e-02, -4.9731e-02, -1.7462e-02,\n",
            "         5.3093e-02, -1.1973e-01, -4.3167e-02,  7.7277e-03,  6.1520e-02,\n",
            "        -6.4361e-03,  3.3494e-02,  6.8797e-02, -6.8644e-02, -1.2263e-01,\n",
            "         3.6359e-01, -2.7784e-03,  4.2856e-02, -1.4364e-02,  3.3354e-02,\n",
            "        -7.1967e-02, -1.3416e-02,  3.3289e-02,  1.3586e-01, -8.1726e-04,\n",
            "        -1.1282e-01, -6.6198e-02, -2.0349e-02,  7.1594e-02, -3.8219e-02,\n",
            "        -9.9592e-04,  4.3075e-02, -4.7170e-03,  9.2223e-02,  7.9907e-03,\n",
            "         1.8319e-01,  6.7595e-03, -1.6716e-01,  3.7392e-02, -1.1114e-01,\n",
            "        -3.5641e-02, -6.7132e-02,  1.0227e-01, -7.8563e-02, -3.4340e-02,\n",
            "        -6.4997e-02, -5.4021e-02,  3.2029e-03, -2.4696e-02, -1.2331e-01,\n",
            "         9.3232e-02, -3.5118e-02,  1.2557e-02,  1.6174e-01, -7.3716e-02,\n",
            "        -4.3041e-02, -2.1121e-02, -9.8188e-03, -8.1827e-02, -1.1507e-01,\n",
            "        -1.4251e-01, -1.3007e-02, -1.1562e-01, -1.3869e-02, -5.5616e-02,\n",
            "        -1.3748e-01,  2.0038e-02, -1.0893e-01,  3.7848e-02,  2.6370e-02,\n",
            "         1.5256e-01, -1.7048e-02, -5.4340e-03,  6.5350e-02, -4.5541e-02,\n",
            "        -1.3889e-02, -1.3546e-02, -1.9981e-02,  1.8412e-02,  2.5642e-02,\n",
            "         3.9688e-02,  1.5539e-02, -5.1998e-02,  1.5934e-01, -1.1188e-01,\n",
            "        -8.8874e-02,  1.6955e-02, -4.8052e-02, -8.5343e-02, -1.3444e-01,\n",
            "         1.3075e-01,  1.8793e-01, -6.2813e-02, -1.0423e-01, -2.3544e-01,\n",
            "        -9.2175e-02,  5.2775e-02,  2.1384e-02,  2.0662e-02, -4.3228e-02,\n",
            "         1.0775e-02,  9.1036e-02, -3.2879e-02, -5.3863e-02,  1.2356e-01,\n",
            "         8.3869e-03, -8.6142e-02, -8.5249e-02, -8.5097e-03,  1.7478e-02,\n",
            "        -1.6477e-01, -1.0826e-02, -3.2626e-02,  4.0981e-02, -6.2419e-02,\n",
            "         2.2502e-02,  3.1876e-02, -5.2900e-02, -1.3520e+00,  4.5146e-02,\n",
            "        -1.3262e-01, -9.9322e-02,  1.7022e-01, -1.0428e-02, -5.8837e-02,\n",
            "        -5.3764e-02, -5.0277e-02,  1.4743e-03, -1.4262e-02, -9.3190e-02,\n",
            "         7.9996e-03,  1.8367e-01, -1.3173e-04, -1.9939e-01,  3.4430e-02,\n",
            "        -1.8154e-01, -2.7990e-02,  3.4381e-01, -5.9272e-02, -8.2675e-02,\n",
            "        -3.7347e-02,  1.6080e-02,  1.0628e-03,  1.3274e-01,  3.6783e-02,\n",
            "        -2.6304e-02, -1.4164e-01, -1.7573e-02, -2.2060e-02, -1.0065e-02,\n",
            "         2.0352e-02,  1.8773e-01, -7.8374e-02,  3.5247e-02, -4.4983e-02,\n",
            "        -1.5688e-01, -2.4710e-02, -9.4099e-02, -4.4270e-03, -2.4106e-01,\n",
            "         3.7364e-03,  1.2799e-02,  2.1714e-01,  2.4250e-02, -5.8620e-02,\n",
            "        -3.4071e-02, -9.4078e-02,  5.6670e-02,  2.0208e-02, -5.5237e-03,\n",
            "         1.1586e-02, -1.6815e-01,  1.3505e-01, -6.3363e-03,  7.6929e-02,\n",
            "         9.7497e-02, -1.7614e-01, -4.1746e-02, -1.1368e-01, -1.1160e-01,\n",
            "        -2.6994e-01,  2.3484e-02,  3.9368e-02, -1.1853e-01, -3.3546e-01,\n",
            "        -6.0605e-03, -5.0075e-02, -1.1067e-01, -5.9053e-02,  8.0018e-02,\n",
            "        -1.3309e-02, -1.8882e+00, -3.5066e-02, -9.8846e-02,  1.2757e-01,\n",
            "         6.2637e-03, -3.9878e-02,  3.0363e-04, -7.2995e-02, -1.4242e-01,\n",
            "         7.0475e-03, -2.7715e-02,  1.8113e-02, -1.0867e-01, -6.6526e-02,\n",
            "        -2.5656e-03, -1.0198e-01, -1.5856e-01,  1.2822e-02, -1.9299e-02,\n",
            "         5.2201e-02, -3.6497e-02,  4.5979e-02, -2.7962e-02, -4.6199e-02,\n",
            "         4.2883e-02, -1.8909e-02, -4.6832e-02,  3.7635e-02, -3.5515e-02,\n",
            "         2.1452e-02, -4.2567e-02,  4.3887e-02, -1.3776e-02, -6.0612e-02,\n",
            "         1.8401e-02,  1.0900e-01,  4.9528e-02,  7.3153e-02, -2.6886e-03,\n",
            "        -3.9657e-03,  2.6168e-02, -9.5414e-04, -1.2313e-01,  8.4072e-02,\n",
            "         4.1441e-02,  2.9816e-02, -1.4693e-01, -4.8402e-02, -5.3208e-02,\n",
            "        -7.6187e-02, -1.3760e-01,  1.3487e-02,  1.0734e-02, -1.3575e-01,\n",
            "        -1.1805e-02, -7.8494e-03,  8.7178e-02, -6.3253e-02, -1.3882e-01,\n",
            "         1.1763e-01,  1.0118e-01,  3.5979e-02, -4.7568e-02, -3.9294e-02,\n",
            "        -4.7182e-01, -8.1394e-02,  9.1848e-03, -3.3915e-03, -8.3868e-02,\n",
            "         4.2592e-02, -3.1215e-03,  1.6061e-01, -3.4119e-03,  8.8006e-02,\n",
            "         3.6886e-02, -3.5935e-02,  4.2436e-02, -1.8397e-01, -1.6714e-01,\n",
            "         2.4234e-02, -6.4019e-02,  4.0601e-02,  1.4015e-01,  2.4564e-01,\n",
            "        -2.8037e-02,  7.1655e-02, -1.4700e-01,  8.2924e-02, -1.0860e-01,\n",
            "        -5.6395e-02, -9.3245e-02, -5.7061e-02,  5.4943e-02, -6.0407e-02,\n",
            "        -1.3406e-01,  1.4752e-02,  3.0653e-02, -1.9031e-01,  2.8196e-02,\n",
            "        -1.6686e-02, -4.3753e-02, -7.7339e-02,  5.0255e-02, -6.3097e-02,\n",
            "        -9.4029e-02, -1.1533e-01,  1.7190e-01, -6.8709e-02, -9.8590e-02,\n",
            "        -5.1946e-02,  2.4300e-02,  7.0611e-02, -3.7195e-02, -2.9533e-02,\n",
            "        -1.0373e-01, -4.3854e-02, -4.1990e-02, -1.5301e-02,  6.3708e-02,\n",
            "         1.4197e-02, -6.9791e-02, -9.4691e-02, -1.6623e-01,  1.6627e-02,\n",
            "        -4.0652e-02, -1.4417e-01,  6.5713e-02, -4.7441e-02, -6.9841e-02,\n",
            "        -1.5034e-02,  2.7414e-02,  2.7576e-02, -1.4552e-01, -4.4078e-03,\n",
            "         1.2355e-01, -1.7122e-01, -6.5077e-02, -1.5522e-01, -8.7137e-02,\n",
            "        -1.2388e-01,  6.8649e-02, -7.5485e-02, -4.4840e-03, -7.6560e-02,\n",
            "        -9.5748e-02,  2.5241e-01, -8.6654e-02, -1.9855e-02, -7.0882e-02,\n",
            "         5.3095e-02, -1.8605e-02, -9.2690e-02,  1.7085e-02,  8.7068e-02,\n",
            "        -4.7949e-02, -1.3915e-02,  4.0671e-03, -3.1902e-02, -3.1709e-01,\n",
            "         2.4549e-02,  1.7502e-02, -3.6478e-02, -8.7535e-02, -1.0812e-01,\n",
            "        -1.9368e-01,  5.6600e-02,  6.2011e-02,  8.3298e-02,  2.9690e-02,\n",
            "        -5.7632e-02, -1.0018e-01,  6.3107e-02,  1.1186e-01,  4.4521e-02,\n",
            "        -5.9027e-02, -2.8306e-02,  1.0422e-01, -9.7934e-04, -1.3316e-01,\n",
            "        -4.8509e-02, -6.3272e-02,  1.2613e-01, -6.9826e-02, -1.3674e-01,\n",
            "         2.8832e-02, -7.8703e-02,  8.5840e-02, -3.6097e-02,  5.7578e-02,\n",
            "        -1.5922e-01,  8.2077e-02,  1.3649e-01, -7.2033e-02, -1.8363e-02,\n",
            "        -3.8284e-02, -3.4231e-02, -1.8843e-01,  7.0095e-02, -7.9218e-02,\n",
            "        -5.9366e-02, -5.6867e-05, -7.8419e-02,  7.7719e-02, -4.0908e-02,\n",
            "        -4.4340e-02,  7.3448e-02, -1.0578e-01, -2.0239e-01,  1.1986e-02,\n",
            "        -5.2369e-02, -4.3708e-02,  1.3372e-02, -2.5477e-01, -5.7132e-02,\n",
            "         3.7881e-02, -7.2967e-02, -2.6268e-02,  1.2307e-01,  4.9715e-02,\n",
            "         1.4006e-02, -1.2879e-02, -6.9322e-02,  9.7442e-02,  1.2173e-01,\n",
            "         5.7379e-02,  1.5167e-03, -4.3250e-02, -1.1526e-01,  5.1005e-03,\n",
            "         9.7860e-02, -8.3183e-02,  2.7053e-02,  2.8275e-02,  4.4384e-02,\n",
            "        -1.7175e-01, -1.0611e-01, -4.4474e-02,  2.4240e-02, -1.7871e-01,\n",
            "         2.4465e-01,  1.0488e-01, -4.2515e-02, -2.7527e-02,  5.0208e-02,\n",
            "        -3.9354e-02,  6.3300e-02, -9.6138e-02, -6.3638e-03,  7.2867e-02,\n",
            "         8.9386e-02,  9.6692e-03,  4.8196e-02,  1.4490e-01, -5.4485e-02,\n",
            "         3.7006e-02,  6.7020e-02,  2.1314e-01,  5.7172e-02,  3.8746e-02,\n",
            "        -5.2525e-02,  4.3298e-02, -1.0086e-01, -1.3973e-01,  1.1766e-01,\n",
            "        -1.5524e-01,  3.9553e-02,  1.8500e-01,  4.3291e-02,  6.8795e-03,\n",
            "         1.1762e-01,  2.7638e-02,  7.9839e-02,  2.3957e-02,  5.0306e-02,\n",
            "        -3.6551e-02,  1.1517e-01,  1.6246e-01, -7.3600e-03, -4.1794e-02,\n",
            "         7.8460e-02,  2.4231e-02,  1.3117e-01, -3.5845e-02,  4.5641e-02,\n",
            "        -2.2538e-02,  4.5256e-03,  1.8366e-01,  5.2601e-02,  2.3233e-02,\n",
            "         1.4683e-01, -4.2960e-02, -3.5630e-02, -6.2859e-02,  8.5867e-02,\n",
            "        -8.0193e-02,  9.7035e-02, -9.7163e-02,  8.4262e-02, -5.3432e-02,\n",
            "         7.3728e-02, -4.5167e-02,  5.1862e-02, -9.0663e-02,  9.1148e-03,\n",
            "        -5.4825e-02,  1.9205e-02,  5.7003e-02, -4.8414e-02, -3.9301e-02,\n",
            "        -1.5805e-01, -1.3792e-01, -1.9177e-02,  1.3517e-02,  4.7648e-02,\n",
            "         1.6847e-03, -5.9760e-02, -1.3280e-01, -1.3314e-02,  5.7509e-03,\n",
            "         9.9600e-02, -1.9460e-02, -5.7305e-02, -9.6649e-03,  8.1935e-02,\n",
            "         1.6404e-02,  1.2222e-01,  4.2565e-02,  3.5585e-03,  8.3056e-02,\n",
            "        -1.6466e-01,  5.9435e-02, -4.2335e-03,  3.8446e-02, -2.8132e-02,\n",
            "        -2.5972e-02, -2.7214e-01, -3.8381e-02,  2.9798e-02,  4.6312e-02,\n",
            "        -9.1643e-01,  2.4492e-03,  4.6300e-03,  4.1784e-02,  3.2197e-02,\n",
            "        -1.5715e-01, -1.7571e-01, -1.7412e-01, -1.1094e-01, -9.1469e-02,\n",
            "        -5.7015e-02, -3.4931e-02, -9.5130e-02,  2.2514e-02,  9.2526e-02,\n",
            "        -5.1622e-02, -8.4395e-02, -3.9269e-02, -1.2635e-01,  7.3185e-02,\n",
            "        -6.1763e-02,  1.1407e-02, -1.3769e-01, -1.4406e-01, -5.7378e-02,\n",
            "         6.8709e-02, -5.2696e-02, -1.0309e-01,  5.8251e-02,  8.8800e-02,\n",
            "        -5.8064e-02, -5.7135e-02,  9.2848e-02,  8.7045e-02, -3.6935e-02,\n",
            "        -1.4810e-01, -2.4457e-01,  6.8419e-02, -1.1099e-01, -2.3767e-02,\n",
            "         1.1027e-01, -6.5348e-02,  1.4737e-03, -1.1822e-01, -2.5063e-02,\n",
            "        -8.1075e-02, -4.6832e-02, -5.4376e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.10.intermediate.dense.weight', Parameter containing:\n",
            "tensor([[-0.0706, -0.0247,  0.0210,  ..., -0.0297,  0.0292,  0.0764],\n",
            "        [ 0.0700,  0.0476, -0.0156,  ..., -0.0873,  0.0162, -0.0298],\n",
            "        [ 0.0143, -0.0025,  0.0005,  ...,  0.0052, -0.0301, -0.0048],\n",
            "        ...,\n",
            "        [ 0.0622, -0.0010,  0.0686,  ..., -0.0191, -0.0082, -0.0104],\n",
            "        [-0.0365, -0.0245, -0.0049,  ...,  0.0207, -0.0059,  0.0157],\n",
            "        [-0.0172, -0.0094,  0.0398,  ..., -0.0134,  0.0020,  0.0644]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.10.intermediate.dense.bias', Parameter containing:\n",
            "tensor([-0.1388, -0.0640, -0.1408,  ..., -0.0511, -0.1060, -0.1020],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.10.output.dense.weight', Parameter containing:\n",
            "tensor([[ 0.0292, -0.0296, -0.0141,  ...,  0.0272,  0.0115,  0.0043],\n",
            "        [ 0.0261,  0.0077,  0.0487,  ..., -0.0432, -0.0136, -0.0180],\n",
            "        [-0.0226,  0.0414, -0.0277,  ...,  0.0526,  0.0347, -0.0258],\n",
            "        ...,\n",
            "        [ 0.0122,  0.0002,  0.1214,  ..., -0.0158,  0.0339,  0.0311],\n",
            "        [-0.0337, -0.0104,  0.0046,  ..., -0.0629,  0.0016, -0.0647],\n",
            "        [-0.0038, -0.0096,  0.0562,  ..., -0.0357,  0.0408,  0.0315]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.10.output.dense.bias', Parameter containing:\n",
            "tensor([-1.7995e-02,  1.0971e-01, -2.3842e-02, -3.3510e-02,  1.0491e-01,\n",
            "         1.2462e-01,  9.1538e-02,  8.3362e-02, -8.3616e-02, -1.1651e-01,\n",
            "        -7.5646e-02,  1.6651e-02, -4.5650e-02,  8.4964e-02,  7.7398e-02,\n",
            "         2.6110e-01,  1.5501e-01,  1.0969e-01, -1.2609e-03, -4.8352e-02,\n",
            "         1.0677e-01, -7.0175e-02,  5.0651e-02,  1.7063e-01,  5.2590e-02,\n",
            "        -2.4445e-02,  1.3517e-01,  5.3397e-04, -4.3884e-02,  2.9947e-02,\n",
            "         2.4362e-02, -9.3155e-02,  1.3213e-01, -8.5065e-02,  1.1234e-02,\n",
            "         4.9889e-02, -8.6082e-02, -2.6871e-02,  5.6951e-02, -3.1787e-04,\n",
            "        -6.0441e-02, -3.0077e-02, -2.2734e-02,  1.3425e-01,  4.2416e-02,\n",
            "         2.3253e-02, -7.0987e-02,  1.2654e-01, -1.1863e-01,  2.8142e-02,\n",
            "        -5.5107e-02,  9.3082e-02,  1.9234e-01,  1.0354e-01,  1.1912e-01,\n",
            "         1.3703e-01, -1.1484e-01, -1.0485e-01, -5.5315e-02, -7.5910e-02,\n",
            "         7.5704e-02,  4.5609e-02,  9.9478e-02, -1.2149e-01,  5.7118e-02,\n",
            "         2.3187e-02, -4.3808e-02,  6.8687e-02, -8.9568e-02, -5.5708e-02,\n",
            "         1.3154e-02,  2.5318e-02, -3.9467e-02, -3.2020e-02,  1.3836e-01,\n",
            "        -5.6971e-02, -1.4879e-01,  1.8833e-01,  6.1097e-02,  1.4927e-01,\n",
            "         1.7302e-03,  5.4062e-02,  1.2872e-01,  6.3467e-03,  4.2999e-02,\n",
            "        -8.5094e-02,  1.0628e-02, -4.1483e-02, -1.6341e-01,  7.1847e-02,\n",
            "        -1.3153e-01, -4.5940e-02,  6.6968e-02,  9.1351e-02,  7.3918e-02,\n",
            "        -8.7973e-02,  9.0517e-02,  4.0386e-03,  5.3734e-02, -8.8457e-02,\n",
            "         3.7241e-03, -3.0153e-02,  9.3921e-02,  2.3458e-02, -1.0372e-01,\n",
            "        -3.2905e-03, -1.9796e-01, -1.2808e-01, -5.7740e-02,  2.9826e-02,\n",
            "         3.8620e-02,  1.4648e-01, -1.4630e-01, -1.0204e-01, -4.9858e-02,\n",
            "         1.3264e-01,  6.0734e-02, -5.9396e-02, -1.7528e-02, -4.7156e-02,\n",
            "         7.8057e-02,  1.0884e-01,  3.9412e-03,  8.1177e-02,  4.7541e-02,\n",
            "         3.7106e-02,  5.4457e-02,  4.1072e-02, -1.2624e-01, -6.6881e-02,\n",
            "        -7.8980e-03,  4.8576e-02,  1.5816e-02,  2.0219e-02, -1.3598e-01,\n",
            "        -4.5142e-02,  5.5800e-02, -3.8924e-02,  1.8318e-02,  7.0822e-02,\n",
            "        -3.7643e-02, -7.8313e-02, -1.3669e-01,  6.0728e-02,  1.5479e-01,\n",
            "        -7.3164e-02, -1.5072e-02, -2.0047e-01, -4.4057e-02,  1.2028e-01,\n",
            "        -9.0094e-02,  2.1857e-02, -9.3233e-02, -4.3292e-02, -3.9760e-02,\n",
            "         9.2491e-02,  2.5874e-02,  8.2756e-02,  2.6395e-02, -1.1891e-01,\n",
            "         3.0343e-02, -2.8207e-03,  3.4420e-02, -1.0121e-01,  6.0478e-02,\n",
            "         1.1989e-01,  6.6446e-02,  8.3514e-02, -7.5873e-02,  1.3789e-02,\n",
            "        -9.1020e-02, -4.9797e-02, -1.7504e-02, -9.6077e-02, -2.8295e-02,\n",
            "         6.4154e-02,  1.2639e-02,  3.9047e-02,  1.6750e-01, -8.7566e-03,\n",
            "        -1.1674e+00,  6.8001e-02, -5.3680e-02,  1.0207e-02,  4.6994e-03,\n",
            "         4.7080e-02,  1.8786e-01, -6.4896e-02,  8.4581e-02, -3.2309e-02,\n",
            "        -8.1123e-02, -4.1923e-02,  6.4483e-02, -1.3005e-01,  1.8813e-01,\n",
            "        -6.5983e-02, -1.2561e-01,  1.4983e-01,  1.1173e-01, -4.4150e-02,\n",
            "        -1.5534e-01, -9.1595e-02,  1.1460e-02, -1.3207e-01, -1.5256e-01,\n",
            "        -4.1176e-02, -4.7700e-02,  1.6164e-02, -2.9475e-02,  1.5617e-01,\n",
            "        -8.6671e-02, -8.2033e-02,  5.6888e-02,  9.6921e-02,  4.4787e-02,\n",
            "        -2.3526e-02, -1.0607e-01, -1.1351e-01,  1.1402e-01, -1.9004e-01,\n",
            "         5.1286e-02,  6.8433e-02, -1.1605e-01,  7.1933e-02, -3.2286e-02,\n",
            "         2.5821e-01,  1.7323e-01, -8.2745e-02,  5.2430e-02, -2.2953e-02,\n",
            "        -8.3487e-02, -9.7600e-03,  1.4317e-01, -1.0847e-01, -1.7219e-02,\n",
            "         1.3492e-01, -5.2467e-02, -6.8475e-02,  1.3292e-01, -1.9564e-01,\n",
            "         6.8070e-02,  1.9418e-01, -6.2243e-02,  1.2608e-01, -1.2321e-01,\n",
            "        -2.5287e-02,  8.7148e-02, -9.1371e-02, -4.5408e-02, -8.2579e-03,\n",
            "        -1.4942e-01,  1.0954e-01,  7.2066e-02, -9.6692e-02,  1.8292e-03,\n",
            "        -4.9481e-02, -6.8682e-02, -7.4818e-02,  5.9089e-02,  1.7255e-01,\n",
            "         1.7968e-02,  1.3947e-02, -2.7290e-02, -5.1044e-02, -6.8944e-02,\n",
            "        -1.2348e-01, -1.1463e-02, -1.5290e-01,  5.5349e-02, -2.8807e-02,\n",
            "         1.2728e-01,  7.0848e-02, -1.0458e-01,  7.3960e-03,  1.3256e-02,\n",
            "         3.7106e-02,  3.7843e-02,  8.3666e-02, -4.1281e-02, -4.3076e-02,\n",
            "        -1.5304e-02,  9.2559e-02, -8.4911e-02, -1.4449e-01, -1.6689e-01,\n",
            "        -8.7871e-02,  1.1766e-01, -1.2728e-01, -5.5489e-02, -5.5576e-02,\n",
            "        -1.1805e-01,  1.1801e-01, -4.3387e-02, -3.3354e-02,  1.1047e-01,\n",
            "         1.3254e-01, -5.7403e-03,  1.4760e-02, -1.6017e-02, -1.8471e-02,\n",
            "        -7.1228e-02,  8.6746e-02,  6.9329e-03,  5.6731e-02,  1.4844e-01,\n",
            "        -6.3354e-02, -6.8515e-02, -9.7300e-02, -8.4891e-01,  4.4882e-03,\n",
            "        -1.4360e-01, -1.0882e-01,  8.4147e-02,  1.2977e-02,  5.2242e-02,\n",
            "        -9.4434e-02, -5.3578e-02,  4.7063e-02, -4.2293e-02, -2.9722e-02,\n",
            "        -9.7553e-02,  5.2752e-02,  1.8653e-01, -2.2487e-02, -8.3135e-02,\n",
            "        -8.6357e-03, -4.1339e-02,  5.3893e-02,  6.4192e-02, -1.1824e-02,\n",
            "        -6.3900e-02,  8.7731e-02,  9.4618e-02, -2.0619e-02, -1.6471e-01,\n",
            "         8.4319e-03, -8.9403e-02, -1.3119e-01,  5.4515e-02, -1.9020e-01,\n",
            "         4.0871e-02, -1.4934e-02,  1.8418e-02,  5.5042e-03,  1.0856e-01,\n",
            "        -1.3969e-01, -6.1153e-03, -1.8192e-01, -7.3493e-02, -1.1588e-01,\n",
            "        -1.6369e-01, -6.8198e-02,  1.7089e-01,  6.0416e-04, -5.1924e-02,\n",
            "        -1.2056e-01, -7.9560e-02,  2.1280e-01,  4.4210e-02,  6.1441e-03,\n",
            "         9.1242e-02, -1.0575e-01,  1.1069e-01,  8.6142e-02,  5.5269e-02,\n",
            "         1.0024e-01, -1.9725e-02,  1.7373e-01, -3.5089e-02, -1.5361e-02,\n",
            "        -1.8871e-01, -1.5235e-01,  9.7347e-02, -1.1252e-01, -4.6535e-02,\n",
            "        -8.7838e-02,  1.0729e-01, -1.3972e-02, -1.9656e-01, -1.8717e-02,\n",
            "        -2.1708e-03,  2.8446e-01, -2.8764e-02, -8.0554e-02,  1.3785e-01,\n",
            "        -6.8261e-02, -1.1250e-01,  4.1699e-02, -5.4590e-02, -7.5197e-02,\n",
            "         7.8242e-03,  2.3785e-01, -2.4773e-02, -3.7007e-02, -1.9728e-02,\n",
            "        -2.8653e-02, -1.1139e-01, -7.8558e-02,  1.5044e-01, -4.3773e-02,\n",
            "        -4.0233e-02, -1.3825e-01,  7.0339e-02,  5.6669e-02, -1.0575e-01,\n",
            "        -3.6392e-04, -2.6391e-02, -6.1202e-02,  3.8423e-02, -4.0577e-02,\n",
            "        -7.4863e-02, -3.5732e-02, -6.3108e-02, -9.4867e-02, -1.0796e-01,\n",
            "         8.1215e-02,  5.8513e-02, -1.2576e-01, -2.1508e-02, -2.2599e-02,\n",
            "         2.9836e-02, -7.2323e-02,  1.0374e-01, -5.4196e-02,  4.6800e-03,\n",
            "         1.3418e-01,  1.2710e-01, -6.0794e-02,  3.6146e-02,  8.2812e-02,\n",
            "        -1.3127e-01, -6.2629e-02, -2.6723e-02,  1.8703e-01, -4.4541e-02,\n",
            "         1.9774e-02,  1.5291e-01, -1.4476e-01, -3.6532e-02,  5.8070e-02,\n",
            "         1.2938e-01,  4.5883e-02,  4.8294e-02, -8.8303e-02, -1.4190e-02,\n",
            "        -1.0303e-01, -9.3362e-02,  1.6577e-01,  7.9578e-03,  2.6112e-02,\n",
            "        -1.4884e-02, -1.3643e-03,  8.6507e-02,  5.8645e-02,  8.7112e-02,\n",
            "        -8.0021e-02,  3.8295e-03,  3.7718e-02, -1.4079e-01, -1.7847e-03,\n",
            "         1.1650e-02, -1.7276e-01, -5.6862e-02,  5.0386e-02, -2.6118e-02,\n",
            "         5.5725e-02,  1.3304e-01, -2.2176e-01, -9.8008e-03, -2.6710e-03,\n",
            "        -5.5598e-02, -2.6109e-02, -2.4123e-02,  8.9729e-02, -1.5887e-01,\n",
            "        -5.4199e-02, -7.5078e-02,  4.4806e-02, -3.7909e-02, -1.2216e-01,\n",
            "        -3.8444e-02,  1.3665e-01,  7.6924e-03,  1.0543e-01, -1.0625e-01,\n",
            "        -2.0793e-02, -3.1573e-02,  9.8600e-02, -1.5674e-03,  1.7399e-02,\n",
            "        -8.9313e-02,  3.1711e-02,  1.1630e-01, -4.5986e-02, -5.1431e-02,\n",
            "        -8.5902e-02, -5.6980e-02, -4.4693e-02, -4.8815e-02,  7.4162e-02,\n",
            "         2.1798e-02,  5.7912e-02, -1.1795e-01, -1.4928e-01, -6.7437e-02,\n",
            "         1.3103e-01, -3.8294e-02,  6.0068e-02, -4.5186e-02, -5.2643e-02,\n",
            "        -7.0272e-02,  2.7545e-02, -6.8696e-03, -6.4726e-02,  6.8701e-02,\n",
            "         1.4413e-01,  5.9031e-02,  7.3412e-02, -9.7229e-02, -8.4358e-02,\n",
            "         5.9435e-03,  2.9162e-02,  6.4978e-02, -1.2677e-02,  9.8074e-02,\n",
            "        -7.3698e-02, -1.1442e-02, -2.4263e-02, -4.2626e-02, -7.5566e-02,\n",
            "         4.6536e-02, -1.3282e-01, -1.7837e-01,  2.3021e-02,  1.4317e-02,\n",
            "        -9.5872e-02,  4.2836e-02, -4.9602e-02,  3.7086e-02, -1.7116e-01,\n",
            "         5.6979e-02,  1.1122e-01,  8.7447e-02,  1.0588e-05,  2.8663e-02,\n",
            "        -5.8395e-02, -1.0470e-01,  1.0111e-01,  1.4072e-01,  5.6217e-02,\n",
            "         8.2693e-03, -7.7912e-02, -4.7529e-03,  4.7483e-02,  5.6406e-02,\n",
            "        -4.5829e-02,  7.4970e-02,  5.2387e-02,  2.4078e-02, -7.2284e-02,\n",
            "        -9.7162e-02,  8.1108e-02,  2.2771e-02, -1.7954e-02, -9.2367e-02,\n",
            "         2.4800e-02, -2.1451e-02,  7.1171e-02,  5.6178e-02, -4.3722e-02,\n",
            "        -6.3887e-02,  1.6619e-02,  1.0942e-01,  3.6518e-02,  6.9667e-02,\n",
            "         6.7877e-02,  6.4529e-02, -1.1764e-01,  4.0310e-02, -2.0631e-01,\n",
            "        -6.4884e-02,  6.0020e-02,  6.6390e-02, -1.5210e-02, -1.7104e-02,\n",
            "        -8.4333e-02,  5.3627e-02, -1.1254e-01, -3.0019e-02, -3.0667e-02,\n",
            "        -4.9629e-02, -5.9274e-02,  1.3652e-01, -6.9717e-02,  4.4389e-02,\n",
            "         8.6893e-02, -8.0192e-02,  6.8680e-02, -5.1807e-02,  1.1188e-01,\n",
            "        -7.6018e-02, -1.5228e-02, -1.2486e-01,  6.3322e-02,  1.1962e-01,\n",
            "         1.2645e-01,  2.2796e-03,  1.1025e-01, -2.5256e-02,  5.4805e-03,\n",
            "         1.7962e-01, -6.1522e-02,  9.4068e-02,  1.7145e-01,  8.6405e-02,\n",
            "        -7.6339e-02, -1.1537e-01,  1.0862e-01,  3.1586e-02, -1.4632e-01,\n",
            "         4.1031e-02, -1.6650e-02, -2.2863e-02,  9.6774e-02,  5.2476e-02,\n",
            "         1.9528e-02,  6.4902e-02,  8.0217e-02,  2.0659e-02,  3.0861e-02,\n",
            "         1.7118e-01, -4.5153e-03,  1.5963e-01,  1.5500e-01, -1.8510e-03,\n",
            "         3.3119e-02,  1.8976e-01,  6.8316e-02,  3.6643e-02, -1.1006e-01,\n",
            "        -1.6243e-02,  1.7070e-01, -1.0074e-01, -8.3597e-02,  1.5089e-02,\n",
            "        -1.1647e-01,  1.7752e-02,  1.1247e-01,  7.3892e-02,  1.8482e-01,\n",
            "         2.2276e-02,  7.1353e-02,  5.4053e-02,  4.3755e-02, -7.2248e-02,\n",
            "        -7.1751e-02,  8.1287e-02,  1.1767e-01,  6.6588e-02, -2.5462e-02,\n",
            "        -4.4256e-03, -1.2061e-01,  5.2382e-02, -9.2048e-02,  1.1553e-01,\n",
            "         3.9400e-02, -4.0201e-02, -9.1284e-03,  5.1471e-02,  1.8908e-01,\n",
            "         2.1997e-01, -1.2138e-02, -1.2784e-01,  2.3819e-03,  1.4362e-01,\n",
            "         8.8826e-02,  2.9723e-02,  7.2240e-02, -4.8822e-02, -4.2037e-02,\n",
            "         4.6750e-02, -1.7720e-02,  1.4852e-01, -7.7128e-02, -7.7611e-02,\n",
            "         4.2609e-02,  3.1927e-02,  8.9026e-03, -1.0540e-01,  2.4909e-02,\n",
            "        -8.3204e-02, -1.3930e-01, -1.4180e-02,  7.9780e-03,  4.1389e-02,\n",
            "        -3.2852e-02, -1.4304e-02, -9.4408e-02,  8.5539e-02, -6.1755e-02,\n",
            "         1.0087e-01,  1.0197e-01, -9.3176e-02,  1.0562e-01,  1.4927e-01,\n",
            "         1.0949e-01,  1.5574e-01,  1.2120e-01,  5.3313e-02,  9.8136e-02,\n",
            "        -9.7711e-02,  2.0255e-02, -6.6456e-02,  1.7323e-01, -1.8183e-02,\n",
            "         1.1731e-02, -7.1566e-02,  9.5698e-03,  9.6228e-03, -3.0733e-02,\n",
            "        -6.7785e-02,  5.7257e-02, -1.1822e-01,  1.0511e-02,  1.5506e-01,\n",
            "        -4.9383e-02, -2.6933e-02, -1.5790e-01,  1.5996e-02,  9.9111e-03,\n",
            "         7.2168e-02,  5.7295e-02, -1.8042e-02,  7.9364e-02,  6.9038e-02,\n",
            "        -5.3469e-03, -5.9349e-02,  1.6824e-01, -7.2458e-02,  8.6870e-02,\n",
            "        -2.1295e-02,  1.3599e-03, -1.3371e-01, -3.1909e-02,  4.4527e-02,\n",
            "         4.6423e-02, -7.2203e-03, -6.8191e-02, -1.0633e-02,  1.2906e-01,\n",
            "        -3.5127e-02, -1.3789e-01, -4.5097e-02,  1.7260e-02, -1.6271e-01,\n",
            "        -1.1913e-01, -1.1071e-01,  1.0279e-01, -1.4868e-02, -3.4222e-02,\n",
            "         1.1403e-01, -1.0091e-01,  1.7242e-01, -8.6222e-02, -9.3097e-02,\n",
            "         3.5066e-02,  3.8108e-02,  3.5905e-03], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.10.output.LayerNorm.weight', Parameter containing:\n",
            "tensor([0.8273, 0.8597, 0.7912, 0.8127, 0.8274, 0.8271, 0.8111, 0.7986, 0.8362,\n",
            "        0.8129, 0.8104, 0.7973, 0.7927, 0.8119, 0.8387, 0.7919, 0.8002, 0.8188,\n",
            "        0.8604, 0.8405, 0.8318, 0.8130, 0.8434, 0.8256, 0.8152, 0.8129, 0.8185,\n",
            "        0.8374, 0.8148, 0.8530, 0.8174, 0.8290, 0.8563, 0.8188, 0.8557, 0.8421,\n",
            "        0.7919, 0.8281, 0.8423, 0.8192, 0.8373, 0.8306, 0.8320, 0.7929, 0.7954,\n",
            "        0.8180, 0.4970, 0.8212, 0.8710, 0.8687, 0.7985, 0.8531, 0.8373, 0.7884,\n",
            "        0.7940, 0.8239, 0.8573, 0.8158, 0.8325, 0.8199, 0.8176, 0.7926, 0.8226,\n",
            "        0.8038, 0.8297, 0.8112, 0.8257, 0.8419, 0.8096, 0.8237, 0.8434, 0.8214,\n",
            "        0.8301, 0.8116, 0.8266, 0.7900, 0.8137, 0.8051, 0.8181, 0.8179, 0.8230,\n",
            "        0.8366, 0.8232, 0.8053, 0.8074, 0.8351, 0.7987, 0.8627, 0.8499, 0.7886,\n",
            "        0.7825, 0.8175, 0.7969, 0.7911, 0.8222, 0.8133, 0.8174, 0.8067, 0.8251,\n",
            "        0.8375, 0.8342, 0.8667, 0.8310, 0.8138, 0.8208, 0.7957, 0.8359, 0.8171,\n",
            "        0.8286, 0.8028, 0.8087, 0.8219, 0.8533, 0.8269, 0.8189, 0.8482, 0.8171,\n",
            "        0.8487, 0.8179, 0.8452, 0.8064, 0.8117, 0.8422, 0.3965, 0.8228, 0.8103,\n",
            "        0.8062, 0.7932, 0.8165, 0.8311, 0.8378, 0.8362, 0.8162, 0.8181, 0.8028,\n",
            "        0.8171, 0.8131, 0.8337, 0.8263, 0.8215, 0.8403, 0.8594, 0.7818, 0.7253,\n",
            "        0.8299, 0.8187, 0.8312, 0.7728, 0.8245, 0.8273, 0.8048, 0.7919, 0.8001,\n",
            "        0.8238, 0.8344, 0.8013, 0.8150, 0.8224, 0.8397, 0.8329, 0.8400, 0.8410,\n",
            "        0.8370, 0.8531, 0.7777, 0.8362, 0.8352, 0.7994, 0.7844, 0.8245, 0.8166,\n",
            "        0.8282, 0.8355, 0.8324, 0.8171, 0.8315, 0.8158, 0.8214, 0.8069, 0.7942,\n",
            "        0.1450, 0.8259, 0.8334, 0.8395, 0.8286, 0.8479, 0.8137, 0.8094, 0.8057,\n",
            "        0.8351, 0.8161, 0.8335, 0.8292, 0.8176, 0.8149, 0.8408, 0.8403, 0.8258,\n",
            "        0.8222, 0.8097, 0.8211, 0.8226, 0.8120, 0.7863, 0.8082, 0.4059, 0.8482,\n",
            "        0.8135, 0.8325, 0.8117, 0.8086, 0.8243, 0.8312, 0.7850, 0.8252, 0.8008,\n",
            "        0.8184, 0.8311, 0.7966, 0.8279, 0.8327, 0.8157, 0.8512, 0.7968, 0.8399,\n",
            "        0.5803, 0.8320, 0.8049, 0.8168, 0.6614, 0.7904, 0.8386, 0.8002, 0.8371,\n",
            "        0.8311, 0.8233, 0.8374, 0.8517, 0.8075, 0.7810, 0.8043, 0.8287, 0.8310,\n",
            "        0.7545, 0.8091, 0.8130, 0.8315, 0.8237, 0.7936, 0.8294, 0.8144, 0.8791,\n",
            "        0.8355, 0.8370, 0.8485, 0.8280, 0.8163, 0.7991, 0.8133, 0.8094, 0.8394,\n",
            "        0.8253, 0.8452, 0.8121, 0.8158, 0.7934, 0.7687, 0.8261, 0.8256, 0.8363,\n",
            "        0.8233, 0.8185, 0.8239, 0.8132, 0.8256, 0.8243, 0.8149, 0.8408, 0.8208,\n",
            "        0.8138, 0.8163, 0.7973, 0.8426, 0.8028, 0.7939, 0.7976, 0.8272, 0.8379,\n",
            "        0.8260, 0.8205, 0.8320, 0.8118, 0.8349, 0.8141, 0.8088, 0.8105, 0.8428,\n",
            "        0.8186, 0.8262, 0.8469, 0.8352, 0.8187, 0.8220, 0.8426, 0.8953, 0.8250,\n",
            "        0.8079, 0.8207, 1.6714, 0.8294, 0.7880, 0.8008, 0.7966, 0.8185, 0.8555,\n",
            "        0.8230, 0.8749, 0.8322, 0.8289, 0.8166, 0.8341, 0.8206, 0.8092, 0.8114,\n",
            "        0.8251, 0.8403, 0.8264, 0.6936, 0.8250, 0.8249, 0.8277, 0.7653, 0.8449,\n",
            "        0.8456, 0.7697, 0.8154, 0.7930, 0.8215, 0.8228, 0.8101, 0.8201, 0.8852,\n",
            "        0.8160, 0.8325, 0.8228, 0.8055, 0.8194, 0.8051, 0.8539, 0.8013, 0.8210,\n",
            "        0.8187, 0.7815, 0.8133, 0.8271, 0.8175, 0.8074, 0.8010, 0.8360, 0.8063,\n",
            "        0.7781, 0.7964, 0.8130, 0.8201, 0.8111, 0.8239, 0.7888, 0.8238, 0.8276,\n",
            "        0.8202, 0.7845, 0.8020, 0.8486, 0.8324, 0.7349, 0.8443, 0.8322, 0.8151,\n",
            "        0.7926, 0.8470, 0.8378, 0.1852, 0.8060, 0.8118, 0.7750, 0.8366, 0.8364,\n",
            "        0.8423, 0.8459, 0.8031, 0.8347, 0.8397, 0.8388, 0.8104, 0.8390, 0.8678,\n",
            "        0.8161, 0.8127, 0.8537, 0.8327, 0.8169, 0.8132, 0.8189, 0.8318, 0.8313,\n",
            "        0.7993, 0.8308, 0.8307, 0.8116, 0.8404, 0.8250, 0.7948, 0.8266, 0.8650,\n",
            "        0.8057, 0.8128, 0.8006, 0.8119, 0.8175, 0.8201, 0.8344, 0.8236, 0.7972,\n",
            "        0.8282, 0.8399, 0.8198, 0.8253, 0.8390, 0.8354, 0.8198, 0.8079, 0.7840,\n",
            "        0.8349, 0.8008, 0.8354, 0.8714, 0.8202, 0.8363, 0.8433, 0.8275, 0.8186,\n",
            "        0.8384, 0.8247, 0.8012, 0.8240, 0.7401, 0.8366, 0.8256, 0.8084, 0.7896,\n",
            "        0.8291, 0.8187, 0.8238, 0.8233, 0.8616, 0.8359, 0.8294, 0.8439, 0.7895,\n",
            "        0.8019, 0.8218, 0.8339, 0.8189, 0.8238, 0.7702, 0.8201, 0.8073, 0.7893,\n",
            "        0.8113, 0.8165, 0.6834, 0.8114, 0.8481, 0.8409, 0.8116, 0.8058, 0.8391,\n",
            "        0.8540, 0.8109, 0.8212, 0.8303, 0.8101, 0.8116, 0.8115, 0.8268, 0.8167,\n",
            "        0.8220, 0.8025, 0.8240, 0.8222, 0.8286, 0.8318, 0.8273, 0.8253, 0.8350,\n",
            "        0.8268, 0.8093, 0.8169, 0.8478, 0.8354, 0.8033, 0.8238, 0.7871, 0.8056,\n",
            "        0.8211, 0.8452, 0.8161, 0.8073, 0.8464, 0.8281, 0.8235, 0.8283, 0.8307,\n",
            "        0.8222, 0.7855, 0.8053, 0.8373, 0.8079, 0.8344, 0.8179, 0.8235, 0.8201,\n",
            "        0.8211, 0.8243, 0.8237, 0.8234, 0.5874, 0.8415, 0.8194, 0.8178, 0.8204,\n",
            "        0.8356, 0.8469, 0.8369, 0.8249, 0.7611, 0.8630, 0.8374, 0.8169, 0.8775,\n",
            "        0.8242, 0.8208, 0.8545, 0.8420, 0.8186, 0.8117, 0.8278, 0.8381, 0.8080,\n",
            "        0.8173, 0.8495, 0.8113, 0.8132, 0.8182, 0.8221, 0.8198, 0.8587, 0.7927,\n",
            "        0.8494, 0.8098, 0.8482, 0.8227, 0.8483, 0.8245, 0.8243, 0.8201, 0.7882,\n",
            "        0.8251, 0.8499, 0.8260, 0.8281, 0.8260, 0.8090, 0.8070, 0.8237, 0.8127,\n",
            "        0.8481, 0.8055, 0.8227, 0.8249, 0.8287, 0.8207, 0.8289, 0.8168, 0.8438,\n",
            "        0.8201, 0.8158, 0.8584, 0.8299, 0.8603, 0.8508, 0.8443, 0.8617, 0.8318,\n",
            "        0.8129, 0.8080, 0.8298, 0.8282, 0.8145, 0.8550, 0.8275, 0.8496, 0.8239,\n",
            "        0.8107, 0.8427, 0.8133, 0.8370, 0.8111, 0.7928, 0.8511, 0.8256, 0.8159,\n",
            "        0.8265, 0.8169, 0.7986, 0.8157, 0.8122, 0.8265, 0.8530, 0.8061, 0.7315,\n",
            "        0.8131, 0.8578, 0.8576, 0.8391, 0.8297, 0.8245, 0.8006, 0.8158, 0.8346,\n",
            "        0.8133, 0.8205, 0.8093, 0.7974, 0.8281, 0.8084, 0.8319, 0.8363, 0.8168,\n",
            "        0.8062, 0.8346, 0.7844, 0.8169, 0.7846, 0.8469, 0.8109, 0.7964, 0.7649,\n",
            "        0.7958, 0.8158, 0.8326, 0.8266, 0.8002, 0.8137, 0.8065, 0.8098, 0.8077,\n",
            "        0.7860, 0.8164, 0.8205, 0.8393, 0.8439, 0.8411, 0.8476, 0.8092, 0.8113,\n",
            "        0.8384, 0.8239, 0.8108, 0.8111, 0.7772, 0.8289, 0.8215, 0.8153, 0.8190,\n",
            "        0.8190, 0.8100, 0.8405, 0.8342, 0.8349, 0.8395, 0.8315, 0.8235, 0.8161,\n",
            "        0.8347, 0.8027, 0.8226, 0.8238, 0.8289, 0.8343, 0.8369, 0.8326, 0.8224,\n",
            "        0.8344, 0.8386, 0.8268, 0.8087, 0.8206, 0.8161, 0.8125, 0.8191, 0.8127,\n",
            "        0.7821, 0.8277, 0.8047, 0.8271, 0.7902, 0.7538, 0.8425, 0.8265, 0.8118,\n",
            "        0.8276, 0.8259, 0.8024, 0.8008, 0.8198, 0.7994, 0.8287, 0.8398, 0.8175,\n",
            "        0.1814, 0.8325, 0.8296, 0.8209, 0.8396, 0.8337, 0.8168, 0.8191, 0.8003,\n",
            "        0.8168, 0.8285, 0.8536, 0.8231, 0.8784, 0.8163, 0.8100, 0.8284, 0.8261,\n",
            "        0.8522, 0.8184, 0.8196, 0.8376, 0.8261, 0.8238, 0.8144, 0.8121, 0.8293,\n",
            "        0.8341, 0.8381, 0.8203, 0.8221, 0.8557, 0.7928, 0.8216, 0.8405, 0.7998,\n",
            "        0.6775, 0.8221, 0.8230, 0.8151, 0.8555, 0.8396, 0.8318, 0.8067, 0.8071,\n",
            "        0.8237, 0.8258, 0.8341], device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.10.output.LayerNorm.bias', Parameter containing:\n",
            "tensor([ 9.9911e-03, -2.1731e-02, -8.5418e-02, -1.1091e-02, -7.9022e-02,\n",
            "        -8.4919e-02, -5.0959e-02, -1.0246e-01, -4.5238e-03, -2.2691e-02,\n",
            "        -2.5858e-02, -5.9837e-02,  1.9699e-02, -1.0160e-01, -2.3683e-02,\n",
            "        -1.3263e-01, -9.5915e-02, -6.0343e-02, -2.2185e-02, -6.3525e-02,\n",
            "        -1.0601e-01, -3.7986e-02, -1.3533e-02, -9.5777e-02, -5.9688e-02,\n",
            "        -7.9165e-02, -6.0853e-02,  1.6724e-03, -1.4806e-02, -3.9187e-02,\n",
            "        -1.2437e-01, -6.9457e-02, -4.0086e-02,  3.3282e-02, -4.6313e-02,\n",
            "        -3.2780e-02, -2.4511e-02, -2.9815e-02, -6.1609e-02, -2.6567e-02,\n",
            "         6.5424e-03,  1.6071e-02, -3.8619e-02, -7.5152e-02, -4.1365e-02,\n",
            "        -2.9133e-02, -4.4066e-02, -1.9910e-02,  6.1147e-03, -2.0784e-02,\n",
            "         5.6153e-02, -6.3402e-02, -8.1718e-02, -2.1161e-02, -1.1285e-01,\n",
            "        -1.0720e-01,  5.2568e-05, -5.5708e-04,  1.9047e-02,  4.8022e-03,\n",
            "        -3.7431e-02, -6.4962e-02, -3.5965e-02, -1.0284e-02, -5.8101e-02,\n",
            "        -1.1849e-01, -5.2727e-02, -7.4062e-02,  5.4283e-02, -5.3025e-02,\n",
            "         3.9204e-03, -9.7835e-02, -3.4598e-02, -1.5429e-02, -5.7405e-02,\n",
            "         2.0642e-02, -2.8590e-02, -6.8071e-02, -7.5334e-02, -4.8628e-02,\n",
            "        -4.0453e-02, -7.4228e-02, -5.4106e-02, -1.0514e-01, -4.6446e-02,\n",
            "        -5.5732e-02, -4.6757e-02, -3.6951e-02, -1.5078e-02, -1.4061e-01,\n",
            "         8.7417e-03,  1.0586e-02, -1.0612e-01, -8.5275e-03, -7.1212e-02,\n",
            "         3.3767e-02, -8.9851e-02, -1.5196e-02, -4.4344e-02, -3.0090e-02,\n",
            "        -2.5190e-02,  1.9014e-03, -8.4510e-02, -3.1113e-02, -2.4284e-02,\n",
            "        -5.5333e-02, -4.8818e-02, -3.2881e-02, -1.4534e-02, -1.6910e-02,\n",
            "        -2.6374e-02, -2.7997e-02, -1.2696e-02,  1.9971e-04, -5.2947e-02,\n",
            "        -5.5305e-02, -5.5210e-02, -1.5646e-02, -1.9907e-02, -7.2995e-02,\n",
            "        -5.0182e-02, -2.8904e-02, -7.6838e-02, -4.1030e-01, -4.4682e-02,\n",
            "        -1.2926e-02, -1.3576e-02, -4.8880e-02, -2.2388e-02, -2.9814e-02,\n",
            "        -5.6254e-02, -7.4661e-02, -4.0026e-02, -1.7584e-03, -1.0772e-03,\n",
            "        -3.2441e-02, -6.4202e-02,  7.3877e-03, -2.3857e-02, -2.8361e-02,\n",
            "        -3.4471e-03, -7.6416e-03, -3.3039e-02, -1.2528e-01, -6.2538e-02,\n",
            "         4.4751e-03, -2.3702e-02,  6.6690e-02, -5.3297e-03, -6.4901e-02,\n",
            "        -3.4022e-02, -7.0986e-02, -4.2420e-04,  6.2338e-03,  7.4419e-04,\n",
            "        -7.4062e-02, -3.6650e-02, -4.8876e-02, -7.2099e-02, -2.3079e-02,\n",
            "        -2.7489e-02, -2.9204e-02, -1.1956e-02, -5.7632e-02, -1.3603e-01,\n",
            "        -3.5237e-02, -8.6895e-02, -8.6527e-02,  1.6948e-02, -4.3703e-02,\n",
            "        -3.6273e-02,  2.6496e-02, -1.1421e-01, -3.9111e-02, -3.8379e-02,\n",
            "        -7.0450e-02, -7.3725e-02, -2.8091e-02, -8.0674e-02, -9.0598e-02,\n",
            "        -1.0157e-01, -1.2004e-01, -3.0780e-02, -3.8558e-02, -5.5876e-02,\n",
            "        -3.9297e-02, -1.0207e-01, -1.4810e-02, -1.0007e-01, -4.7311e-02,\n",
            "        -3.4091e-02,  1.4821e-03, -1.1007e-01, -2.9160e-02, -6.8113e-02,\n",
            "        -4.6947e-02, -1.2606e-02, -3.3736e-02, -6.4685e-02, -3.9513e-02,\n",
            "        -2.3827e-02, -6.3594e-02, -6.4615e-02, -2.7288e-02, -1.3813e-02,\n",
            "        -1.8821e-01, -2.1046e-02, -3.8563e-02, -5.3869e-02, -5.7992e-02,\n",
            "        -4.7478e-02, -5.8891e-02, -4.7414e-02, -1.0252e-01, -3.8033e-02,\n",
            "        -3.2360e-03,  5.8322e-03, -3.7740e-02, -1.1409e-01, -7.0290e-04,\n",
            "        -6.6285e-02, -1.0832e-01, -4.5646e-02, -8.4208e-02, -5.4448e-02,\n",
            "        -2.5062e-01, -5.1225e-02,  6.6958e-02, -7.1123e-02, -7.2297e-03,\n",
            "        -1.1737e-03, -1.3975e-02, -1.1764e-01, -2.8519e-02, -5.2637e-02,\n",
            "        -3.5194e-02, -2.9844e-02, -2.6238e-02, -1.0883e-01,  4.7634e-02,\n",
            "        -6.7663e-02, -7.7689e-02, -1.7273e-02, -1.1139e-01, -1.8778e-02,\n",
            "        -2.5276e-02, -6.3853e-02, -5.4246e-02,  1.9715e-03,  1.4811e-02,\n",
            "         5.9031e-03, -1.0934e-01,  7.5820e-03, -9.1352e-03, -6.1152e-02,\n",
            "        -6.5646e-03, -6.6989e-03,  5.0139e-02, -4.6572e-02, -5.2098e-02,\n",
            "        -8.9267e-02, -1.0850e-02, -1.7117e-02, -3.3324e-02, -1.9247e-02,\n",
            "         1.6208e-02, -3.4365e-02, -5.7199e-02, -7.0635e-02, -6.4506e-02,\n",
            "        -6.8236e-02, -4.0036e-02,  1.7157e-03, -5.0003e-02, -2.4505e-02,\n",
            "        -2.5174e-02, -5.4199e-02, -5.8951e-02, -2.6914e-02, -6.2709e-03,\n",
            "        -8.8090e-02, -1.2465e-01, -1.7531e-02, -2.7450e-02,  3.2209e-02,\n",
            "        -5.0641e-02, -1.0313e-01, -4.7239e-02, -2.3382e-02, -2.8086e-02,\n",
            "        -6.2909e-02, -6.3351e-02, -3.1319e-02, -4.8341e-02, -9.7531e-02,\n",
            "        -6.7401e-02,  1.3437e-02, -5.8164e-02, -6.4501e-02, -5.8095e-02,\n",
            "         1.3405e-02, -3.6890e-02, -3.6888e-02, -7.8711e-02, -1.1046e-01,\n",
            "        -2.4371e-02, -4.6823e-02,  8.0789e-03, -6.6701e-01, -5.2926e-02,\n",
            "        -1.1325e-02, -1.4399e-03, -1.0808e-01, -4.1863e-02, -3.6180e-02,\n",
            "        -5.2937e-02, -5.2122e-02, -7.9372e-02, -3.9934e-02,  1.7669e-02,\n",
            "        -3.5720e-02, -9.6919e-02, -7.8563e-02,  4.3456e-02, -1.5752e-02,\n",
            "        -7.3775e-03, -5.2622e-02, -1.4332e-01, -2.4892e-02,  8.7368e-03,\n",
            "        -5.3342e-02, -5.0091e-02, -5.8282e-02, -1.5009e-01, -2.4953e-02,\n",
            "        -8.6614e-03,  1.5207e-02, -4.2207e-02, -4.6732e-02, -1.2320e-02,\n",
            "        -4.9060e-02, -1.0421e-01, -3.4343e-02, -5.9889e-02, -4.5474e-02,\n",
            "         2.2524e-02, -3.9818e-02,  9.7119e-03, -4.2121e-02,  6.7828e-02,\n",
            "        -3.1296e-02, -2.6843e-02, -1.5530e-01, -4.6035e-02, -1.5778e-02,\n",
            "        -1.2878e-02, -3.0815e-02, -5.4324e-02, -3.9495e-02, -6.7273e-02,\n",
            "        -3.4010e-02, -2.4445e-02, -9.7994e-02, -4.5251e-02, -7.7136e-02,\n",
            "        -7.4479e-02, -4.6176e-04, -3.6473e-02, -1.2272e-02, -1.1400e-02,\n",
            "         6.4480e-02, -4.6266e-02, -3.7088e-02, -7.8402e-03,  3.3160e-02,\n",
            "        -4.2512e-02, -9.6050e-02,  1.2693e-02, -8.7282e-03, -7.9768e-02,\n",
            "        -2.6688e-02,  1.2342e+00, -1.8039e-02, -1.9496e-02, -1.2410e-01,\n",
            "        -4.1538e-02, -5.3155e-03, -5.9706e-02, -1.2424e-02, -4.7047e-03,\n",
            "        -6.9884e-02, -6.3183e-02, -6.7926e-02, -1.1283e-02, -5.4642e-02,\n",
            "        -2.9658e-02,  8.7688e-03,  1.7863e-02, -7.7521e-02, -2.6839e-02,\n",
            "        -5.4859e-02, -3.8077e-02, -9.4398e-02, -3.1434e-02, -3.1474e-02,\n",
            "        -1.2879e-02, -4.8862e-02,  4.8742e-02, -8.7172e-02, -3.7206e-02,\n",
            "        -7.0836e-02, -5.0987e-02, -2.7047e-02,  1.2548e-02, -2.7316e-02,\n",
            "        -6.1959e-02, -6.3211e-02, -3.3921e-02, -6.0652e-02, -4.4191e-02,\n",
            "        -4.6352e-02, -7.9696e-03, -8.0579e-02, -3.1754e-02, -5.6834e-02,\n",
            "        -9.4625e-02, -3.3766e-02,  1.0041e-02,  4.8681e-03, -5.6882e-02,\n",
            "         1.8110e-02,  1.4456e-02, -1.7093e-02, -6.4950e-02, -1.4863e-02,\n",
            "        -4.3046e-02, -3.0155e-02, -3.0272e-02, -4.6320e-03, -1.2794e-02,\n",
            "        -5.9728e-02, -3.8144e-02, -6.7831e-02,  1.4091e-02, -1.5094e-02,\n",
            "         6.2337e-02, -3.8341e-02, -7.2135e-02, -7.3657e-02, -4.3075e-02,\n",
            "        -8.9091e-02, -4.4390e-02, -9.8070e-02, -4.3190e-02, -1.0500e-01,\n",
            "        -8.2559e-02, -6.5659e-03, -4.7039e-02,  2.8142e-02,  6.6390e-02,\n",
            "        -2.8393e-02,  7.1410e-03, -4.8453e-02, -1.2122e-01, -1.2166e-01,\n",
            "        -1.1018e-02, -8.1017e-02,  1.8763e-02, -7.0300e-02, -1.6904e-02,\n",
            "        -1.8623e-02, -1.9928e-02, -6.5258e-02, -5.6976e-02, -7.7981e-03,\n",
            "         9.8662e-03, -5.8731e-02, -7.4344e-02,  2.4232e-03, -1.5830e-02,\n",
            "        -2.2368e-02, -4.0641e-02, -1.2160e-02, -8.6705e-02, -1.5942e-02,\n",
            "        -2.8012e-02, -4.9698e-02, -1.0570e-01, -5.4495e-03, -1.4825e-02,\n",
            "        -1.9538e-02, -4.2587e-02, -5.9300e-02, -3.5139e-02, -4.9682e-02,\n",
            "         1.2428e-02, -4.6710e-03, -7.1063e-02, -7.7453e-02, -1.1406e-01,\n",
            "        -6.3688e-02, -7.0842e-02, -1.7464e-02,  1.5732e-02, -6.9575e-02,\n",
            "        -5.0245e-02,  1.6815e-03, -6.3286e-02, -3.5803e-02, -8.6692e-03,\n",
            "        -1.8495e-02, -6.7725e-02, -2.2573e-02,  1.5954e-02, -4.2736e-02,\n",
            "        -1.1319e-01, -1.6139e-02, -3.8273e-02,  1.8396e-02, -3.6924e-02,\n",
            "        -1.4614e-02, -7.8820e-02, -3.9939e-02,  1.7853e-02, -3.5268e-02,\n",
            "        -5.1438e-03,  2.7042e-03, -1.2847e-02, -8.0948e-02, -2.0056e-02,\n",
            "        -9.5752e-02, -8.4829e-03, -9.1452e-03, -6.1099e-02, -8.7580e-02,\n",
            "         1.0184e-02, -6.3736e-02, -6.0583e-03, -6.6168e-02,  9.5015e-02,\n",
            "        -9.3529e-02, -4.2957e-02, -1.0764e-02, -2.6883e-02,  2.9078e-03,\n",
            "         1.6555e-02, -1.4094e-02, -4.3040e-02, -8.9755e-02, -6.9320e-02,\n",
            "        -1.8519e-02, -1.0838e-03, -6.7991e-02, -5.3360e-02, -6.4697e-02,\n",
            "        -7.3795e-03, -2.9285e-02, -8.2708e-02, -4.6001e-02,  6.5025e-03,\n",
            "        -6.0623e-03, -9.6565e-03, -9.4287e-02, -7.0777e-03,  2.2010e-02,\n",
            "        -8.5708e-02, -4.1289e-02, -4.7729e-02, -3.6953e-02, -4.0076e-02,\n",
            "        -1.6337e-02, -8.1847e-02, -1.1734e-01, -2.8326e-02, -1.1577e-02,\n",
            "        -5.4508e-02, -3.5414e-02,  1.9469e-02, -8.9905e-02,  2.4397e-02,\n",
            "        -8.5392e-03, -6.6797e-02, -6.4009e-02, -6.3319e-02, -1.7018e-02,\n",
            "        -5.7650e-03, -1.0963e-01,  2.4166e-03, -5.2971e-02, -3.1823e-02,\n",
            "        -5.1509e-02, -5.2670e-02, -5.1378e-02,  1.2320e-02, -5.8371e-02,\n",
            "        -5.2755e-02, -4.5402e-02, -1.7602e-02, -7.9624e-02, -4.9163e-02,\n",
            "        -4.5584e-02, -2.9496e-02,  1.5783e-02, -5.8591e-02, -1.2061e-01,\n",
            "        -6.3204e-02, -6.0637e-02, -4.7426e-02,  5.2212e-03, -1.0803e-02,\n",
            "        -8.8708e-02, -8.4110e-03, -4.1089e-02, -6.1244e-02, -6.9400e-02,\n",
            "         5.9572e-02, -6.2619e-04, -3.9389e-02, -2.0552e-02,  3.3747e-02,\n",
            "        -1.2618e-01, -4.6918e-02, -4.4600e-02, -5.4035e-02, -5.6527e-02,\n",
            "        -5.9106e-02, -8.9185e-02,  1.6636e-03, -7.0490e-02, -7.8546e-02,\n",
            "        -9.4421e-02, -6.7967e-02, -1.1163e-01, -1.5173e-01, -2.0537e-02,\n",
            "        -3.0213e-02, -8.1726e-02, -1.1522e-01, -5.6791e-02, -4.1666e-02,\n",
            "         2.8827e-02, -6.0505e-02,  3.4731e-02,  3.6714e-02, -6.2664e-02,\n",
            "         3.2626e-02, -7.2699e-02, -1.1528e-01, -6.3234e-02, -1.0146e-01,\n",
            "        -1.1614e-01, -4.8142e-02, -1.0865e-01, -6.9105e-02, -3.3361e-02,\n",
            "        -2.5578e-02, -1.3720e-01, -1.0591e-01, -5.5810e-02, -4.9357e-02,\n",
            "        -7.5403e-02, -2.2140e-02, -1.1475e-01, -2.2073e-02, -9.0478e-02,\n",
            "        -3.7927e-02, -5.4683e-02, -1.2830e-01, -9.2655e-02, -6.7291e-02,\n",
            "        -1.1527e-01,  5.0327e-03, -8.8101e-03, -1.7784e-02, -1.1600e-01,\n",
            "         2.9478e-03, -1.0082e-01, -6.8860e-02, -7.0515e-02, -2.1972e-02,\n",
            "        -7.5812e-02, -1.2126e-02, -7.8754e-02, -3.5369e-02, -5.8462e-02,\n",
            "        -2.3800e-02, -5.0041e-02, -1.0561e-01, -2.2384e-02, -5.1946e-02,\n",
            "         1.3943e-02,  1.0339e-03,  5.1043e-03, -3.4733e-02, -4.6358e-02,\n",
            "        -6.1365e-02, -1.4683e-02, -3.9663e-03, -2.7667e-02, -3.0266e-02,\n",
            "        -9.8227e-02, -2.9119e-02,  6.8651e-03, -3.5015e-02, -9.4804e-02,\n",
            "        -8.8822e-02, -1.5799e-01, -2.5026e-02, -2.3272e-02, -4.6642e-02,\n",
            "         1.8524e-02, -8.2921e-02, -7.5322e-02, -9.0289e-02, -2.9547e-02,\n",
            "        -9.8346e-02,  2.5116e-02, -6.2788e-02, -8.0618e-02, -5.6549e-02,\n",
            "         3.7705e-01, -8.2464e-02, -5.7788e-02, -5.6724e-02, -6.7727e-02,\n",
            "        -5.0570e-03,  2.6409e-02, -9.3525e-03, -6.7059e-03,  1.7871e-02,\n",
            "        -4.7371e-02, -4.6696e-02, -1.0365e-02, -7.4026e-02, -8.8747e-02,\n",
            "        -3.5788e-02, -2.9538e-02, -5.2848e-02, -3.8083e-02, -7.7811e-02,\n",
            "        -5.1079e-02, -4.2770e-02,  2.6733e-02, -5.9677e-03, -7.4430e-03,\n",
            "        -9.0626e-02, -2.0063e-02, -2.0051e-02, -4.1825e-02, -7.8440e-02,\n",
            "        -3.3627e-02, -4.4238e-02, -2.2995e-02, -6.5732e-02,  4.6892e-03,\n",
            "         1.6609e-02,  1.9837e-02, -5.4074e-02, -1.7401e-02, -2.0425e-02,\n",
            "        -5.9345e-02, -1.1818e-02, -5.0286e-02,  4.0857e-02, -3.0308e-03,\n",
            "        -3.4822e-02, -5.7651e-02, -4.0377e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.11.attention.self.query.weight', Parameter containing:\n",
            "tensor([[-0.0414, -0.0009, -0.0389,  ..., -0.0488,  0.0483, -0.0530],\n",
            "        [-0.0649, -0.0347, -0.0141,  ...,  0.0332, -0.0758,  0.0059],\n",
            "        [-0.0320,  0.0619, -0.0941,  ...,  0.0221, -0.0589, -0.0778],\n",
            "        ...,\n",
            "        [ 0.0453,  0.0145, -0.0392,  ..., -0.0525, -0.0572,  0.0079],\n",
            "        [ 0.0710, -0.0197,  0.0472,  ...,  0.0008, -0.0789, -0.0107],\n",
            "        [-0.0549,  0.0375,  0.0308,  ...,  0.0273,  0.0536, -0.0444]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.11.attention.self.query.bias', Parameter containing:\n",
            "tensor([ 5.0130e-02,  1.1887e-01,  2.1868e-01,  8.7210e-02, -2.0477e-01,\n",
            "         2.2648e-01, -3.4360e-01,  6.4057e-02, -4.1554e-01,  1.1557e-01,\n",
            "         2.8522e-01, -1.0724e-01,  2.1037e-01,  6.5960e-02, -8.9728e-02,\n",
            "        -6.1263e-02,  1.1344e-01, -1.9265e-01, -1.2109e-02, -2.4905e-01,\n",
            "         7.1558e-02, -2.0746e-01, -1.8167e-01, -1.9616e-02,  5.7593e-02,\n",
            "        -4.0425e-02,  3.1487e-01, -1.6641e-01,  3.6615e-01,  2.8458e-02,\n",
            "        -8.9834e-02, -4.3720e-02, -1.2218e-01,  1.1063e-01,  4.6565e-02,\n",
            "         5.6489e-02,  2.5875e-01,  1.0859e-01, -2.2724e-01, -1.5846e-01,\n",
            "        -2.0762e-01, -2.8562e-01, -1.6824e-01,  1.7382e-01,  2.8304e-02,\n",
            "        -8.5946e-02, -1.2882e-01, -3.9624e-05,  1.3995e-01, -1.4809e-01,\n",
            "        -1.4983e-01,  8.0181e-02, -2.4638e-01,  1.3984e-01, -4.8627e-01,\n",
            "        -3.8021e-02,  7.8774e-02, -5.4215e-01,  1.8015e-01, -2.4764e-01,\n",
            "        -8.2056e-02,  2.0403e-01, -1.2871e-01,  1.7048e-01,  3.3818e-01,\n",
            "        -5.3207e-01,  2.6975e-01,  1.5490e-01,  2.6109e-01, -6.6078e-02,\n",
            "        -1.0768e-01,  6.7982e-02,  5.1441e-03, -1.7760e-01, -8.6212e-03,\n",
            "        -1.2084e-01,  2.7072e-02,  9.5277e-02, -1.1946e-01, -1.7435e-01,\n",
            "         1.2826e-02, -1.4399e-01,  5.4034e-02, -7.1218e-02, -1.0277e-01,\n",
            "        -8.0027e-02,  1.2395e-01,  1.2472e-01,  2.8782e-01, -2.4318e-01,\n",
            "         6.9833e-02, -1.5565e-01,  5.3963e-02,  2.2090e-01, -6.5430e-02,\n",
            "        -2.9557e-01,  1.9449e-01, -2.0094e-02, -3.5260e-02, -1.2373e-01,\n",
            "         1.1984e-01,  1.1227e-01,  4.9598e-02,  2.9100e-01, -1.6697e-01,\n",
            "         9.0494e-02, -5.6501e-02,  1.8282e-01,  1.1605e-01, -1.1072e-01,\n",
            "        -1.3768e-01, -1.3056e-01,  5.1009e-03,  3.5995e-02, -2.2659e-01,\n",
            "        -1.7014e-01, -3.6584e-02,  1.4101e-01,  1.6270e-01, -1.3676e-01,\n",
            "         4.6082e-01, -1.5044e-02,  2.6074e-01,  2.4027e-01, -3.2432e-01,\n",
            "        -1.0082e-01, -6.3582e-03, -1.2997e-01,  1.7895e-01, -5.6416e-03,\n",
            "        -3.2919e-02,  9.1986e-03,  3.9944e-02, -4.8234e-03, -1.0553e-02,\n",
            "         4.5920e-02, -5.7454e-02, -5.0634e-03,  3.1936e-02,  2.5940e-02,\n",
            "         2.4509e-02,  5.9364e-03, -7.3236e-02,  4.9799e-02, -6.5353e-02,\n",
            "        -3.0103e-02, -5.1781e-02,  1.8562e-02, -1.2511e-02, -2.4903e-02,\n",
            "        -1.9358e-02, -2.2507e-02, -5.2231e-02,  8.5270e-02,  5.6999e-03,\n",
            "         5.2514e-02, -7.0874e-02,  3.2521e-02,  5.5978e-02,  3.1149e-02,\n",
            "        -3.7748e-02, -1.6950e-02, -1.9682e-02, -7.0004e-02,  6.0100e-02,\n",
            "        -3.0003e-02, -1.9542e-02, -2.6614e-02, -5.4306e-02, -3.2268e-02,\n",
            "        -2.3941e-02,  4.8159e-02, -6.7734e-02,  9.6824e-03,  6.2304e-02,\n",
            "         1.5483e-02,  4.7038e-02, -4.1295e-02, -2.6080e-02,  7.5054e-02,\n",
            "        -9.9345e-03,  6.7070e-02, -5.5948e-03, -4.9686e-02,  6.9155e-02,\n",
            "        -1.2513e-02, -9.6080e-03,  1.4420e-02,  8.9716e-02, -3.2649e-02,\n",
            "         1.3103e-02, -5.8897e-02, -2.8231e-01, -2.1987e-01,  7.4569e-02,\n",
            "        -1.9329e-01, -2.3904e-01, -2.0547e-01,  1.4832e-01, -2.0592e-01,\n",
            "        -1.0556e-01, -3.3246e-01,  2.9869e-01,  1.4742e-02, -3.4112e-02,\n",
            "        -5.1247e-02,  1.7858e-01, -1.5464e-01,  4.4350e-01, -3.4233e-01,\n",
            "         6.0810e-02, -4.8418e-01, -1.7234e-01,  5.5829e-01, -1.9054e-01,\n",
            "        -1.7085e-01, -6.5740e-02,  4.6172e-01,  1.1685e-01, -1.0867e-01,\n",
            "         2.5521e-02,  9.3015e-02, -3.3565e-01,  9.7723e-02,  1.1447e-01,\n",
            "         2.5188e-01,  2.4162e-01, -3.2859e-02,  1.1418e-01, -1.3454e-01,\n",
            "         1.1214e-01,  1.9344e-01,  2.8808e-01,  3.6662e-02, -9.1874e-02,\n",
            "        -1.8925e-02,  8.3655e-02, -2.9657e-01,  4.3850e-01, -3.1043e-01,\n",
            "         2.6670e-01, -2.4112e-01,  1.0252e-01, -1.7780e-04, -7.8823e-02,\n",
            "        -2.7052e-01, -8.6674e-03,  1.0519e-02,  2.3936e-01, -2.9538e-01,\n",
            "         8.4221e-02,  3.3672e-02,  1.9706e-01, -6.2164e-02, -2.4718e-02,\n",
            "         8.7717e-02, -2.5071e-01,  1.5706e-01,  2.3768e-01,  3.9946e-01,\n",
            "         1.1246e-01, -1.7711e-01, -3.4676e-01, -4.5355e-01,  7.5780e-02,\n",
            "        -7.4163e-02, -5.9656e-03,  1.8497e-01,  4.2538e-01,  2.2415e-02,\n",
            "        -1.9526e-01, -8.4955e-02, -2.0987e-01, -3.9819e-01, -1.6228e-01,\n",
            "        -1.0210e-01,  1.6659e-01,  1.4552e-01, -2.1444e-01, -1.1108e-01,\n",
            "         7.1328e-02, -1.7965e-01,  5.0596e-03, -1.4955e-01, -2.5073e-01,\n",
            "        -2.3035e-01,  1.9926e-01, -1.9052e-01, -5.7047e-02, -2.4461e-02,\n",
            "        -4.0321e-01,  1.3474e-01, -4.3975e-03, -2.7324e-01, -1.3796e-01,\n",
            "         6.3967e-02,  2.7403e-01,  1.9344e-01, -1.2892e-01, -1.6142e-02,\n",
            "         3.8657e-01,  1.5079e-01,  9.9633e-02,  6.6772e-01,  1.5956e-01,\n",
            "         1.2797e-01,  1.6047e-01,  1.4334e-01, -7.4570e-02, -3.9579e-01,\n",
            "         1.7510e-02, -3.4705e-02,  1.4462e-01,  1.2939e-01, -1.1103e-01,\n",
            "         4.3722e-02, -1.4519e-01,  6.8075e-02,  3.0203e-02, -4.9669e-02,\n",
            "         3.4470e-01,  1.4825e-02, -1.2054e-01, -3.6352e-01,  1.1949e-02,\n",
            "        -1.1544e-01,  5.5563e-01,  6.3900e-01, -9.1271e-02,  5.7076e-02,\n",
            "         1.8457e-02, -1.9434e-01,  5.8411e-01,  1.5039e-01, -2.1969e-01,\n",
            "         8.3831e-02,  3.5248e-01, -2.1035e-01,  1.2112e-01, -1.5372e-01,\n",
            "         7.9632e-02, -2.5212e-01, -4.9995e-02,  4.1693e-01,  1.3844e-01,\n",
            "         8.8435e-02,  7.5546e-01,  2.8799e-01,  7.1233e-03,  1.6018e-01,\n",
            "        -4.3708e-01,  4.0694e-02, -2.8835e-01, -1.8012e-01,  7.6160e-01,\n",
            "         6.1730e-02,  2.0175e-01,  2.5984e-01, -1.8264e-01, -1.3711e-01,\n",
            "         1.8038e-01,  9.9823e-02,  2.9867e-03,  3.7477e-01, -7.3096e-02,\n",
            "        -5.5421e-02,  7.9605e-01, -3.7314e-01,  4.5567e-01,  3.8785e-01,\n",
            "        -4.6460e-01, -3.1176e-01, -9.7430e-02,  1.1275e-01,  1.7523e-01,\n",
            "        -1.7209e-01,  2.6395e-01,  5.0246e-01, -2.1430e-01, -1.3977e-01,\n",
            "         1.2911e-01,  2.6221e-01,  1.2876e-01,  1.8243e-02,  1.9331e-02,\n",
            "         2.0036e-01, -3.7061e-03, -5.2489e-02, -3.8090e-02,  1.7509e-01,\n",
            "         1.4210e-01,  2.6394e-01, -2.4669e-01, -2.0842e-01,  7.3437e-02,\n",
            "         8.0304e-02, -8.8610e-02,  9.5805e-02, -2.8193e-03,  9.6245e-02,\n",
            "        -5.9579e-02,  1.8303e-02, -5.2196e-02,  2.4602e-03, -8.9981e-03,\n",
            "        -1.4188e-01,  1.6982e-01,  1.4684e-02,  8.0432e-02,  1.4415e-01,\n",
            "        -7.9289e-02, -4.3563e-02, -1.3048e-01,  9.0140e-02, -1.1332e-01,\n",
            "         6.5817e-02, -2.8919e-02, -5.0692e-02, -6.8233e-02,  1.2676e-01,\n",
            "        -7.7853e-03, -1.1944e-01, -7.1633e-02,  3.8406e-02,  5.1482e-02,\n",
            "        -2.3815e-01, -7.2538e-02,  5.0957e-01, -7.4238e-02, -1.2476e-01,\n",
            "        -7.3363e-02,  2.0673e-01, -3.3949e-02, -1.0945e-04, -1.1571e-02,\n",
            "        -2.1345e-01,  2.1062e-01,  4.1730e-02, -5.9223e-03,  3.1824e-01,\n",
            "        -1.7664e-01, -2.7752e-02, -1.7509e-02, -2.5941e-01, -5.6525e-02,\n",
            "         2.2949e-01,  5.6018e-02,  5.1439e-02,  4.4295e-01, -6.6992e-02,\n",
            "         1.8303e-02,  1.6685e-01,  1.9616e-01,  8.3671e-02,  5.5415e-01,\n",
            "        -1.4475e-01, -3.9549e-01, -1.6309e-02,  3.3830e-01,  1.1835e-01,\n",
            "        -3.6023e-01, -4.5967e-01, -1.0929e-01,  1.9510e-01, -8.4783e-02,\n",
            "         1.7358e-01, -1.9851e-01,  3.5490e-02,  9.7441e-02, -3.1306e-01,\n",
            "         2.6871e-01,  1.3092e-01, -7.5986e-02, -5.5100e-01, -1.9846e-01,\n",
            "        -1.8693e-01,  2.4242e-01, -3.1841e-01,  3.2628e-02, -2.6412e-01,\n",
            "         6.1704e-02,  1.4636e-01,  2.6656e-01,  2.6279e-01,  1.6979e-01,\n",
            "         3.0427e-01,  3.4861e-02,  1.0094e-01,  3.0639e-01, -1.0081e-01,\n",
            "         3.4950e-01,  7.8413e-01,  5.2060e-01,  2.1603e-03, -7.0045e-03,\n",
            "        -4.1164e-01, -4.8645e-01, -4.4651e-01,  3.3513e-01, -1.1313e-01,\n",
            "        -3.9717e-01,  3.8403e-01, -2.3518e-01, -6.8511e-02, -1.0130e-01,\n",
            "        -1.5456e-01, -3.1723e-01,  1.4869e-01, -4.2325e-02, -5.3928e-01,\n",
            "        -1.3634e-02,  4.4899e-01,  2.1111e-01,  8.5819e-02, -4.3049e-01,\n",
            "        -8.3070e-01, -2.2593e-01, -2.0729e-01,  2.3948e-01, -1.5970e-01,\n",
            "        -3.6727e-01,  8.4436e-02,  8.1711e-03, -5.3988e-02,  1.2152e-01,\n",
            "         5.3374e-01,  1.3877e-01, -1.0957e-01, -7.8825e-02, -2.0775e-01,\n",
            "        -1.7110e-01,  9.0019e-02, -5.4240e-02,  1.1426e-02,  5.0588e-01,\n",
            "         9.0979e-02,  2.6900e-01, -2.9941e-01, -4.1123e-01,  1.7487e-01,\n",
            "        -4.8576e-02,  2.7232e-01, -4.5879e-01,  1.1320e-01,  7.7917e-02,\n",
            "        -1.9511e-01,  9.0838e-02, -2.7221e-02,  3.0969e-01,  1.3033e-01,\n",
            "        -4.9067e-01,  1.0965e-01, -3.1539e-01, -2.0408e-02, -8.4141e-02,\n",
            "         4.9824e-01,  2.5152e-01,  1.5938e-01,  1.8462e-01,  3.6651e-01,\n",
            "        -9.1929e-02, -5.0355e-01, -5.0853e-01,  2.4071e-01, -1.8947e-01,\n",
            "        -1.9674e-01,  2.3726e-01,  2.3691e-01,  3.6136e-01, -1.4146e-01,\n",
            "        -2.5364e-01, -1.4089e-01,  5.2075e-01, -2.7502e-01,  5.7724e-02,\n",
            "         1.7701e-01, -2.4592e-02,  6.1815e-02, -1.7824e-02,  7.1862e-02,\n",
            "         1.9092e-01, -1.4360e-01,  6.1019e-02,  2.2350e-01, -7.7964e-02,\n",
            "        -4.3328e-02,  1.7803e-01,  7.6450e-02,  1.6623e-01, -6.7891e-02,\n",
            "        -2.3149e-01,  1.7047e-01, -2.0200e-02, -3.5068e-02,  8.5247e-03,\n",
            "        -9.5607e-02, -2.3400e-01, -1.5214e-01, -4.5329e-02,  2.6065e-01,\n",
            "         1.1526e-01,  1.8007e-01, -3.8886e-02, -1.6848e-01,  2.4758e-01,\n",
            "         5.4496e-02, -5.3286e-02, -8.9750e-02, -1.1491e-01, -1.1738e-01,\n",
            "        -1.7055e-01, -4.1841e-02,  1.2854e-01,  8.8451e-02,  1.2169e-01,\n",
            "        -1.9706e-01, -3.5176e-02,  4.0673e-02, -1.5466e-01,  3.3641e-01,\n",
            "         1.1405e-01,  2.8445e-02, -1.4156e-01, -1.5457e-01,  9.3087e-02,\n",
            "         3.6545e-02,  4.2470e-02, -8.3606e-02,  1.0506e-01,  8.1112e-02,\n",
            "        -1.2319e-01, -7.9738e-02,  3.0404e-02,  2.2610e-01, -2.3057e-01,\n",
            "         1.2007e-01,  6.6130e-02, -1.0911e-01,  2.1609e-01,  7.3638e-02,\n",
            "         1.5479e-01,  3.4500e-01, -2.7938e-01,  5.7017e-01, -1.1569e-01,\n",
            "         2.4812e-01, -6.9649e-02, -2.7978e-01, -1.2075e-01,  1.1278e-01,\n",
            "        -1.6568e-01, -4.5727e-02,  2.3313e-02,  3.2640e-01, -2.2461e-01,\n",
            "        -8.2861e-02, -5.8357e-01, -3.9771e-02,  2.9434e-01, -2.3609e-01,\n",
            "        -5.3624e-01,  1.0189e-01, -1.8835e-01,  2.4618e-01, -1.0786e-01,\n",
            "        -7.9863e-02, -7.3921e-02,  3.8703e-02, -2.5798e-01,  4.7645e-03,\n",
            "        -2.3076e-01,  9.6446e-02,  1.0595e-01,  2.1178e-01,  3.2419e-01,\n",
            "        -1.3776e-01, -1.5145e-01,  3.0578e-01,  4.3816e-02,  8.1696e-02,\n",
            "         3.4741e-03,  3.6626e-01,  1.7579e-01,  1.0635e-01,  1.5077e-01,\n",
            "        -3.0174e-01, -1.8396e-01, -1.2838e-01,  1.3289e-01, -1.0084e-02,\n",
            "         6.4295e-02, -3.6652e-03, -1.7156e-01,  2.1100e-01,  4.2541e-01,\n",
            "        -2.0022e-01,  9.4931e-02,  1.8836e-01, -2.3158e-01,  3.2177e-01,\n",
            "        -1.6557e-02, -1.4628e-02,  2.9085e-02, -1.9244e-01,  4.9188e-02,\n",
            "         1.0409e-01,  4.2626e-01,  2.3523e-01,  4.4313e-01, -3.6384e-02,\n",
            "         1.1373e-01, -9.6126e-03,  2.3600e-01, -1.0489e-01, -1.2586e-01,\n",
            "         3.0308e-01,  6.5636e-01,  1.8376e-01, -1.0827e-02, -1.0237e-01,\n",
            "        -2.8121e-01,  4.9903e-01, -9.4880e-02, -1.5334e-01, -5.1055e-02,\n",
            "         8.2555e-02, -4.1501e-01,  1.9477e-01,  5.8070e-02,  7.0947e-02,\n",
            "         1.0771e-02, -4.8819e-02,  3.6631e-01,  8.3896e-02,  4.0893e-01,\n",
            "         4.5631e-01,  3.2684e-01,  4.7213e-01,  2.4745e-01,  8.8205e-03,\n",
            "        -1.4258e-01, -8.1988e-01, -2.4345e-01, -7.6428e-02,  1.5556e-01,\n",
            "        -2.0622e-01, -5.8932e-01,  1.5509e-01,  5.9741e-02,  1.1991e-01,\n",
            "        -1.1299e-01, -1.5199e-01,  4.3523e-01, -1.5052e-01,  9.4984e-03,\n",
            "         4.5568e-01,  9.4621e-03,  1.6628e-01,  6.1444e-02,  5.3525e-02,\n",
            "        -8.2070e-02,  2.8694e-01,  9.8581e-02, -1.5380e-01, -1.9750e-01,\n",
            "         2.6130e-01,  1.8281e-01,  1.5343e-01], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.11.attention.self.key.weight', Parameter containing:\n",
            "tensor([[ 0.0263, -0.0122, -0.0140,  ...,  0.0246,  0.0719, -0.0255],\n",
            "        [-0.0136, -0.0583,  0.0358,  ..., -0.0417,  0.0430,  0.0056],\n",
            "        [-0.0608, -0.0095,  0.0293,  ..., -0.0390, -0.0027,  0.0691],\n",
            "        ...,\n",
            "        [ 0.0440, -0.0391, -0.0065,  ...,  0.0465, -0.0178, -0.0106],\n",
            "        [-0.0055, -0.0124,  0.0750,  ...,  0.0077, -0.0265,  0.0786],\n",
            "        [ 0.0482,  0.0488, -0.0022,  ...,  0.0537, -0.0033, -0.0484]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.11.attention.self.key.bias', Parameter containing:\n",
            "tensor([-7.7980e-04, -6.5806e-04, -1.0521e-04,  1.1914e-03, -1.8009e-03,\n",
            "        -2.7502e-03, -7.8233e-04,  2.5860e-03, -2.2570e-02, -2.6881e-03,\n",
            "         2.3669e-03,  3.2777e-03,  1.7142e-03,  1.7085e-03,  4.7215e-04,\n",
            "        -4.3301e-03,  1.2966e-03,  3.8041e-03,  5.5218e-05, -2.4422e-03,\n",
            "        -2.8807e-03, -3.4955e-03, -3.4653e-03,  2.6404e-03, -5.9517e-03,\n",
            "        -3.8325e-03,  5.8708e-03, -2.6597e-03,  4.1050e-03,  2.5653e-03,\n",
            "        -3.9080e-03,  4.9746e-03, -2.3052e-03,  1.9391e-03,  6.0000e-03,\n",
            "         2.3973e-03,  7.9802e-05,  4.9902e-04, -2.0522e-03, -2.3937e-03,\n",
            "        -1.8747e-03,  3.5503e-03, -1.5680e-04,  5.1923e-04, -3.8865e-04,\n",
            "        -5.5323e-03,  2.4720e-03, -1.7647e-03,  1.8318e-03,  4.0571e-03,\n",
            "        -1.6462e-03,  1.8299e-04, -2.5929e-03, -4.7880e-03, -7.7311e-04,\n",
            "         4.5449e-03,  2.3443e-03, -3.8923e-03,  3.0964e-03, -6.2480e-03,\n",
            "         2.9857e-03,  4.5695e-03,  5.4555e-04, -7.6326e-03, -6.1901e-03,\n",
            "         2.1954e-03, -2.0032e-03, -5.5015e-03,  4.7041e-03,  8.0262e-03,\n",
            "        -6.5731e-04,  5.1535e-03,  6.6659e-03, -1.6829e-03, -1.2338e-03,\n",
            "        -4.1165e-03,  5.8288e-03,  6.5286e-03, -2.8862e-03, -5.5435e-03,\n",
            "        -4.1922e-03,  3.4933e-03, -2.5085e-03,  2.6705e-03, -1.9673e-03,\n",
            "         1.4505e-03, -5.2694e-04,  3.4777e-03, -2.1290e-03, -1.0827e-02,\n",
            "         5.7160e-04, -1.2771e-02,  1.2197e-02,  9.2011e-04, -7.0105e-05,\n",
            "        -2.5185e-03, -2.5450e-03,  1.4186e-03,  3.5430e-03, -1.0059e-02,\n",
            "        -9.0462e-03, -2.1483e-03, -7.2961e-03,  1.6446e-03, -1.9546e-03,\n",
            "         1.6745e-03,  3.0747e-05,  6.8765e-04,  4.6925e-05, -4.6175e-03,\n",
            "         1.4915e-03, -1.8102e-03,  5.8387e-03,  2.6187e-03,  8.0551e-04,\n",
            "        -3.7605e-04,  4.7826e-03,  4.5015e-03,  1.8880e-03,  2.0086e-03,\n",
            "        -3.0556e-03,  1.1215e-03, -3.3826e-03, -1.6811e-03,  2.6084e-03,\n",
            "        -2.3130e-03,  9.7316e-03,  4.0729e-03,  3.3158e-02, -3.9357e-03,\n",
            "        -5.8574e-03, -8.3691e-03, -4.3808e-03,  9.6372e-04,  2.7483e-03,\n",
            "         9.2655e-04, -9.5048e-03,  5.4070e-03,  4.4320e-03, -5.7101e-03,\n",
            "         7.0762e-03,  3.9604e-03, -4.0571e-03,  5.1850e-03, -2.2990e-03,\n",
            "        -1.6759e-03,  3.5689e-03, -8.1071e-04, -8.3714e-03,  2.4655e-06,\n",
            "         8.8419e-03, -4.9797e-03,  2.9355e-03,  3.7562e-03,  2.3886e-03,\n",
            "        -1.3101e-02,  2.7496e-03, -3.6806e-03,  5.4301e-04,  3.3448e-04,\n",
            "        -3.5470e-03, -3.4434e-04,  2.0638e-02,  6.3679e-03,  4.3933e-03,\n",
            "        -7.7175e-03,  2.5263e-03,  6.5277e-03,  1.0303e-04,  9.2934e-03,\n",
            "         4.5492e-03, -3.1950e-03,  5.3094e-03, -8.3932e-03,  1.7044e-03,\n",
            "        -1.6819e-03,  1.6287e-03,  2.5236e-03, -8.7690e-04, -8.4405e-03,\n",
            "         5.1847e-03,  2.9581e-03, -8.6386e-03, -4.4322e-03, -2.2902e-03,\n",
            "         7.4777e-03, -5.2038e-03,  1.0842e-03,  6.3830e-03,  4.6294e-03,\n",
            "        -4.4676e-03, -1.4646e-02, -2.4057e-03, -1.0346e-03, -1.0573e-03,\n",
            "         1.2500e-03,  2.8151e-03,  6.4751e-03, -3.0001e-03,  1.8604e-03,\n",
            "         3.2033e-03,  3.3507e-03,  3.8772e-03, -1.0555e-03,  3.5869e-03,\n",
            "         2.9678e-03,  2.2149e-03, -6.3072e-05,  2.4339e-04, -1.5107e-03,\n",
            "         8.0292e-04,  5.0392e-03, -1.8044e-03, -3.5083e-03, -4.3115e-04,\n",
            "        -1.2534e-03,  4.0202e-03, -2.8886e-03,  1.6242e-03, -1.4304e-03,\n",
            "         2.4730e-03,  3.2120e-04,  1.5405e-03,  2.3828e-03, -1.3154e-02,\n",
            "        -9.1298e-03, -3.3335e-03, -1.6285e-03, -7.3392e-03, -4.1661e-03,\n",
            "        -7.2247e-04,  6.0622e-04, -1.3110e-04, -3.1256e-03, -1.6078e-03,\n",
            "         8.9988e-03,  5.2480e-03, -3.8470e-03,  2.5489e-03, -8.3292e-04,\n",
            "        -9.3382e-04, -2.3466e-03, -6.4202e-03, -5.4375e-03, -1.0460e-03,\n",
            "        -8.6690e-04,  9.9264e-04, -1.3279e-03, -3.3971e-03, -4.1026e-03,\n",
            "        -2.0531e-03, -4.4115e-03,  4.9086e-04, -2.7030e-03,  4.8619e-03,\n",
            "        -4.1838e-03, -2.6691e-04,  3.2127e-03,  2.1037e-03,  5.1669e-03,\n",
            "        -2.0005e-04, -3.6263e-03, -5.4068e-03, -2.5229e-03,  6.4348e-03,\n",
            "         3.3690e-04, -4.2479e-03,  8.0654e-04, -7.7276e-03,  9.2269e-04,\n",
            "        -2.4067e-03, -3.0091e-03,  3.1880e-03, -8.5397e-03, -2.6227e-03,\n",
            "         1.1948e-03, -1.8824e-03, -2.7381e-03,  1.9049e-03, -9.1579e-03,\n",
            "         6.0436e-03,  2.6585e-03,  2.3298e-03, -6.6079e-03,  1.2441e-02,\n",
            "         3.5928e-03,  1.5441e-03, -5.5536e-03, -3.7747e-03,  2.9258e-03,\n",
            "        -3.2777e-03, -3.6607e-03, -2.5393e-03,  3.6387e-03,  2.5710e-03,\n",
            "        -5.8111e-04,  1.0417e-02,  3.9542e-03, -5.3282e-03, -1.0404e-03,\n",
            "         7.7495e-03, -4.3610e-03, -1.1252e-03,  5.4011e-04,  3.0494e-03,\n",
            "         5.4364e-03, -3.0751e-03,  3.2636e-03, -1.8717e-03,  2.1970e-03,\n",
            "        -5.7621e-04,  2.5957e-03,  1.4918e-03, -2.4306e-03, -1.9329e-03,\n",
            "        -3.2307e-03, -1.1912e-02,  8.8809e-03,  2.5453e-03, -1.9595e-03,\n",
            "        -6.3161e-03, -1.3533e-02, -1.4055e-02, -5.8536e-03, -5.1156e-03,\n",
            "        -9.1114e-05,  6.3963e-03,  1.4257e-02, -6.1021e-04,  2.2210e-03,\n",
            "         5.9605e-03,  2.2709e-04,  6.7200e-03, -5.4176e-03, -2.1599e-03,\n",
            "        -3.0826e-03, -3.7544e-03,  1.6585e-03, -2.3840e-03, -1.1991e-03,\n",
            "        -1.1667e-03, -5.3543e-03, -4.0198e-04,  5.4359e-03, -3.5473e-03,\n",
            "        -4.3826e-03,  6.3052e-03,  5.3327e-03, -1.7406e-03, -1.3597e-02,\n",
            "         9.1391e-04,  3.7985e-03, -3.9346e-03, -2.1155e-03,  8.7951e-03,\n",
            "        -1.1909e-02,  5.5996e-03, -3.5501e-03,  1.5334e-02,  6.5179e-03,\n",
            "        -3.9729e-03, -7.4206e-03,  7.4139e-03, -1.9055e-03,  2.3612e-03,\n",
            "        -1.1238e-02, -2.9476e-03, -1.7415e-02, -1.7678e-03, -1.1767e-03,\n",
            "        -2.2893e-03, -4.6260e-03,  5.0488e-03,  1.9066e-04, -4.3138e-03,\n",
            "        -9.5225e-04, -5.9966e-04,  7.7655e-04,  4.9846e-03, -4.6708e-04,\n",
            "         5.4228e-03,  8.7103e-03, -4.8813e-03,  3.7611e-03,  2.1191e-03,\n",
            "        -6.8024e-03, -2.7951e-03,  3.1339e-03, -9.4608e-05, -1.0741e-03,\n",
            "         5.4122e-03,  9.2780e-03,  2.5490e-03,  4.9811e-03,  7.9523e-03,\n",
            "         3.1308e-03, -3.8672e-03, -3.1193e-03,  5.7576e-04,  2.2750e-03,\n",
            "         3.5901e-03, -1.6208e-03,  8.4059e-04, -7.0913e-04, -1.6487e-03,\n",
            "        -3.3045e-03, -7.6376e-04, -3.9418e-04, -3.9272e-04,  4.0530e-03,\n",
            "         2.5568e-03, -8.0097e-03, -5.9956e-04,  1.4689e-03, -4.9968e-03,\n",
            "        -4.1456e-03,  1.7927e-03, -1.9524e-03, -2.9090e-03, -1.7050e-03,\n",
            "        -9.6934e-04, -4.9693e-03, -2.8804e-03,  2.8753e-03,  1.8471e-03,\n",
            "        -3.5075e-03, -3.2588e-03,  3.5774e-03, -1.5532e-03,  4.1343e-03,\n",
            "        -2.8292e-04,  8.5009e-03,  7.4510e-03, -5.7594e-03, -1.9755e-03,\n",
            "        -1.7901e-03, -8.2944e-04, -1.7002e-03, -3.0182e-03, -9.5820e-04,\n",
            "        -7.4499e-04, -2.5865e-03,  3.4379e-04, -3.4571e-03,  2.8764e-03,\n",
            "         1.0027e-03, -1.4246e-04, -3.7130e-03, -1.4341e-03,  1.8129e-03,\n",
            "         1.8307e-03, -4.8006e-03,  1.9188e-03,  1.1575e-02, -5.8070e-03,\n",
            "        -1.0208e-02,  1.2869e-02,  2.2097e-04, -9.7891e-04,  1.7213e-03,\n",
            "         2.0133e-03,  6.4054e-03,  6.5047e-04,  1.9842e-03,  6.1641e-03,\n",
            "        -6.0819e-03, -5.5761e-03,  5.0444e-03,  3.0619e-03,  2.5571e-03,\n",
            "         3.7050e-03, -5.4695e-03,  4.3742e-04,  3.5363e-03, -3.4058e-03,\n",
            "        -4.0434e-03, -1.8076e-02,  4.2477e-03, -1.8634e-03, -6.6455e-03,\n",
            "        -2.7215e-03, -5.4903e-03, -1.2667e-03,  7.8642e-03,  2.2318e-03,\n",
            "         4.2974e-04,  4.3467e-03,  2.2900e-03, -1.2665e-03, -6.3440e-03,\n",
            "        -2.4504e-03, -8.1339e-03, -6.3484e-03,  2.1217e-04, -3.9087e-04,\n",
            "        -2.7943e-03,  3.5598e-03, -3.3474e-04,  5.6040e-03,  9.1944e-03,\n",
            "        -2.2236e-03, -1.8260e-03,  1.2461e-03, -1.7190e-03, -9.4363e-04,\n",
            "         4.5357e-04, -5.1931e-03,  1.9887e-03, -9.6306e-04,  2.2765e-03,\n",
            "        -2.0058e-02,  4.2592e-03,  1.7020e-03,  3.1692e-03,  7.1921e-04,\n",
            "        -2.8597e-03, -6.8326e-03, -2.6993e-03, -9.3574e-03,  6.2351e-03,\n",
            "        -7.3901e-04,  5.7935e-03, -1.0155e-03,  4.8164e-03,  2.6249e-03,\n",
            "        -2.8891e-03, -3.3473e-03, -2.9976e-03,  1.9446e-03,  1.3205e-03,\n",
            "        -2.5491e-03, -1.9851e-03,  3.9177e-03,  3.7030e-03,  2.3339e-03,\n",
            "        -2.8783e-03,  1.4636e-03, -3.2535e-03, -9.5034e-03,  2.8271e-03,\n",
            "        -9.3190e-03, -8.2312e-05,  2.7465e-03,  2.1135e-04, -7.4581e-03,\n",
            "         1.3575e-02,  2.9917e-03, -6.5620e-03,  2.8324e-04,  8.0884e-03,\n",
            "        -8.2170e-03, -4.3072e-03,  3.2660e-03, -1.5638e-04,  1.1665e-02,\n",
            "         3.5811e-03, -2.6998e-03,  8.6581e-03,  9.2929e-03,  9.2497e-03,\n",
            "        -7.1043e-03, -4.5374e-04,  5.2602e-03,  9.8420e-04, -7.9500e-03,\n",
            "        -8.1448e-03, -3.7335e-03, -8.4095e-03,  9.2634e-03,  2.8333e-03,\n",
            "        -3.6720e-03, -2.3433e-03, -2.0668e-03, -1.2748e-02, -3.5524e-03,\n",
            "        -2.0240e-03,  4.8800e-04, -9.0661e-05,  1.9597e-03,  2.5359e-03,\n",
            "        -1.1524e-03, -3.3582e-03, -3.0711e-03,  3.3554e-03,  4.8448e-03,\n",
            "         8.4079e-03,  3.9626e-03, -7.8786e-03, -7.3669e-03,  5.0285e-03,\n",
            "         4.0662e-03,  4.0981e-03, -8.1725e-03,  2.9604e-03,  5.4628e-04,\n",
            "         3.1695e-03, -2.4278e-03,  8.9728e-05,  8.1722e-03,  1.4756e-03,\n",
            "         1.0930e-03, -3.0006e-04, -3.2150e-04,  5.7529e-03,  5.6333e-03,\n",
            "        -8.4237e-03, -7.3909e-04, -3.0721e-03, -5.4863e-04,  6.4530e-03,\n",
            "         5.0608e-03, -1.1604e-03,  3.7420e-03, -3.3024e-03,  1.5683e-04,\n",
            "        -4.7267e-03,  3.1898e-03,  2.2888e-03, -4.3417e-03,  2.1898e-04,\n",
            "        -7.8783e-03,  2.8381e-04,  2.8753e-03,  9.1599e-03,  8.3369e-03,\n",
            "        -2.4673e-03,  1.9995e-03, -1.7099e-03,  8.8134e-03, -4.3299e-03,\n",
            "        -4.0968e-03,  3.0996e-05,  5.8185e-03,  6.4429e-03,  8.1068e-04,\n",
            "         1.9685e-03,  2.0342e-03,  1.5221e-03,  4.4179e-03,  3.7360e-03,\n",
            "        -1.8440e-03,  3.3737e-03, -4.5799e-03,  1.1131e-02, -3.5976e-03,\n",
            "        -7.1801e-03,  8.4121e-04, -3.9281e-03, -3.8446e-03,  4.5133e-03,\n",
            "        -5.2526e-03,  3.4947e-03, -1.9892e-03, -4.8708e-05, -6.0389e-03,\n",
            "         3.9629e-03, -1.4668e-02,  8.8518e-03,  1.9909e-03,  5.2217e-03,\n",
            "        -6.5975e-03, -5.1146e-03, -1.4401e-03,  4.9188e-03, -7.7127e-03,\n",
            "         3.1345e-03,  4.1862e-03,  6.2971e-03, -4.5204e-03, -2.1628e-03,\n",
            "        -4.7647e-03, -4.5725e-03,  7.6832e-03, -4.1930e-03,  5.7594e-03,\n",
            "         2.0022e-03, -4.5950e-03,  4.5008e-03, -3.0466e-03,  3.1537e-04,\n",
            "         1.4641e-03,  2.9766e-02, -8.8563e-04, -3.7466e-03,  3.6527e-03,\n",
            "        -6.1827e-03, -1.3231e-03,  4.5193e-03,  2.3934e-03,  3.9589e-03,\n",
            "         3.6224e-03,  1.6189e-04, -3.7789e-03,  1.1938e-03,  3.0674e-03,\n",
            "        -6.5973e-03, -4.1290e-03,  9.0389e-03, -6.8314e-03,  4.1216e-03,\n",
            "         1.2163e-03, -2.6441e-04,  2.2211e-03, -2.0252e-03,  3.6908e-03,\n",
            "        -1.3886e-03, -2.6085e-03, -5.1436e-04,  8.6401e-04,  3.0427e-03,\n",
            "         2.9934e-03, -1.2365e-02, -1.0787e-02, -8.6920e-03,  1.5860e-03,\n",
            "        -1.6784e-03, -1.3433e-03, -1.1842e-03,  2.9026e-03,  7.1351e-03,\n",
            "         1.0433e-02, -3.9235e-03, -1.6798e-02, -6.1654e-03, -8.9753e-03,\n",
            "        -7.4827e-04,  5.2294e-03, -4.9602e-03,  4.1733e-04,  1.2334e-03,\n",
            "        -1.0352e-02,  5.2679e-03, -3.7480e-04, -4.4957e-03, -7.8214e-03,\n",
            "         2.0334e-03,  4.6368e-04, -7.7520e-03,  4.9632e-03,  4.0213e-03,\n",
            "         3.9320e-03,  6.0079e-03, -2.0121e-03, -2.2320e-03,  6.9779e-03,\n",
            "         6.6463e-03,  8.9440e-03, -1.9337e-03, -5.5393e-04, -8.6614e-04,\n",
            "        -1.0080e-02, -1.4497e-03, -6.3666e-03, -2.2946e-03,  3.6482e-04,\n",
            "        -4.6345e-03,  4.6946e-04, -2.0178e-03, -2.7923e-03, -1.1921e-02,\n",
            "         2.0607e-03,  3.1313e-03,  3.3156e-03,  1.0050e-02,  1.4181e-04,\n",
            "        -7.5899e-03,  3.1369e-03, -1.5074e-03], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.11.attention.self.value.weight', Parameter containing:\n",
            "tensor([[ 0.0352, -0.0539,  0.0050,  ...,  0.0362,  0.0308, -0.0604],\n",
            "        [-0.0068, -0.0244,  0.0292,  ..., -0.0088,  0.0237, -0.0474],\n",
            "        [-0.0288,  0.0568,  0.0531,  ..., -0.0101,  0.0289, -0.0680],\n",
            "        ...,\n",
            "        [ 0.0159,  0.0165, -0.0316,  ...,  0.0406,  0.0096,  0.0099],\n",
            "        [-0.0500, -0.0191,  0.0224,  ...,  0.0392,  0.1194, -0.0104],\n",
            "        [ 0.0028, -0.0149,  0.0481,  ..., -0.0908,  0.0080, -0.0616]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.11.attention.self.value.bias', Parameter containing:\n",
            "tensor([ 1.5023e-02, -5.3094e-03,  2.3572e-04,  2.0522e-03, -5.7804e-03,\n",
            "         8.1983e-03, -1.1096e-02, -4.4216e-03,  1.0920e-03, -5.7501e-03,\n",
            "        -1.2049e-03,  3.3312e-03, -5.7881e-03,  3.2863e-03,  1.6009e-02,\n",
            "        -3.8654e-03,  7.0702e-03,  9.0583e-03,  9.7348e-03, -8.1410e-03,\n",
            "         1.1158e-02, -5.4021e-03, -1.4808e-02, -6.7811e-03,  5.0396e-03,\n",
            "         4.8309e-03,  1.2264e-02,  1.7033e-03,  3.8622e-03, -5.0164e-03,\n",
            "         1.0777e-02,  2.2679e-03,  1.2602e-02,  1.7358e-03, -8.8981e-03,\n",
            "        -1.6617e-03, -5.4248e-03,  1.5820e-02, -2.7107e-03,  9.3095e-03,\n",
            "         1.4642e-02,  1.8524e-03, -6.2581e-03,  1.3867e-02,  1.6465e-02,\n",
            "        -9.9101e-03, -5.9330e-03,  3.1781e-03,  6.2056e-03, -5.7156e-03,\n",
            "         1.5173e-02,  8.2161e-03, -7.5105e-03, -6.8956e-03,  1.1878e-02,\n",
            "         1.2468e-02, -1.9451e-03,  1.4344e-02,  3.4198e-03,  9.5356e-03,\n",
            "         1.9738e-03, -2.3401e-03,  2.2367e-03, -9.8477e-04,  2.0704e-02,\n",
            "         1.1214e-02,  2.2082e-02, -2.3108e-03, -2.9715e-02,  6.5029e-02,\n",
            "         2.0563e-02, -1.7909e-03,  4.6880e-02,  3.9289e-02, -1.5563e-02,\n",
            "        -1.6345e-02, -1.1725e-02,  8.4864e-03, -2.1302e-03,  1.5644e-03,\n",
            "        -2.6728e-02,  4.6210e-03, -8.3575e-04,  2.1788e-02, -1.8891e-02,\n",
            "         4.3932e-02, -3.4591e-02,  2.0594e-02,  2.4536e-02, -4.4848e-02,\n",
            "        -9.8070e-04, -2.2324e-02, -1.9449e-02,  4.4326e-02, -6.9930e-02,\n",
            "        -2.1216e-02,  1.3391e-02,  2.3917e-02,  2.3278e-02,  7.0228e-03,\n",
            "         4.4435e-02,  1.3210e-02, -5.3480e-02, -4.0530e-02, -3.2905e-02,\n",
            "        -3.2087e-02, -2.6361e-02,  1.2619e-02,  6.9959e-03,  3.2442e-02,\n",
            "        -1.8067e-02, -1.1462e-02, -3.3291e-02, -3.1658e-03, -7.0824e-03,\n",
            "         3.4511e-03, -2.0612e-02,  1.9700e-03,  2.2766e-02,  3.1088e-02,\n",
            "         7.8084e-03,  1.4979e-02,  8.9085e-03, -4.2074e-02, -2.5084e-02,\n",
            "         2.5190e-02,  1.0802e-02,  7.8169e-02,  7.3822e-03, -1.6420e-02,\n",
            "         1.7475e-04, -1.7904e-02, -1.2519e-02, -4.5338e-03,  3.0484e-02,\n",
            "        -1.1422e-02,  2.1765e-02, -1.9468e-03, -8.3815e-03, -7.3409e-03,\n",
            "         1.0873e-02, -1.2188e-02, -1.0292e-02,  8.7603e-03,  7.4500e-03,\n",
            "         1.7819e-03,  2.2365e-02,  1.7328e-02, -1.2085e-02,  8.4014e-03,\n",
            "         2.1003e-02,  5.0341e-03, -5.2389e-03,  1.8634e-02,  5.7822e-03,\n",
            "        -1.0785e-02,  3.9486e-02,  5.5078e-03, -7.9926e-03, -8.9409e-03,\n",
            "        -1.6955e-02, -1.5862e-02,  1.1029e-02, -1.6837e-02, -8.8895e-04,\n",
            "         2.1774e-02,  1.0489e-02, -1.5112e-02,  1.0510e-02,  8.1326e-03,\n",
            "        -1.4907e-02,  1.4861e-02, -4.6929e-03,  3.4145e-02, -1.2089e-02,\n",
            "        -1.2213e-02,  2.2625e-02, -3.7597e-03, -8.0968e-03,  4.7368e-03,\n",
            "         1.8800e-02, -9.8079e-02, -4.5514e-02, -2.0916e-02, -1.4395e-02,\n",
            "        -2.4690e-02, -2.6879e-03,  1.1432e-02, -6.5998e-03,  1.1431e-02,\n",
            "        -1.4127e-04,  2.0039e-02, -2.1528e-02, -1.9943e-02, -5.8667e-03,\n",
            "        -2.6670e-02, -7.7820e-03, -1.3542e-02, -7.0426e-03, -1.3945e-02,\n",
            "         1.1313e-02, -6.0147e-03,  1.9826e-02, -3.6162e-03,  1.1581e-02,\n",
            "        -6.5605e-03, -1.2457e-02,  1.5682e-02,  2.9204e-02,  3.3701e-03,\n",
            "        -6.8273e-03,  5.6615e-04, -1.4298e-03,  1.8995e-02,  3.9813e-02,\n",
            "         3.1168e-02,  2.4817e-02,  2.0974e-02, -4.7637e-03,  1.4747e-02,\n",
            "         2.1972e-02, -5.0200e-03,  1.1263e-02,  5.1302e-02,  5.6657e-03,\n",
            "         5.3651e-04, -2.1440e-02,  1.2138e-02, -1.0497e-02,  4.6990e-03,\n",
            "         2.7727e-02, -2.7869e-02, -5.4554e-04,  3.2653e-02,  1.5652e-02,\n",
            "         1.1317e-02, -1.0897e-02, -4.0616e-02, -3.4058e-02, -7.5577e-03,\n",
            "         6.4053e-03,  6.6166e-03, -2.4033e-02,  1.1459e-02, -1.3984e-02,\n",
            "        -8.1502e-03,  1.1163e-02,  8.2542e-03, -5.3897e-03,  2.8768e-02,\n",
            "         1.1388e-02,  1.0066e-02, -2.4167e-03,  1.8377e-02,  1.2755e-02,\n",
            "         3.8524e-02,  1.9355e-02, -1.7824e-03, -1.0501e-02, -2.3361e-02,\n",
            "         7.0351e-03,  9.4095e-03,  1.2125e-02,  1.9675e-02, -7.4414e-03,\n",
            "         7.8398e-05,  3.4587e-03, -1.0619e-02,  1.5135e-02, -4.1784e-02,\n",
            "         2.5068e-02, -6.0265e-04,  5.0474e-03, -1.8176e-02,  1.2636e-02,\n",
            "         2.9639e-03, -2.2596e-02, -1.4335e-02,  1.3512e-02, -1.1183e-02,\n",
            "         1.5760e-02,  1.0042e-03,  1.4593e-02,  3.8581e-03, -7.9545e-03,\n",
            "        -1.2658e-02, -1.6685e-02, -3.6695e-02, -1.0958e-02, -4.0801e-04,\n",
            "         2.7571e-03,  2.4460e-02, -2.5951e-03, -1.9919e-02,  9.2233e-04,\n",
            "         7.4014e-04,  2.2471e-02,  2.5441e-03,  2.8494e-02,  7.1286e-03,\n",
            "        -2.4914e-02,  1.2981e-02, -3.0747e-03, -3.3040e-02,  2.0458e-02,\n",
            "        -1.9150e-02,  2.1457e-02, -2.2732e-02,  4.8147e-03, -2.1482e-02,\n",
            "        -2.3176e-02,  1.6821e-02,  1.2087e-02,  2.5303e-02, -3.5760e-03,\n",
            "         3.3084e-02,  1.6775e-02, -1.6464e-02, -1.8466e-02,  5.6927e-03,\n",
            "        -5.8718e-02, -5.9075e-03,  2.9579e-03,  4.8263e-02,  1.8685e-02,\n",
            "         2.6903e-02, -1.6432e-02,  8.4332e-03,  4.9153e-02,  1.7822e-02,\n",
            "        -4.2425e-02,  1.8471e-02,  5.7121e-02,  1.2551e-02, -1.5298e-02,\n",
            "        -1.2078e-02,  2.3887e-02,  8.0117e-03,  5.2497e-02,  3.8526e-02,\n",
            "        -3.4434e-02, -2.6399e-02, -4.5675e-02,  3.6598e-02, -3.1015e-02,\n",
            "         1.6242e-02, -3.9804e-02, -4.9304e-03, -1.7245e-02, -2.5515e-02,\n",
            "        -1.2602e-02, -4.1680e-03,  3.8595e-02, -5.9159e-02, -7.4405e-03,\n",
            "         4.8126e-02, -2.0069e-02, -9.4412e-03, -2.6123e-02,  6.4682e-03,\n",
            "         5.1684e-03, -4.6512e-04, -4.9542e-03, -2.4016e-02,  3.6240e-02,\n",
            "        -1.5456e-02,  9.1785e-03,  1.7381e-02,  2.5564e-02, -1.5134e-02,\n",
            "        -1.9038e-02, -3.8031e-02,  6.5497e-03, -1.2592e-02, -1.2116e-02,\n",
            "         3.5311e-02,  4.4152e-02,  1.0126e-03, -4.1217e-04,  1.1888e-02,\n",
            "         1.3587e-02, -1.6456e-02,  4.7564e-02, -3.4366e-02,  4.2985e-04,\n",
            "         8.2520e-03, -4.6107e-03,  1.0151e-03, -8.5547e-03, -1.0867e-02,\n",
            "         1.1150e-02,  1.0995e-02, -6.8069e-03,  8.5477e-03,  1.4168e-02,\n",
            "        -2.4357e-04,  1.0649e-02, -1.5016e-02,  5.9280e-03,  3.4130e-03,\n",
            "         7.2499e-03, -2.3598e-02, -2.2433e-03,  1.0498e-03,  2.2947e-03,\n",
            "        -9.5744e-03, -5.1591e-03,  2.3983e-02, -8.9972e-03,  6.9834e-03,\n",
            "         4.4013e-02,  1.4017e-02,  6.8826e-03,  1.8283e-03, -5.9012e-03,\n",
            "        -1.1004e-02, -7.9089e-03,  2.0125e-03, -3.0178e-03, -6.6287e-03,\n",
            "        -1.4866e-02,  9.6949e-03,  1.2629e-02, -9.4153e-03, -1.8098e-03,\n",
            "         6.4564e-04,  5.0525e-03,  8.1944e-03, -1.4976e-02,  1.5315e-02,\n",
            "        -1.5144e-02, -3.7738e-03, -2.8949e-04,  7.7387e-03, -5.3487e-03,\n",
            "        -1.7106e-02,  1.8400e-02,  4.3561e-03,  1.9203e-02,  8.5607e-03,\n",
            "         8.2309e-03,  5.0004e-03, -1.8184e-02,  9.7382e-03,  2.7278e-04,\n",
            "         1.4275e-02,  5.5432e-03,  1.2724e-02,  1.6008e-02, -1.7855e-02,\n",
            "         1.2448e-02,  1.1468e-02, -2.9315e-03, -2.9225e-02,  4.0265e-02,\n",
            "         1.6124e-03, -7.0005e-03,  2.3734e-04,  3.2821e-03,  2.4101e-02,\n",
            "        -3.6280e-02,  2.0089e-02,  3.1981e-02,  3.9811e-03, -3.0766e-02,\n",
            "         3.8253e-03,  2.3385e-02,  8.5327e-03, -7.5271e-03,  1.5639e-03,\n",
            "         6.1634e-06, -1.1759e-02,  3.0260e-03,  1.0960e-02, -7.2339e-03,\n",
            "        -2.1711e-02,  1.7470e-02,  1.9915e-02,  2.2215e-02, -2.2295e-03,\n",
            "        -1.8607e-03,  1.7004e-02,  5.7661e-03, -5.6229e-03, -5.3306e-03,\n",
            "        -1.1477e-02, -6.7000e-04,  9.3079e-03, -2.9755e-03, -4.7655e-03,\n",
            "         3.3435e-03, -1.1307e-02,  3.9731e-03,  1.0513e-02, -3.2250e-02,\n",
            "        -9.2486e-03, -2.2905e-02,  7.3241e-03, -3.5092e-02, -1.1060e-02,\n",
            "         9.0543e-03, -4.7568e-03, -3.6439e-04, -7.6407e-04,  2.9272e-02,\n",
            "        -2.7470e-02,  1.0781e-02,  1.6108e-02,  1.9351e-02,  1.5294e-02,\n",
            "         1.4930e-02,  2.7142e-02, -1.6444e-03,  5.1867e-02, -4.1590e-02,\n",
            "         2.7485e-02, -1.8159e-02,  1.4616e-02, -2.8223e-02, -2.1836e-02,\n",
            "        -3.7035e-02, -1.2575e-02,  1.2293e-02, -5.4237e-03,  2.2001e-02,\n",
            "        -7.5271e-03, -5.5768e-02, -1.6298e-02,  3.0502e-03, -1.3728e-02,\n",
            "         3.3698e-03, -4.3384e-04, -6.9684e-03, -3.1232e-02,  4.5009e-02,\n",
            "         2.0616e-02, -1.0409e-02, -2.3090e-02, -3.4293e-02,  1.1018e-03,\n",
            "        -8.5000e-03, -2.0849e-02, -3.2696e-02,  4.0556e-02,  2.3771e-02,\n",
            "         3.9398e-02, -1.3882e-02,  5.9056e-03, -9.9360e-03,  1.5168e-02,\n",
            "        -3.1733e-02, -9.0265e-03, -3.6604e-02, -1.7821e-02,  1.9675e-02,\n",
            "         4.0934e-02, -1.2232e-02, -4.6826e-03,  7.0779e-02, -5.4972e-02,\n",
            "         1.6072e-02, -2.5883e-02, -1.1649e-03, -1.1291e-02,  1.5803e-02,\n",
            "         5.6192e-02, -2.3610e-02, -9.9647e-04, -3.2063e-02, -3.2199e-03,\n",
            "        -9.1232e-03, -2.0148e-02, -7.0093e-02, -1.2437e-01,  2.3373e-02,\n",
            "        -5.0414e-04, -2.1878e-02, -1.2511e-02, -3.4763e-02, -3.2012e-03,\n",
            "         1.2513e-02, -1.1341e-02, -4.5446e-02, -2.1909e-02,  1.0888e-02,\n",
            "         1.8216e-02,  6.7742e-03, -5.3086e-03, -2.8458e-02, -2.0761e-02,\n",
            "         3.3507e-03,  4.7860e-03,  8.0991e-03, -1.3183e-02,  2.3937e-03,\n",
            "        -1.5038e-03, -1.7413e-02,  1.8932e-03,  8.7939e-03,  8.2045e-03,\n",
            "        -3.2781e-04,  8.4948e-03, -1.5060e-02,  1.7544e-02,  1.6740e-02,\n",
            "        -3.9248e-03,  2.3059e-03,  1.2013e-02,  7.5180e-03, -4.2036e-02,\n",
            "         8.0001e-04,  1.8606e-02, -4.2438e-03,  8.7675e-03, -2.2118e-02,\n",
            "         1.1333e-03, -9.4967e-03,  1.8470e-02,  1.2243e-02, -7.8087e-03,\n",
            "         7.0338e-03,  1.5369e-02, -2.7767e-03,  1.7380e-02, -1.3810e-02,\n",
            "        -1.8087e-02, -2.5262e-04,  2.8160e-03, -1.0571e-02, -1.2355e-02,\n",
            "        -4.7663e-04, -1.0160e-03,  7.8082e-03,  9.1927e-02,  1.7398e-02,\n",
            "        -4.6346e-03, -7.7768e-03, -1.5489e-02,  6.7890e-03,  1.2695e-02,\n",
            "        -2.2040e-02,  1.0588e-03, -9.3757e-03,  1.0971e-02,  1.1449e-02,\n",
            "        -1.2551e-02,  1.2195e-02, -6.5241e-03,  3.3563e-03, -5.3109e-03,\n",
            "        -7.4463e-03, -2.6397e-03,  7.1599e-03, -1.4743e-02,  2.9362e-03,\n",
            "         5.0424e-03,  4.7287e-03, -4.4464e-03,  1.3988e-02, -2.1329e-04,\n",
            "         6.0693e-03,  3.1315e-02,  2.7540e-02, -1.9092e-02, -1.0151e-02,\n",
            "        -2.2125e-03, -1.5175e-02,  1.7594e-02, -1.3841e-03,  1.5670e-02,\n",
            "         4.5302e-03, -2.3162e-03, -2.3303e-03,  1.9857e-02, -7.0773e-03,\n",
            "        -8.5168e-03,  7.7492e-03, -1.1886e-02, -8.7682e-03,  6.6262e-03,\n",
            "        -7.4530e-04,  9.6382e-03, -2.1810e-02,  3.0818e-04,  1.5030e-02,\n",
            "        -1.1015e-02,  1.8579e-02, -2.0459e-02,  8.5800e-03,  1.7035e-02,\n",
            "         5.3892e-05, -3.4280e-03,  1.0407e-02, -4.1839e-03, -6.5801e-03,\n",
            "         2.4782e-02, -1.2955e-03,  5.8868e-03,  8.7253e-03,  2.5282e-03,\n",
            "        -8.6695e-03, -1.7313e-02, -6.2276e-03, -1.4733e-02,  3.7079e-02,\n",
            "        -2.1547e-04,  2.7257e-03, -5.9696e-04,  1.9285e-02, -2.0536e-02,\n",
            "         1.7143e-02,  1.4449e-02, -6.5542e-03, -2.3365e-02,  1.2347e-02,\n",
            "         8.0866e-03, -2.2223e-02,  3.1805e-03,  3.2623e-02, -1.3899e-02,\n",
            "        -4.5315e-02, -4.4333e-03, -8.2556e-04,  1.6854e-02,  5.2781e-03,\n",
            "        -2.3699e-02,  4.9956e-03,  4.7935e-03,  2.9074e-02,  8.2377e-04,\n",
            "        -1.3017e-02, -4.3768e-02,  1.0733e-02,  1.4151e-03, -1.1715e-02,\n",
            "         3.6786e-02,  3.3582e-04, -1.3081e-02, -2.4121e-03, -1.9138e-02,\n",
            "        -1.5444e-02,  4.6571e-03,  2.4032e-03,  2.0608e-02,  1.9170e-03,\n",
            "        -4.8462e-03, -4.6177e-03, -1.2237e-02, -2.1239e-02, -6.7373e-03,\n",
            "         2.1705e-03,  3.7158e-03,  6.8323e-03, -1.2950e-02,  1.8724e-02,\n",
            "         1.3274e-02,  1.7787e-02, -3.5668e-03,  2.7728e-03, -2.0766e-02,\n",
            "         2.5364e-03,  1.2686e-03, -1.5676e-02, -9.3327e-03, -5.6336e-03,\n",
            "         3.8475e-03, -1.5278e-02,  1.6453e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.11.attention.output.dense.weight', Parameter containing:\n",
            "tensor([[ 0.0236, -0.0468,  0.0435,  ...,  0.0162, -0.0310, -0.0275],\n",
            "        [ 0.0311, -0.0222,  0.0545,  ...,  0.0040, -0.0058, -0.0304],\n",
            "        [-0.0006, -0.0160,  0.0460,  ...,  0.0392,  0.0151,  0.0572],\n",
            "        ...,\n",
            "        [-0.0148, -0.0276, -0.0219,  ...,  0.0260,  0.0732, -0.0707],\n",
            "        [-0.0117, -0.0652, -0.1287,  ...,  0.0433,  0.0770,  0.0071],\n",
            "        [-0.0846,  0.0354,  0.0357,  ..., -0.0091, -0.0571,  0.0281]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.11.attention.output.dense.bias', Parameter containing:\n",
            "tensor([ 2.5661e-02,  2.8438e-03, -4.7568e-03,  2.1495e-02, -1.7552e-02,\n",
            "        -5.8286e-02, -8.2861e-03, -1.9896e-02,  4.7550e-02,  4.5496e-02,\n",
            "         7.5562e-02, -2.5829e-02,  2.9784e-02, -2.9771e-02,  7.9811e-03,\n",
            "        -1.3309e-02,  9.8513e-03, -3.7418e-02, -6.9451e-03, -8.4682e-03,\n",
            "         3.6152e-02,  2.0826e-02, -3.5835e-02,  1.2061e-03,  8.2127e-03,\n",
            "        -3.4215e-02, -5.0382e-02, -3.7946e-02,  1.3244e-02, -2.5479e-02,\n",
            "        -7.1411e-02,  4.4119e-03, -5.0616e-02,  2.1423e-03,  6.5417e-02,\n",
            "         5.8312e-02,  4.3113e-02, -3.1691e-02,  3.3276e-02,  3.9858e-02,\n",
            "         4.3144e-03, -2.1405e-02,  4.3141e-02, -3.2827e-02, -3.3253e-04,\n",
            "         4.5474e-03,  3.2558e-02,  1.1090e-02,  1.1631e-02,  9.3667e-02,\n",
            "         2.5723e-02, -3.6623e-02, -9.7763e-03, -5.6858e-02,  8.9644e-03,\n",
            "        -1.0341e-03,  4.1245e-02, -9.4715e-02, -5.0758e-02, -2.9382e-02,\n",
            "        -4.8360e-02, -7.0828e-03, -7.2977e-02, -2.2347e-02,  2.9260e-03,\n",
            "        -1.9503e-02, -5.4568e-02,  2.1875e-02, -4.5742e-02, -3.4955e-02,\n",
            "         1.5623e-02,  1.7605e-02, -4.6641e-04,  6.3658e-03,  1.3580e-02,\n",
            "         8.2934e-02, -8.5081e-02,  2.1419e-02, -2.6295e-02, -4.3614e-02,\n",
            "        -6.1995e-02,  1.3076e-02,  2.8390e-02,  1.6619e-03,  5.5403e-02,\n",
            "         8.4154e-03,  4.5806e-02, -3.9007e-02, -3.0372e-02, -4.1927e-02,\n",
            "         3.4220e-02, -1.9319e-02, -2.6155e-02, -6.3656e-02, -4.6763e-04,\n",
            "         7.6021e-02, -3.0854e-02,  2.9210e-03, -4.9407e-02,  3.4103e-02,\n",
            "         1.3549e-02,  9.0339e-03, -1.5099e-02,  2.8884e-02,  4.2694e-02,\n",
            "         1.3670e-02, -1.3515e-02,  3.4982e-02,  1.0633e-02,  8.1628e-02,\n",
            "         7.3829e-02, -3.7645e-02,  2.5425e-02, -4.3103e-02,  5.6904e-02,\n",
            "        -5.3126e-02, -3.4979e-02,  2.1494e-02,  6.6136e-03, -2.9801e-03,\n",
            "        -4.0099e-02, -9.0502e-02,  1.9767e-02, -3.6408e-02, -1.4459e-02,\n",
            "        -3.4766e-03, -6.3397e-02, -1.8621e-02, -7.1522e-04,  5.2631e-02,\n",
            "        -2.4067e-02, -6.0373e-02, -2.4351e-02,  2.7051e-02, -1.2240e-02,\n",
            "         1.6177e-02, -4.1943e-02,  3.2331e-02, -4.9130e-02,  5.1233e-02,\n",
            "         1.8584e-02,  4.3312e-02,  3.8919e-02,  9.5328e-04,  2.9318e-02,\n",
            "         4.4681e-02,  4.6009e-02,  3.2909e-02,  6.9443e-02, -4.3578e-02,\n",
            "         2.3167e-02, -3.9903e-02,  2.7478e-02,  2.1105e-02, -2.0274e-02,\n",
            "        -5.6866e-02, -7.1785e-02, -1.9990e-03,  1.5426e-02,  4.6471e-04,\n",
            "        -5.9776e-02,  4.3889e-02,  1.2913e-02,  4.5920e-02, -1.5320e-02,\n",
            "        -3.5193e-02, -5.8929e-02,  7.4193e-03, -1.8163e-04, -1.0267e-02,\n",
            "         1.5405e-02,  2.0184e-02,  4.0796e-03, -1.5596e-02,  1.8059e-02,\n",
            "         9.8132e-03, -2.5639e-02, -3.9419e-02, -3.9124e-02, -4.3264e-02,\n",
            "         8.3899e-02, -3.2598e-02,  2.8068e-02, -2.0178e-02,  1.0051e-02,\n",
            "        -2.8135e-02, -3.6558e-02,  2.2010e-02,  2.0954e-02, -1.0334e-02,\n",
            "         8.8971e-03,  2.8212e-02, -6.6715e-02,  3.7474e-02,  2.1460e-02,\n",
            "        -1.5647e-02,  3.5343e-02, -3.2962e-02, -2.1474e-02, -8.3971e-02,\n",
            "         6.4572e-02,  3.4832e-02, -8.0240e-02,  3.3343e-02,  3.0641e-02,\n",
            "        -8.0081e-02,  5.2524e-03, -9.7763e-03, -4.0240e-02, -3.0048e-02,\n",
            "         1.1611e-02,  3.0359e-02, -4.9964e-02, -9.5533e-03, -2.3882e-02,\n",
            "         1.8626e-02,  3.1076e-02, -2.8956e-02,  8.9075e-04,  1.2022e-02,\n",
            "        -1.1348e-01,  2.8774e-02, -2.4897e-03, -3.1775e-02,  5.3694e-03,\n",
            "        -5.5049e-02,  1.7559e-02, -1.5409e-02, -7.1508e-02,  4.7330e-02,\n",
            "         1.6682e-02,  1.1172e-02,  4.5701e-02,  1.5628e-02, -3.1049e-02,\n",
            "         2.5248e-02, -4.6465e-02,  7.8496e-03, -5.0930e-03, -5.0428e-02,\n",
            "        -1.1631e-01, -1.0528e-02,  7.3653e-02, -8.1540e-02,  5.3539e-02,\n",
            "         1.4288e-02, -3.9082e-02, -1.4117e-02,  5.2567e-02, -3.0875e-02,\n",
            "         3.6666e-03,  2.4879e-02,  3.9342e-02, -2.2215e-02,  5.7524e-03,\n",
            "         1.6564e-02,  2.3978e-02,  2.2593e-02, -1.6909e-02, -5.2501e-02,\n",
            "         4.0561e-02,  2.3099e-02,  1.6978e-02, -1.0553e-02, -3.0434e-03,\n",
            "         1.2279e-03,  3.3629e-03,  5.0138e-02, -5.4341e-02, -1.9108e-02,\n",
            "         9.4589e-03, -2.4958e-02, -1.0721e-03,  6.2184e-02, -2.6718e-02,\n",
            "        -3.5827e-02,  1.3325e-02,  1.9591e-02,  3.3563e-02, -3.0726e-02,\n",
            "        -3.7181e-02, -5.5001e-02, -1.6310e-02,  4.5146e-02,  2.9605e-02,\n",
            "         4.2929e-02, -7.2622e-02,  7.6866e-02,  1.6351e-02, -6.9909e-02,\n",
            "         1.3670e-02, -1.5051e-02, -7.6326e-03, -3.4489e-02, -9.6903e-03,\n",
            "         6.0245e-03,  3.5591e-02,  8.8463e-03,  5.2448e-02, -2.6586e-03,\n",
            "         1.4362e-03, -1.8378e-02,  4.4712e-02, -2.6613e-03, -1.2319e-02,\n",
            "        -1.3705e-02,  6.2081e-03,  2.8958e-02,  1.2483e-01, -2.0615e-02,\n",
            "         2.2608e-02, -1.9340e-02,  1.7909e-02, -1.5324e-02, -1.5443e-02,\n",
            "         7.1784e-02,  3.7500e-02, -2.2002e-02,  2.1981e-02, -2.8093e-02,\n",
            "        -3.9746e-03, -3.1899e-02, -7.3583e-04,  1.8395e-02,  1.5110e-02,\n",
            "         2.1177e-02, -6.4467e-02, -6.3118e-02, -2.1338e-03, -1.0738e-02,\n",
            "         6.5255e-02, -2.8122e-02,  6.8499e-02, -4.4276e-02,  9.3972e-03,\n",
            "        -3.7623e-03,  1.2524e-02,  3.1410e-03,  9.9651e-05, -1.1524e-02,\n",
            "        -2.6397e-02,  6.4055e-02,  3.4766e-02,  2.3612e-02, -7.2052e-03,\n",
            "        -3.3810e-02, -7.8489e-03, -5.2412e-03,  3.5894e-02,  1.7026e-02,\n",
            "        -1.5733e-03, -8.4395e-03, -5.4472e-02, -1.4355e-02,  3.7700e-02,\n",
            "         4.3268e-02, -7.4828e-03, -5.5729e-03,  3.4090e-02, -1.9263e-02,\n",
            "        -7.6067e-02,  3.2265e-02, -4.4947e-02,  4.3225e-02,  3.1662e-03,\n",
            "        -2.7786e-02,  5.8743e-02, -1.2443e-02, -1.7314e-02,  7.0499e-03,\n",
            "        -1.0274e-02, -1.6667e-02, -3.7694e-03,  7.1680e-03,  2.7316e-02,\n",
            "         3.3137e-02, -2.7361e-02,  5.6893e-03,  3.0824e-03, -4.4608e-03,\n",
            "        -2.1742e-02,  1.3567e-02,  3.9650e-02,  2.5170e-02, -2.0057e-02,\n",
            "         8.9578e-03, -1.9388e-02,  1.1835e-02,  5.2143e-02,  3.0606e-03,\n",
            "        -2.5052e-02, -3.6218e-02, -3.0498e-02,  1.9551e-02,  2.3248e-02,\n",
            "        -5.6564e-03,  7.4358e-03, -2.7215e-02,  3.3683e-02,  2.6620e-02,\n",
            "        -4.4610e-02,  5.7572e-02, -3.1604e-02, -2.3627e-02,  2.1448e-02,\n",
            "         7.4826e-02,  1.9757e-02, -2.5340e-02, -4.6560e-02,  7.0425e-02,\n",
            "         3.6600e-02,  6.1359e-02,  2.8134e-02,  2.0915e-02, -3.8265e-02,\n",
            "        -9.8512e-02, -4.6729e-02, -2.1719e-02, -5.3298e-02, -4.8249e-02,\n",
            "         4.7664e-02,  3.4870e-02, -6.3118e-02,  4.6895e-02,  8.7264e-03,\n",
            "         3.6550e-03,  3.8573e-02,  1.5227e-02,  2.0382e-02, -3.1078e-02,\n",
            "         3.8493e-02, -3.8904e-02,  1.9797e-02, -1.7568e-02,  3.3714e-02,\n",
            "         2.1158e-02,  1.4755e-03, -4.0277e-02, -2.2181e-02,  4.5928e-02,\n",
            "        -3.9831e-02,  1.7957e-02,  2.1303e-02,  3.6688e-02,  9.8751e-03,\n",
            "         4.4210e-02,  2.5282e-02,  8.5110e-02,  3.3132e-02,  1.3477e-02,\n",
            "        -6.1588e-02, -3.9152e-02, -6.1249e-02, -5.5067e-03,  1.6928e-02,\n",
            "        -8.8260e-03,  2.8438e-02, -2.5177e-02, -1.1409e-02,  2.4568e-02,\n",
            "        -8.9766e-03,  1.1614e-02,  5.4627e-03, -2.0370e-02, -6.5533e-02,\n",
            "         8.9341e-03, -9.5544e-03, -4.2616e-02, -3.4375e-03,  1.0111e-01,\n",
            "         2.5815e-02,  1.8628e-02, -2.1954e-02, -4.7797e-02,  6.2480e-02,\n",
            "         4.9612e-03, -6.8434e-02, -6.3310e-02, -7.3980e-03,  3.8054e-03,\n",
            "         4.2839e-04,  1.1967e-02, -3.4700e-02,  9.2769e-03,  6.2205e-02,\n",
            "         1.5765e-02, -5.4350e-02, -1.1647e-02, -3.2354e-02,  1.5515e-02,\n",
            "        -1.7500e-03, -1.9691e-03, -4.2725e-02,  4.3707e-02,  2.1509e-02,\n",
            "        -4.5497e-03,  2.8150e-02, -2.1341e-02, -3.6321e-02,  2.9687e-02,\n",
            "         4.2727e-02,  2.2152e-02,  4.2795e-02,  3.5837e-02,  9.6598e-02,\n",
            "         5.9712e-03,  5.0632e-02, -2.0811e-03,  2.6818e-02, -3.0629e-02,\n",
            "         1.3760e-01, -1.4097e-02, -3.7076e-02,  9.3921e-02, -4.3322e-02,\n",
            "        -3.4324e-02,  5.4981e-02, -4.7629e-02,  7.0833e-02,  1.7546e-02,\n",
            "         2.1167e-02, -1.3044e-02,  2.3744e-02,  2.1364e-02, -1.1893e-02,\n",
            "         1.1523e-02, -2.5295e-02,  5.4490e-02, -4.5487e-02,  2.5299e-02,\n",
            "         3.1280e-02,  5.4161e-03, -7.2364e-02, -5.7143e-02,  2.6617e-02,\n",
            "         8.0478e-02, -9.4555e-03, -6.4540e-03, -4.5154e-02,  5.7233e-02,\n",
            "        -4.3240e-02, -4.7108e-02,  2.2601e-02,  1.0235e-01,  3.2031e-02,\n",
            "        -4.0476e-03, -3.6157e-04,  4.4914e-03, -3.0613e-02, -4.4907e-02,\n",
            "        -1.7892e-02,  2.1532e-02,  5.7970e-02, -4.6251e-02, -4.3412e-02,\n",
            "         3.6807e-02, -1.4456e-02, -7.5287e-02,  1.9401e-03,  8.2555e-02,\n",
            "         3.7524e-02, -4.4795e-03, -7.3578e-02,  1.8694e-02, -9.5478e-03,\n",
            "        -1.7393e-02, -3.1529e-02, -1.7297e-02, -7.9018e-02,  4.8072e-02,\n",
            "         5.8333e-02, -5.3367e-02, -2.4755e-02, -2.4387e-02,  2.6818e-02,\n",
            "         4.0837e-02, -1.2251e-02,  1.8094e-02, -2.3803e-02,  1.4613e-02,\n",
            "         6.4903e-02, -1.3786e-02, -1.3899e-02, -3.9337e-02, -1.4666e-02,\n",
            "        -1.1109e-02, -4.4043e-02,  6.9114e-03, -2.7411e-02, -4.7538e-02,\n",
            "        -1.4882e-02, -3.1888e-02,  3.2904e-02,  8.3559e-02, -2.4537e-02,\n",
            "        -2.1875e-02,  2.4720e-02, -2.7432e-02, -3.8548e-02,  1.7113e-02,\n",
            "        -6.7927e-03, -2.0156e-02, -3.4586e-02,  2.7359e-02, -3.1860e-02,\n",
            "        -3.1545e-02,  7.5282e-02, -1.5417e-02,  2.8683e-02, -5.7584e-02,\n",
            "        -4.2532e-02,  1.3981e-02, -1.3137e-02,  5.3427e-02,  2.0525e-02,\n",
            "         4.0794e-02,  1.8850e-02,  7.0226e-02,  7.7246e-03,  3.5578e-02,\n",
            "        -3.1588e-02, -9.3457e-03,  1.9679e-02, -1.5787e-02, -2.7081e-02,\n",
            "         4.7237e-02,  3.1430e-02,  3.3851e-02,  9.3408e-03,  2.6763e-02,\n",
            "        -1.6265e-02,  1.1736e-04,  2.4960e-02, -4.3881e-02, -5.3282e-02,\n",
            "        -1.4752e-02, -1.4754e-02, -1.7534e-02, -8.6470e-02, -5.6863e-02,\n",
            "        -2.6347e-02,  5.2486e-03,  7.7843e-02,  8.2860e-02, -4.1278e-02,\n",
            "         9.1167e-02,  2.1269e-03, -6.5450e-02, -1.3494e-02, -7.1936e-04,\n",
            "        -7.3792e-03, -4.2810e-02, -3.5779e-03, -1.9401e-02, -2.1825e-02,\n",
            "        -3.1193e-02, -1.6985e-02,  1.9392e-02, -4.7002e-02,  1.8754e-02,\n",
            "         2.8218e-04,  1.0279e-02, -3.2457e-02,  3.6972e-02, -3.5447e-02,\n",
            "         1.6364e-02,  2.2258e-02,  1.1818e-02, -9.6817e-03,  8.5271e-03,\n",
            "        -5.5865e-02, -6.0954e-02,  4.2918e-03, -2.5881e-02, -3.2346e-02,\n",
            "         2.5538e-02, -1.3024e-02, -3.9184e-02, -3.0027e-02, -6.1984e-02,\n",
            "        -1.3559e-02,  3.0840e-02, -1.0358e-02,  1.0155e-02, -4.0098e-02,\n",
            "         6.9040e-03, -4.5944e-02, -3.5748e-02,  1.0294e-02,  4.3971e-05,\n",
            "         5.4092e-02, -1.9467e-02,  1.3419e-02,  1.6137e-02, -5.0566e-03,\n",
            "        -3.0796e-02,  3.4840e-02,  3.0833e-02, -2.9789e-02,  7.1727e-03,\n",
            "        -5.6943e-02, -1.8304e-02,  6.2423e-02,  2.1363e-02, -1.2050e-02,\n",
            "        -2.4211e-02, -1.2922e-02, -7.7194e-04, -6.0428e-02, -5.0137e-02,\n",
            "        -3.4469e-03, -3.0698e-03,  5.6597e-02, -1.8726e-02,  2.8393e-02,\n",
            "         2.8395e-02,  5.6358e-02, -1.5945e-02, -4.2779e-02,  3.8556e-02,\n",
            "        -8.2085e-03, -6.2440e-03, -3.7290e-02, -1.2846e-02, -4.0179e-02,\n",
            "         5.2332e-02,  5.0167e-02,  7.8664e-02, -2.7305e-03, -4.5874e-03,\n",
            "         2.8956e-03, -3.4683e-02, -4.0468e-04,  1.0754e-02, -2.0960e-02,\n",
            "         4.8482e-02,  2.1213e-02, -6.5629e-02,  4.4222e-02,  3.2211e-02,\n",
            "         9.3537e-03,  1.4696e-03,  2.8208e-02,  4.5527e-02, -5.7067e-02,\n",
            "         1.1416e-02,  1.6224e-02,  1.3811e-02,  5.3839e-02,  4.2043e-02,\n",
            "         1.6189e-02, -3.2005e-02,  8.8293e-02, -2.0637e-03,  2.7410e-02,\n",
            "         2.9377e-02,  4.1529e-02, -4.1723e-02,  4.1841e-02,  1.3697e-02,\n",
            "        -6.8449e-02,  1.1665e-02, -3.2171e-02,  1.7119e-02,  3.8358e-02,\n",
            "         1.3254e-03, -5.5786e-02, -5.3087e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.11.attention.output.LayerNorm.weight', Parameter containing:\n",
            "tensor([0.8520, 0.8020, 0.8554, 0.8150, 0.8442, 0.8227, 0.8563, 0.8517, 0.8476,\n",
            "        0.8635, 0.8254, 0.8606, 0.8426, 0.8118, 0.8281, 0.9073, 0.8642, 0.8385,\n",
            "        0.8235, 0.8600, 0.8387, 0.8405, 0.8271, 0.8326, 0.8258, 0.8467, 0.8510,\n",
            "        0.8035, 0.8450, 0.8336, 0.8817, 0.8028, 0.8418, 0.8576, 0.8510, 0.8368,\n",
            "        0.8673, 0.8282, 0.8233, 0.8328, 0.8379, 0.8346, 0.8188, 0.8750, 0.8391,\n",
            "        0.8413, 1.1254, 0.8492, 0.8118, 0.8285, 0.8939, 0.8430, 0.8605, 0.8560,\n",
            "        0.8493, 0.8606, 0.8180, 0.8470, 0.8082, 0.8299, 0.8415, 0.8650, 0.8327,\n",
            "        0.8603, 0.8658, 0.8812, 0.8387, 0.8172, 0.8931, 0.8885, 0.8196, 0.8302,\n",
            "        0.8447, 0.8454, 0.8261, 0.8812, 0.8591, 0.8650, 0.8281, 0.8417, 0.8359,\n",
            "        0.8303, 0.8623, 0.8417, 0.8185, 0.8217, 0.8374, 0.8196, 0.8117, 0.9004,\n",
            "        0.8762, 0.8366, 0.8584, 0.8453, 0.8473, 0.8528, 0.8545, 0.8678, 0.8649,\n",
            "        0.8712, 0.8332, 0.8691, 0.8300, 0.8504, 0.8626, 0.8236, 0.8181, 0.8370,\n",
            "        0.8406, 0.9652, 0.8724, 0.8404, 0.8218, 0.8162, 0.8251, 0.8224, 0.8512,\n",
            "        0.8259, 0.8450, 0.8153, 0.8587, 0.8993, 0.8241, 1.2920, 0.8287, 0.8297,\n",
            "        0.8681, 0.8488, 0.8393, 0.8451, 0.8472, 0.8094, 0.8018, 0.8529, 0.8470,\n",
            "        0.8798, 0.8535, 0.8492, 0.8686, 0.8382, 0.8385, 0.8307, 1.0296, 0.9308,\n",
            "        0.8808, 0.8298, 0.8514, 0.8985, 0.8438, 0.8417, 0.8342, 0.8715, 0.8730,\n",
            "        0.8460, 0.8054, 0.8398, 0.8586, 0.8332, 0.8217, 0.7981, 0.8342, 0.8277,\n",
            "        0.8168, 0.8385, 0.8649, 0.8550, 0.8443, 0.8911, 0.8568, 0.8623, 0.8185,\n",
            "        0.8600, 0.8310, 0.8460, 0.8509, 0.8397, 0.8454, 0.8457, 0.8253, 0.8643,\n",
            "        1.1840, 0.8535, 0.8279, 0.8479, 0.8503, 0.8324, 0.8675, 0.8430, 0.8236,\n",
            "        0.8451, 0.8621, 0.8554, 0.8516, 0.8620, 0.8416, 0.8643, 0.8535, 0.8616,\n",
            "        0.8649, 0.8345, 0.8596, 0.8026, 0.8355, 0.8560, 0.8635, 1.2936, 0.8091,\n",
            "        0.8206, 0.8587, 0.8788, 0.8294, 0.8213, 0.8474, 0.8747, 0.8431, 0.8346,\n",
            "        0.8436, 0.8378, 0.8255, 0.8402, 0.8609, 0.8418, 0.8252, 0.8469, 0.8228,\n",
            "        1.0993, 0.8294, 0.8562, 0.8261, 1.0967, 0.8632, 0.8498, 0.8616, 0.8442,\n",
            "        0.8458, 0.8609, 0.8296, 0.8142, 0.8627, 0.8550, 0.8378, 0.8412, 0.8500,\n",
            "        0.9119, 0.8637, 0.8188, 0.8246, 0.8469, 0.8447, 0.8039, 0.8527, 0.8527,\n",
            "        0.8055, 0.8358, 0.8419, 0.8348, 0.8158, 0.8385, 0.8372, 0.8460, 0.8587,\n",
            "        0.8313, 0.8400, 0.8633, 0.8494, 0.8659, 0.8852, 0.8484, 0.8725, 0.8815,\n",
            "        0.8591, 0.8487, 0.8529, 0.9536, 0.8555, 0.8516, 0.8515, 0.8438, 0.8383,\n",
            "        0.8323, 0.8229, 0.8680, 0.8332, 0.8290, 0.8668, 0.8555, 0.8582, 0.9376,\n",
            "        0.8376, 0.8351, 0.8133, 0.8465, 0.8208, 0.8636, 0.8641, 0.8529, 0.8250,\n",
            "        0.8188, 0.8456, 0.8390, 0.8307, 0.8390, 0.8566, 0.8196, 0.7716, 0.8295,\n",
            "        0.8440, 0.8294, 1.6117, 0.8321, 0.8725, 0.8724, 0.8583, 0.8386, 0.8363,\n",
            "        0.7997, 0.7855, 0.8234, 0.8731, 0.8391, 0.8603, 0.8488, 0.8502, 0.8635,\n",
            "        0.8604, 0.8325, 0.8267, 0.9433, 0.8411, 0.8297, 0.8507, 0.8619, 0.8198,\n",
            "        0.8596, 0.9341, 0.8629, 0.8587, 0.8290, 0.8592, 0.8635, 0.8309, 0.7260,\n",
            "        0.8483, 0.8215, 0.8450, 0.8619, 0.8555, 0.8625, 0.8018, 0.8728, 0.8412,\n",
            "        0.8330, 0.9033, 0.8347, 0.8568, 0.8650, 0.8478, 0.8590, 0.8300, 0.8117,\n",
            "        0.9446, 0.8524, 0.8504, 0.8431, 0.8505, 0.8232, 0.8646, 0.8924, 0.8613,\n",
            "        0.8237, 0.8907, 0.8542, 0.8381, 0.8493, 0.9394, 0.8559, 0.8457, 0.8347,\n",
            "        0.8384, 0.8523, 0.8497, 0.7008, 0.8630, 0.8686, 0.8991, 0.8517, 0.8630,\n",
            "        0.8588, 0.8568, 0.8810, 0.8367, 0.8362, 0.8216, 0.8617, 0.8304, 0.7871,\n",
            "        0.8425, 0.8594, 0.8298, 0.8511, 0.8365, 0.8434, 0.8607, 0.8294, 0.8358,\n",
            "        0.8798, 0.8147, 0.8358, 0.8634, 0.8583, 0.8488, 0.8582, 0.8690, 0.8204,\n",
            "        0.8500, 0.8367, 0.8347, 0.8550, 0.8554, 0.8579, 0.8344, 0.8463, 0.8962,\n",
            "        0.8290, 0.8439, 0.8604, 0.8367, 0.7990, 0.8059, 0.8492, 0.8747, 0.8827,\n",
            "        0.8564, 0.8559, 0.8367, 0.8087, 0.8446, 0.8243, 0.8188, 0.8481, 0.8595,\n",
            "        0.8728, 0.8444, 0.8342, 0.8165, 0.9047, 0.8274, 0.8475, 0.8678, 0.8303,\n",
            "        0.8312, 0.8790, 0.8942, 0.8525, 0.8330, 0.8378, 0.8249, 0.8349, 0.8707,\n",
            "        0.8478, 0.8532, 0.8067, 0.8345, 0.8687, 0.8831, 0.8205, 0.8745, 0.8849,\n",
            "        0.8116, 0.8607, 1.0646, 0.8105, 0.8714, 0.8553, 0.8233, 0.8396, 0.8215,\n",
            "        0.8131, 0.8671, 0.8644, 0.8547, 0.8483, 0.8177, 0.8354, 0.8391, 0.8526,\n",
            "        0.8758, 0.8899, 0.8336, 0.8404, 0.8413, 0.8585, 0.8640, 0.8350, 0.8547,\n",
            "        0.8350, 0.8335, 0.8430, 0.8507, 0.8385, 0.8965, 0.8858, 0.8368, 0.8680,\n",
            "        0.8462, 0.8884, 0.8296, 0.8275, 0.8526, 0.8689, 0.8584, 0.8190, 0.8756,\n",
            "        0.8311, 0.9045, 0.8605, 0.8543, 0.8210, 0.8476, 0.8063, 0.8726, 0.8491,\n",
            "        0.8436, 0.8547, 0.8466, 0.8363, 1.0828, 0.8279, 0.8633, 0.8279, 0.8396,\n",
            "        0.8248, 0.8555, 0.8601, 0.8649, 0.9414, 0.8409, 0.8382, 0.8422, 0.7070,\n",
            "        0.8349, 0.8393, 0.8890, 0.8678, 0.8809, 0.8638, 0.8489, 0.8288, 0.8630,\n",
            "        0.8506, 0.8118, 0.8649, 0.8647, 0.8499, 0.8278, 0.8581, 0.7798, 0.8732,\n",
            "        0.8623, 0.8570, 0.8415, 0.8307, 0.8156, 0.8518, 0.8377, 0.8572, 0.8434,\n",
            "        0.8699, 0.8062, 0.8603, 0.8478, 0.8088, 0.8536, 0.8418, 0.8255, 0.8336,\n",
            "        0.8214, 0.8632, 0.8536, 0.8572, 0.8295, 0.8693, 0.8435, 0.8618, 0.8350,\n",
            "        0.8450, 0.8762, 0.8235, 0.8998, 0.8017, 0.8178, 0.8160, 0.8456, 0.8700,\n",
            "        0.8464, 0.8341, 0.8547, 0.8416, 0.8600, 0.8253, 0.8222, 0.8422, 0.8318,\n",
            "        0.8716, 0.8259, 0.8703, 0.8493, 0.8493, 0.8867, 0.8211, 0.8444, 0.8451,\n",
            "        0.8555, 0.8471, 0.8320, 0.8377, 0.8427, 0.8544, 0.8529, 0.8680, 0.8911,\n",
            "        0.8428, 0.8413, 0.8471, 0.8487, 0.8346, 0.8576, 0.8398, 0.8257, 0.8380,\n",
            "        0.8574, 0.8316, 0.8696, 0.8554, 0.8419, 0.8407, 0.8487, 0.8443, 0.8418,\n",
            "        0.8603, 0.8382, 0.8771, 0.8511, 0.8505, 0.8360, 0.8882, 0.8473, 0.9015,\n",
            "        0.8756, 0.8869, 0.8460, 0.8616, 0.8442, 0.8155, 0.8514, 0.8648, 0.8657,\n",
            "        0.8804, 0.8519, 0.8621, 0.8479, 0.8607, 0.8232, 0.8773, 0.8574, 0.8427,\n",
            "        0.8494, 0.9454, 0.8615, 0.8602, 0.9206, 0.8314, 0.8427, 0.8468, 0.8528,\n",
            "        0.8728, 0.8320, 0.8267, 0.8138, 0.8464, 0.8511, 0.8437, 0.8251, 0.8903,\n",
            "        0.8346, 0.8762, 0.8286, 0.8463, 0.8644, 0.8704, 0.8401, 0.8421, 0.8382,\n",
            "        0.8371, 0.8343, 0.8397, 0.8621, 0.8402, 0.8429, 0.8619, 0.8596, 0.8597,\n",
            "        0.8786, 0.8325, 0.8564, 0.8275, 0.8584, 0.8863, 0.8499, 0.8511, 0.8212,\n",
            "        0.8188, 0.8686, 0.8701, 0.8482, 0.8704, 0.8937, 0.8291, 0.8391, 0.8516,\n",
            "        1.5371, 0.8554, 0.8568, 0.8563, 0.8404, 0.8409, 0.8398, 0.8875, 0.8691,\n",
            "        0.8495, 0.8180, 0.8389, 0.8536, 0.6582, 0.8556, 0.8524, 0.8522, 0.8569,\n",
            "        0.8311, 0.8471, 0.8558, 0.8501, 0.8438, 0.8706, 0.8647, 0.8470, 0.8286,\n",
            "        0.8387, 0.8455, 0.8571, 0.9022, 0.8090, 1.0631, 0.8578, 0.8398, 0.8367,\n",
            "        0.9711, 0.8156, 0.8653, 0.8613, 0.7998, 0.7831, 0.8282, 0.8466, 0.8523,\n",
            "        0.8346, 0.8365, 0.8424], device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.11.attention.output.LayerNorm.bias', Parameter containing:\n",
            "tensor([-3.1344e-02,  1.2080e-02, -4.6364e-02, -3.0130e-02,  7.9443e-02,\n",
            "         5.0981e-02,  1.1908e-02, -2.0652e-02, -1.0936e-01, -1.1688e-01,\n",
            "        -8.6071e-02,  2.6358e-02, -1.1645e-01,  1.5676e-02, -3.1420e-02,\n",
            "         3.6622e-02,  5.6841e-02,  4.6632e-02, -8.3829e-03, -7.8806e-02,\n",
            "        -2.4978e-02, -3.3134e-02,  6.8759e-02,  3.2692e-02,  3.3648e-02,\n",
            "        -2.8723e-02,  8.5249e-03, -5.7247e-02, -1.0357e-01, -1.4365e-02,\n",
            "         6.1772e-02, -7.3953e-02, -3.5921e-04, -3.0949e-02, -1.3592e-01,\n",
            "        -7.2458e-02, -5.0496e-02, -1.5356e-02, -5.2733e-02, -1.0026e-01,\n",
            "        -1.4037e-01, -6.2274e-02, -7.1671e-02, -1.5139e-02, -8.2626e-02,\n",
            "        -4.7771e-02, -1.4454e-01,  3.6936e-02, -7.9435e-02, -3.5952e-02,\n",
            "        -1.0434e-01,  7.1274e-02,  3.2480e-02,  6.3918e-02,  2.8611e-02,\n",
            "        -1.6015e-02, -1.3967e-01, -1.6648e-02, -6.6715e-02, -7.7961e-02,\n",
            "        -8.4401e-03,  4.0578e-02,  1.0819e-01, -4.4659e-02,  1.1845e-02,\n",
            "         5.9508e-02,  1.6610e-02,  1.8677e-02, -1.1001e-01, -1.2207e-01,\n",
            "        -1.0786e-01, -9.6549e-02, -6.1122e-02, -3.2128e-02, -7.1845e-02,\n",
            "        -1.2359e-01, -2.4683e-02, -1.9532e-02,  1.5631e-02,  9.3122e-02,\n",
            "        -7.1852e-02, -2.3720e-02, -3.4551e-02, -2.2903e-02, -4.2727e-02,\n",
            "        -8.6009e-02, -9.0888e-02,  9.3918e-02, -5.9314e-02,  7.0650e-02,\n",
            "        -1.0035e-01,  1.4553e-02,  1.3380e-02,  1.0654e-01,  7.3451e-03,\n",
            "        -1.3439e-01,  1.1810e-01, -7.5076e-02, -3.9675e-02, -1.2730e-02,\n",
            "        -9.3441e-02, -7.8124e-02,  5.0937e-03, -4.6052e-02, -1.3498e-01,\n",
            "        -9.7142e-02, -7.0380e-02, -9.0184e-02, -5.4622e-02, -1.1805e-02,\n",
            "        -9.8805e-02,  9.0518e-03, -6.6125e-02, -7.6516e-02, -1.6316e-01,\n",
            "         9.1859e-02,  6.8594e-02, -1.1347e-01, -9.3453e-02, -1.6409e-02,\n",
            "         4.9831e-02,  5.9697e-02, -2.5936e-02,  3.3220e-01, -1.5791e-02,\n",
            "        -4.6808e-02,  1.6120e-03,  5.4064e-02, -1.0811e-01, -1.0923e-01,\n",
            "        -2.3789e-02,  1.1774e-01, -2.7513e-03, -1.0053e-01, -6.7293e-02,\n",
            "        -1.4769e-01, -5.7897e-02, -1.0835e-01, -3.9113e-03, -3.5699e-02,\n",
            "        -5.1963e-02, -9.8398e-02, -8.2750e-02, -6.5031e-02, -5.8932e-02,\n",
            "        -3.8372e-02, -3.2102e-02, -1.0399e-01, -2.4155e-02, -6.1841e-02,\n",
            "        -5.7919e-02,  1.0234e-03, -1.5286e-01, -1.6116e-01, -6.3931e-02,\n",
            "         5.4084e-02,  6.7514e-02, -4.2360e-02, -9.1926e-02, -3.0753e-02,\n",
            "         3.0540e-02, -6.8048e-02, -6.3836e-02, -1.0519e-01,  3.6709e-02,\n",
            "         2.3915e-02,  8.1677e-02,  6.4061e-02, -1.0920e-01,  6.5083e-02,\n",
            "        -7.1140e-02, -9.0670e-02,  5.8689e-02, -7.2455e-02, -8.5068e-02,\n",
            "        -6.9670e-02,  6.7066e-02,  2.5771e-02,  6.9231e-02, -1.5279e-02,\n",
            "        -6.2065e-01,  4.2855e-02, -1.0108e-01,  1.4958e-02, -5.1842e-02,\n",
            "        -2.9548e-02,  1.2663e-01, -1.9485e-02, -1.4611e-02, -7.0454e-02,\n",
            "        -1.5010e-01, -1.1479e-01, -2.0274e-02, -4.6186e-02,  3.7312e-02,\n",
            "        -4.6845e-03, -1.1243e-01,  5.7662e-03, -4.8168e-02,  4.1642e-02,\n",
            "        -1.2994e-01, -1.0522e-01,  5.8554e-02, -7.5463e-02, -1.3078e-01,\n",
            "         3.4147e-02,  3.6588e-02, -4.6127e-02, -3.3408e-02,  3.4817e-03,\n",
            "        -4.2363e-02, -4.2850e-02,  6.9876e-02, -4.7035e-02,  3.6683e-02,\n",
            "         2.3717e-02, -5.6663e-02, -2.3482e-02,  2.2695e-03, -7.1891e-02,\n",
            "         3.4086e-02, -3.9050e-02, -5.4687e-02, -1.0103e-02, -4.6291e-02,\n",
            "        -1.3215e-01, -4.2017e-03, -6.4696e-02,  7.4318e-02, -3.7038e-02,\n",
            "        -7.4783e-02,  3.7466e-03, -4.4902e-02, -5.4526e-02, -3.2154e-02,\n",
            "        -6.9844e-02, -1.7045e-02, -9.5056e-02,  5.8156e-02, -1.0002e-01,\n",
            "        -2.9361e-03,  5.3961e-02, -1.5724e-02,  1.2291e-01, -7.9840e-02,\n",
            "        -6.2188e-02, -3.2282e-02,  8.2793e-03, -1.1508e-01, -1.4068e-01,\n",
            "        -5.5942e-02, -1.2841e-01, -4.0451e-02, -4.0718e-02, -6.1471e-02,\n",
            "        -8.3071e-02, -1.5063e-02, -2.3455e-02,  1.1974e-02,  7.6468e-02,\n",
            "        -6.5692e-02, -1.8200e-02, -1.4435e-02, -6.2212e-02, -1.2290e-01,\n",
            "        -1.7033e-01, -2.5049e-02, -9.7031e-02,  7.9129e-03, -3.5679e-02,\n",
            "        -6.2448e-03,  5.5563e-02, -9.0274e-02,  3.9126e-02, -1.1295e-01,\n",
            "        -5.1131e-02,  5.9279e-03, -9.5505e-03, -6.1520e-02, -2.4447e-02,\n",
            "        -1.8512e-02,  8.4790e-02, -9.8691e-02, -8.7056e-02, -1.1217e-01,\n",
            "        -1.5344e-01,  5.8255e-02, -1.4153e-01, -7.1881e-03, -1.1657e-03,\n",
            "        -7.4974e-02,  4.0591e-02, -8.0208e-02,  3.8199e-02, -5.8486e-04,\n",
            "         5.0867e-02,  3.5725e-03, -1.4645e-02, -2.7721e-04, -5.1357e-02,\n",
            "        -5.7399e-02, -8.0470e-03, -6.7928e-02,  2.5977e-02, -1.1493e-01,\n",
            "        -2.4930e-02, -1.7211e-02, -1.4713e-01, -2.2575e+00,  3.7316e-02,\n",
            "        -1.8979e-02, -1.0595e-01, -1.0567e-02, -7.6957e-02,  2.2319e-02,\n",
            "        -1.2596e-01, -5.9056e-02,  3.7224e-02,  1.6287e-02, -6.8733e-02,\n",
            "        -4.9052e-02,  4.2215e-02, -6.1539e-03, -9.2575e-02, -2.2745e-02,\n",
            "        -4.3827e-02,  1.5006e-02,  2.1218e-02, -5.9763e-02, -1.6321e-01,\n",
            "        -1.3407e-01,  7.0248e-02, -2.2305e-02,  4.9774e-02, -7.5658e-02,\n",
            "        -5.4044e-02, -1.0192e-01, -7.0567e-02, -4.1365e-02, -1.0061e-01,\n",
            "        -1.5344e-02, -2.0617e-02, -7.9850e-02, -9.0090e-02, -1.2089e-03,\n",
            "        -1.7109e-02, -1.2406e-01, -1.1144e-01, -1.0369e-01, -7.0175e-02,\n",
            "        -5.0133e-02, -1.3557e-02,  1.6262e-01, -9.9455e-02, -7.2051e-02,\n",
            "        -1.1553e-01,  3.9777e-03,  5.5598e-02, -1.8694e-02, -9.6043e-04,\n",
            "         6.9841e-02, -7.7826e-02,  5.5478e-02, -4.6886e-02, -7.8571e-03,\n",
            "        -1.7885e-02, -1.2144e-01,  3.0650e-02,  5.2436e-03, -8.6549e-02,\n",
            "        -1.2067e-01, -1.2578e-01, -3.0886e-02, -1.4648e-01, -1.3343e-01,\n",
            "        -9.9811e-02,  1.0904e-01,  5.2390e-02, -6.2593e-02,  7.3330e-02,\n",
            "        -8.2041e-02, -2.8029e-01, -1.3285e-02, -1.2791e-01,  2.9784e-02,\n",
            "        -4.9950e-02,  1.3312e-02, -2.3558e-04, -1.0833e-01, -9.1062e-02,\n",
            "        -3.6797e-02,  4.6885e-02, -8.6031e-03, -7.7853e-02, -4.7832e-02,\n",
            "         2.6897e-02, -4.3142e-02, -8.8117e-02,  1.3069e-02, -7.0432e-02,\n",
            "        -3.8987e-02, -9.9468e-02,  4.2137e-02,  2.7479e-02, -6.7665e-02,\n",
            "        -4.4414e-02,  7.5395e-03, -2.0309e-03,  4.0419e-02, -1.4872e-01,\n",
            "        -8.2230e-02, -6.8386e-02, -6.8401e-02, -4.1047e-02, -8.8755e-02,\n",
            "         5.6166e-02,  5.6017e-02, -5.5956e-02,  2.1983e-02, -2.2961e-02,\n",
            "        -7.1032e-03, -7.5360e-02,  6.5075e-02, -9.5735e-02,  4.0330e-02,\n",
            "         7.4802e-02,  8.9258e-03,  2.9245e-02, -1.5354e-02,  6.8174e-02,\n",
            "        -1.0322e-01, -2.5460e-03, -1.0041e-01, -1.2218e-02, -8.0847e-02,\n",
            "        -6.2028e-02,  6.3530e-02,  8.3978e-03, -5.7599e-02, -1.0613e-01,\n",
            "        -2.3226e-02,  5.4509e-02,  1.6102e-02, -4.9709e-02, -1.4171e-01,\n",
            "        -1.9604e-01, -9.9754e-02, -9.7179e-02, -8.0188e-02,  3.6264e-02,\n",
            "         3.3763e-02, -1.4644e-02,  1.3093e-01,  8.6540e-03,  2.5184e-02,\n",
            "         1.3682e-02, -9.1370e-02, -6.9037e-04, -2.6415e-02, -6.0097e-02,\n",
            "        -3.5762e-03, -4.9125e-02, -8.7560e-02,  5.8042e-02,  8.4096e-02,\n",
            "         3.6079e-03, -5.9528e-02, -7.0748e-02,  3.0864e-02, -1.0618e-01,\n",
            "        -8.6109e-02, -4.3467e-02, -7.9412e-03,  1.1864e-01, -8.4393e-02,\n",
            "        -6.7752e-02, -1.3682e-02,  1.3190e-01, -4.1765e-02, -9.5315e-02,\n",
            "        -8.5517e-02, -1.0381e-02,  7.4308e-03,  2.5057e-02, -1.2178e-01,\n",
            "        -1.0872e-01, -3.0953e-02, -6.2515e-03, -6.9138e-02, -1.0413e-01,\n",
            "        -9.2249e-02, -8.9106e-03,  1.4600e-01, -1.2124e-01, -8.7356e-02,\n",
            "        -1.2041e-01, -1.1749e-01, -3.5789e-02,  3.8827e-02, -8.9954e-03,\n",
            "        -2.3699e-02, -7.3208e-02, -8.5094e-02, -1.4362e-01, -5.3073e-02,\n",
            "         1.5437e-02, -8.0135e-02,  1.2968e-02, -2.8527e-02, -1.8904e-02,\n",
            "        -1.3154e-01,  3.9962e-02,  7.3063e-02, -2.0348e-01,  3.3176e-02,\n",
            "        -4.0622e-02, -7.5475e-02,  6.5075e-02, -1.1102e-01, -6.7218e-02,\n",
            "        -7.5741e-02,  2.4920e-02, -1.2435e-02, -1.9510e-02, -1.4103e-02,\n",
            "        -6.3889e-02,  1.4682e-01, -1.2999e-01, -4.1460e-02, -1.1495e-01,\n",
            "        -1.9698e-02, -1.1444e-01, -3.0427e-02,  4.8089e-02, -6.2266e-02,\n",
            "        -7.7924e-02,  2.4362e-04, -1.4317e-01, -3.6246e-02, -1.2393e-01,\n",
            "         4.1629e-02,  8.1272e-02, -8.8061e-02, -7.7921e-02, -8.0148e-02,\n",
            "        -9.8167e-02, -8.5414e-02, -1.5176e-02,  1.9671e-02,  5.1377e-02,\n",
            "        -1.5865e-02, -1.1811e-01, -7.8402e-02,  2.3705e-02,  2.9540e-02,\n",
            "        -8.7734e-02, -8.4668e-02,  9.9336e-02, -4.6445e-02, -1.6185e-01,\n",
            "        -1.3077e-01, -4.2604e-02,  5.8688e-02,  1.1358e-02, -1.2765e-01,\n",
            "        -6.1923e-02, -8.4753e-02, -3.0332e-02, -6.0014e-04, -7.5688e-02,\n",
            "        -1.0452e-01, -8.2666e-03,  4.5868e-02, -3.2654e-02, -4.4266e-02,\n",
            "        -5.2327e-02,  8.4792e-02, -7.2354e-02,  2.3774e-02, -3.6025e-02,\n",
            "        -1.1545e-01, -2.8853e-02, -2.0315e-02,  2.3356e-02, -5.5108e-02,\n",
            "        -3.2399e-02,  1.0912e-01, -1.3768e-01, -9.9473e-02, -1.2217e-02,\n",
            "        -9.2016e-02, -3.4299e-02,  1.1813e-02, -2.0268e-01, -1.2499e-02,\n",
            "         3.3174e-02, -1.0902e-01, -2.4698e-02, -1.8718e-02, -1.3656e-02,\n",
            "        -1.8760e-02, -5.9220e-02, -2.1743e-02, -5.2343e-02,  6.8899e-02,\n",
            "         3.8413e-02, -1.1812e-01,  4.0992e-02, -8.7708e-02,  6.0118e-02,\n",
            "         1.1129e-01, -2.3863e-03, -2.4479e-02,  5.5275e-02, -4.8966e-02,\n",
            "        -1.3996e-01, -1.2337e-01,  5.8795e-03,  1.5241e-02, -1.1743e-01,\n",
            "         2.0023e-02, -1.0781e-02, -8.2880e-02, -6.0243e-02,  9.4822e-03,\n",
            "        -1.2696e-01,  4.8240e-03, -6.5649e-02,  6.8928e-03, -5.0116e-02,\n",
            "         8.7673e-02,  3.1290e-02, -2.8279e-02,  1.1816e-02, -7.9418e-03,\n",
            "         3.5138e-02, -3.7828e-02,  1.0395e-01,  5.2070e-02,  1.1879e-02,\n",
            "        -3.6229e-02,  5.4177e-02, -1.7798e-01, -9.3546e-02,  3.3682e-02,\n",
            "        -1.0662e-01, -3.3063e-02,  2.0191e-02,  5.7537e-02, -1.8093e-02,\n",
            "        -9.0532e-02,  9.5468e-02, -7.4289e-02, -2.5821e-02, -2.6561e-02,\n",
            "        -4.5162e-02,  2.9504e-02, -3.1473e-02,  2.8452e-02, -2.9851e-02,\n",
            "        -2.8579e-02, -9.0557e-02,  5.2943e-03, -5.8344e-02,  4.7541e-02,\n",
            "        -2.5308e-02, -1.0288e-01, -4.9240e-02, -5.8421e-02,  1.6592e-02,\n",
            "         1.2111e-01, -5.4313e-02, -8.1466e-02, -4.9400e-02,  7.0303e-03,\n",
            "        -4.2782e-02, -4.3525e-02, -8.5807e-03,  6.2815e-02,  5.9304e-02,\n",
            "        -1.0129e-01, -6.6303e-02, -7.5956e-03, -6.3686e-02, -2.5517e-02,\n",
            "        -2.0086e-02, -6.0147e-02, -2.3865e-02, -3.3763e-02, -7.4924e-02,\n",
            "        -1.3382e-01, -1.6790e-01, -1.3245e-01, -3.7097e-02,  2.2390e-02,\n",
            "        -1.0047e-01, -8.1847e-02, -7.7880e-02,  5.0404e-02, -1.4665e-01,\n",
            "         5.9582e-02, -1.1912e-02, -1.1096e-01, -1.0069e-01,  6.5853e-02,\n",
            "         1.5271e-02,  4.4003e-02,  1.9579e-02,  4.8464e-02,  6.3348e-02,\n",
            "        -6.8227e-02,  3.1810e-02, -3.2205e-02,  8.8901e-02, -5.0658e-02,\n",
            "        -1.0127e-01, -1.6246e-01,  1.6157e-02, -4.4182e-02, -6.6988e-02,\n",
            "        -5.1723e-01,  1.4864e-02, -7.6332e-02, -7.5355e-03,  2.3505e-02,\n",
            "        -4.6339e-02, -1.4055e-02, -1.6824e-01,  7.3489e-03, -3.3287e-02,\n",
            "        -6.3824e-04,  5.9921e-03, -8.3247e-02,  9.5896e-02,  3.1140e-02,\n",
            "        -5.0645e-02, -1.2309e-01,  2.1778e-02, -8.4379e-02, -7.0738e-02,\n",
            "        -8.9048e-02, -4.2593e-02, -1.2673e-01, -1.4987e-01,  8.0047e-02,\n",
            "         7.0084e-03, -8.5599e-02, -6.0880e-02, -5.7156e-02, -8.4430e-03,\n",
            "        -6.1077e-02, -9.3670e-02, -6.0107e-02, -5.3923e-02, -5.4385e-02,\n",
            "        -1.1415e-01, -1.8597e-01, -2.0845e-02, -4.5953e-02,  1.9309e-02,\n",
            "         9.4666e-02, -1.1881e-01, -4.2700e-03, -1.2863e-01, -4.4709e-02,\n",
            "        -4.3273e-02,  5.3781e-02,  2.5261e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.11.intermediate.dense.weight', Parameter containing:\n",
            "tensor([[ 0.0587,  0.0547, -0.0215,  ...,  0.0323,  0.0472, -0.0258],\n",
            "        [-0.0112, -0.0313, -0.0500,  ...,  0.0239,  0.0018,  0.0107],\n",
            "        [ 0.0021,  0.0415, -0.0370,  ...,  0.0286,  0.0118, -0.0310],\n",
            "        ...,\n",
            "        [ 0.0422,  0.0793,  0.0202,  ...,  0.0414, -0.0607,  0.0131],\n",
            "        [-0.0060,  0.0452,  0.0393,  ..., -0.0117,  0.0522,  0.0407],\n",
            "        [-0.0437, -0.0512,  0.0376,  ...,  0.0037,  0.0824, -0.0069]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.11.intermediate.dense.bias', Parameter containing:\n",
            "tensor([-0.0976, -0.0618, -0.0515,  ..., -0.1151, -0.0466, -0.1224],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.11.output.dense.weight', Parameter containing:\n",
            "tensor([[-0.0224,  0.0540, -0.0089,  ..., -0.0939, -0.0108,  0.0464],\n",
            "        [ 0.0107, -0.0081, -0.0156,  ...,  0.0101, -0.0167,  0.0185],\n",
            "        [-0.0136, -0.0597, -0.0227,  ...,  0.0007,  0.0513,  0.0980],\n",
            "        ...,\n",
            "        [-0.0232, -0.0076,  0.0177,  ..., -0.0031, -0.0152,  0.0120],\n",
            "        [-0.0114,  0.0444, -0.0052,  ..., -0.0605, -0.0259,  0.0889],\n",
            "        [-0.0367, -0.0215,  0.0213,  ..., -0.0128, -0.0067,  0.0139]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.11.output.dense.bias', Parameter containing:\n",
            "tensor([-6.5741e-02,  4.2078e-02,  1.2011e-02,  2.2932e-03,  5.5518e-02,\n",
            "         4.5913e-02,  2.2022e-02, -5.5168e-02, -2.1741e-02, -1.0107e-01,\n",
            "        -9.8937e-02,  5.2764e-02, -7.3483e-02,  8.6208e-02,  6.0888e-02,\n",
            "         1.2144e-01,  1.0733e-02,  4.1013e-03,  2.8674e-02, -3.3970e-02,\n",
            "         2.2772e-02, -6.7903e-03,  4.9564e-03,  1.4364e-02,  2.9709e-02,\n",
            "        -6.4162e-03,  5.6278e-02,  6.3540e-02, -1.5162e-02, -1.7254e-02,\n",
            "        -2.8550e-04,  4.7133e-02,  3.4972e-02, -7.6128e-02,  2.4526e-02,\n",
            "        -2.5571e-02, -3.0283e-02, -5.3822e-03,  2.4152e-02, -5.9109e-02,\n",
            "        -4.5285e-03, -1.7886e-02, -4.1091e-02,  1.0376e-02,  4.9845e-02,\n",
            "         4.7674e-02, -8.8754e-02,  7.5486e-02, -1.0166e-01, -7.9692e-02,\n",
            "         4.3310e-03,  5.8896e-02,  7.9482e-02,  2.7634e-03, -2.2596e-02,\n",
            "         6.1754e-02, -2.9951e-02,  6.2471e-02,  3.7189e-02,  4.2943e-02,\n",
            "        -1.5058e-02, -2.4581e-02,  6.8931e-02, -8.8080e-03, -2.2560e-02,\n",
            "        -5.4985e-02,  5.4128e-02,  7.5724e-02,  2.8788e-02,  4.6660e-02,\n",
            "        -4.4814e-03,  3.1812e-02, -3.7511e-02, -8.0002e-02,  1.0749e-01,\n",
            "        -6.5850e-02,  4.5588e-02,  2.3601e-02,  5.3216e-02,  8.7131e-02,\n",
            "         7.3938e-02, -3.2171e-02,  2.5774e-02,  9.8898e-03,  2.4024e-03,\n",
            "        -6.3372e-02, -9.0037e-02, -3.0310e-02, -1.5730e-02,  9.7426e-02,\n",
            "        -7.6012e-02, -5.0295e-03,  8.4208e-02,  5.3904e-02,  3.9876e-02,\n",
            "        -1.5031e-01,  1.3342e-02,  5.5709e-03,  8.6004e-02, -3.2644e-02,\n",
            "        -6.3840e-02, -4.0073e-02,  1.4759e-03, -2.9463e-03, -1.3734e-01,\n",
            "        -7.9349e-02, -5.9826e-02, -7.3977e-02, -7.9907e-02, -9.2668e-02,\n",
            "        -2.6027e-02,  2.4271e-02, -9.1017e-02,  1.4410e-02, -5.3724e-02,\n",
            "         1.5609e-02,  4.0855e-02,  4.5932e-02, -4.2466e-02, -3.3923e-02,\n",
            "         5.1573e-02,  1.4564e-01,  3.8452e-02, -3.4191e-03,  4.3361e-02,\n",
            "         3.6211e-02,  5.3221e-02, -2.2619e-02, -1.5577e-02, -7.8316e-02,\n",
            "        -1.2720e-02,  6.6331e-02,  7.5486e-03, -5.7261e-03, -1.3333e-02,\n",
            "        -8.1408e-02,  4.1948e-02,  1.4382e-02,  8.5527e-02, -3.4351e-02,\n",
            "        -5.0566e-02, -4.0421e-02, -1.1631e-01,  3.7037e-02,  6.9312e-02,\n",
            "         6.3102e-03, -7.9192e-02, -1.0396e-01, -9.4308e-02,  1.1846e-01,\n",
            "        -6.8122e-02,  6.9707e-02,  8.7248e-03,  7.6877e-04, -4.4632e-02,\n",
            "         8.9779e-02, -5.9622e-03,  3.3064e-02, -1.0618e-02,  2.4632e-02,\n",
            "         4.5643e-02, -7.5217e-03,  5.2801e-02, -3.7286e-02,  2.0312e-03,\n",
            "         2.8662e-02,  3.2067e-02, -1.7333e-02,  4.4542e-02,  3.4369e-02,\n",
            "        -7.0471e-02, -4.3833e-03, -2.4420e-02, -5.7783e-02, -2.8720e-02,\n",
            "        -1.1356e-02, -2.3630e-03, -2.0051e-03, -7.4737e-02, -8.6264e-03,\n",
            "        -1.6157e-01,  3.9566e-03, -3.2327e-02, -5.7044e-02, -6.0499e-02,\n",
            "         2.5157e-02,  5.5642e-02, -2.2992e-02,  7.6248e-02,  1.8392e-02,\n",
            "         4.4709e-02, -9.9869e-02,  5.4718e-02, -4.5948e-04,  6.2032e-02,\n",
            "        -7.7357e-02,  1.8792e-02,  4.1057e-02,  6.4767e-02,  4.7432e-02,\n",
            "        -9.0086e-02, -2.4933e-02,  4.9760e-02, -9.5229e-02,  2.1131e-02,\n",
            "         1.2652e-01,  2.1365e-02,  3.0726e-02,  3.9724e-02,  2.1305e-02,\n",
            "         1.0902e-02, -5.3310e-02, -5.6873e-03,  1.0282e-02,  9.9563e-02,\n",
            "        -8.4239e-02, -2.7006e-02, -1.1546e-01,  4.0102e-02, -8.9660e-02,\n",
            "         4.6134e-02,  4.1681e-02,  9.0483e-04,  4.2167e-03,  7.7366e-03,\n",
            "         5.3710e-02,  2.2877e-02, -3.4592e-02,  4.5413e-02, -9.4250e-02,\n",
            "        -5.7224e-02,  1.9963e-02,  3.3544e-02,  2.6429e-02,  4.3978e-02,\n",
            "         7.0906e-02, -1.4172e-02,  3.1707e-02,  1.1556e-01,  2.2510e-02,\n",
            "         1.3776e-01,  3.4043e-03,  1.9217e-02,  9.7301e-02, -9.2280e-02,\n",
            "        -1.0608e-02,  5.2683e-02, -1.4630e-03, -2.5501e-02,  1.8408e-02,\n",
            "        -1.0153e-02, -5.9191e-02, -6.9154e-04, -5.0810e-02, -5.9029e-03,\n",
            "        -3.4107e-02, -8.1468e-02, -4.7834e-02,  5.1254e-02,  1.1035e-01,\n",
            "        -7.3325e-02, -2.8772e-02, -2.7043e-02, -1.8885e-02,  5.3347e-02,\n",
            "        -7.9410e-03, -7.5070e-02, -4.7684e-02,  5.5057e-02,  8.8665e-02,\n",
            "         7.7637e-02,  1.4622e-02,  7.4473e-03, -1.2827e-01,  2.7860e-02,\n",
            "         9.5059e-02,  6.5501e-03, -1.1090e-02, -5.4270e-02, -2.4547e-02,\n",
            "         7.8927e-03,  2.2875e-02,  3.0793e-03, -1.2587e-02, -7.1923e-02,\n",
            "        -3.4396e-02,  7.3324e-02, -8.3631e-02, -6.4532e-02,  8.5521e-02,\n",
            "         4.4709e-02, -1.5898e-02, -2.5457e-02,  1.0278e-01,  3.4183e-02,\n",
            "         6.7814e-03,  3.5959e-03,  7.0658e-02, -4.9623e-02, -3.2984e-03,\n",
            "         2.4855e-03,  4.9145e-02, -9.1689e-03,  1.4704e-02,  2.1152e-02,\n",
            "        -8.3013e-02, -7.5539e-02, -7.5281e-02, -5.7309e-01, -6.0819e-02,\n",
            "        -8.5979e-02, -1.9025e-02,  4.4833e-02, -2.6851e-02, -1.3231e-02,\n",
            "        -1.7288e-02, -3.0165e-02,  3.5889e-02, -5.5218e-02,  4.3945e-02,\n",
            "        -4.3719e-02,  9.3420e-03,  3.9414e-02,  6.3030e-02, -8.1831e-02,\n",
            "        -3.7091e-02,  3.6320e-02,  1.1276e-01, -1.1581e-02, -5.2862e-02,\n",
            "        -1.2598e-02, -3.1647e-02,  4.8075e-03,  1.4833e-02, -6.5028e-02,\n",
            "        -7.6895e-03, -7.3512e-02, -2.5486e-02, -4.1813e-02,  1.5875e-04,\n",
            "         2.7444e-02, -7.8871e-02,  6.8268e-02, -6.3687e-03,  2.2613e-02,\n",
            "        -3.2094e-02,  4.0743e-02,  2.3803e-02, -4.9665e-02, -3.8302e-03,\n",
            "         1.7203e-03, -6.6815e-02,  2.7893e-02,  2.8895e-02,  1.1447e-02,\n",
            "        -2.9330e-02, -2.4495e-02,  7.2686e-02, -6.2831e-02, -3.8198e-02,\n",
            "         5.3170e-02, -2.2470e-03,  1.3094e-02, -2.8768e-03, -5.7709e-02,\n",
            "         1.2637e-02,  3.5921e-02,  7.7178e-02, -1.2786e-02,  6.8424e-02,\n",
            "         4.2647e-02, -3.6230e-02,  6.6335e-02,  1.8345e-02,  5.5026e-02,\n",
            "        -1.6044e-02,  9.5089e-02, -5.7743e-02, -3.9953e-03, -5.6321e-02,\n",
            "         1.7654e-02, -1.1355e-01, -2.0111e-02, -1.2025e-02, -2.2342e-02,\n",
            "        -3.8111e-02, -1.8493e-02,  4.7788e-02, -1.1700e-01,  2.9329e-02,\n",
            "         1.0777e-01,  8.1691e-02,  1.0103e-02, -3.9286e-02,  4.3974e-02,\n",
            "        -1.0193e-01,  4.6519e-04,  3.4982e-02,  1.7681e-02, -2.2531e-02,\n",
            "         5.9354e-03, -4.4545e-02,  8.4809e-02,  1.2845e-02,  7.2413e-03,\n",
            "        -8.9966e-02, -2.0195e-02, -5.5660e-02,  5.1682e-03, -2.9537e-02,\n",
            "         3.0734e-02, -6.4153e-02, -8.2416e-02, -4.1658e-02,  3.5524e-02,\n",
            "         8.4056e-02,  2.6158e-02, -7.0579e-02,  5.0375e-02, -1.5945e-02,\n",
            "        -3.6871e-02, -5.4097e-02,  7.0046e-02, -5.9001e-02, -6.6906e-02,\n",
            "         2.8931e-02,  2.1156e-02,  1.4465e-02, -4.2688e-02,  3.0843e-02,\n",
            "        -9.5841e-02,  3.1049e-02, -1.7267e-02,  5.8211e-02, -1.1215e-02,\n",
            "         2.5011e-02, -5.2271e-02, -7.2524e-03,  4.1636e-02, -1.2922e-02,\n",
            "         3.3012e-02, -1.3943e-01, -2.3983e-02, -1.0884e-01, -4.5540e-02,\n",
            "         8.2418e-03, -1.1399e-02,  1.9262e-02,  3.5149e-02,  2.1226e-02,\n",
            "         4.1662e-02,  5.5880e-02,  5.5789e-02,  7.1931e-02,  1.1613e-02,\n",
            "        -3.0148e-02, -6.4731e-03,  1.6994e-02,  4.7024e-02, -1.6237e-02,\n",
            "         7.8670e-02, -3.5580e-02, -9.2719e-03, -8.0301e-02,  7.5849e-02,\n",
            "         1.6408e-03, -1.3350e-02, -1.3303e-02, -1.0529e-02, -6.1056e-02,\n",
            "        -4.6966e-02, -2.0982e-02, -1.9628e-02,  1.8064e-02, -5.4067e-02,\n",
            "        -4.4243e-02, -1.2639e-02, -3.5794e-03,  5.3594e-02, -6.1120e-02,\n",
            "        -3.5429e-02,  2.9079e-02, -1.0818e-02, -2.0240e-02, -4.4640e-02,\n",
            "         5.4709e-02,  5.5514e-02,  4.5176e-02,  5.2774e-02,  2.0449e-03,\n",
            "         2.4148e-02,  7.5699e-02,  9.8555e-03, -1.4599e-02, -3.2677e-02,\n",
            "        -1.8330e-02, -4.1715e-02,  3.6530e-02,  7.1768e-02,  2.3452e-03,\n",
            "         1.4440e-03,  9.1412e-02, -6.8613e-02, -1.9729e-02, -5.9021e-02,\n",
            "         2.3761e-03, -4.1591e-02, -1.1875e-02, -5.7171e-02, -3.9905e-02,\n",
            "        -1.8021e-01,  1.4446e-02, -2.0283e-02, -3.4604e-02,  1.0934e-01,\n",
            "         9.3487e-02, -1.6994e-02, -2.2198e-02, -7.0739e-02, -3.3051e-02,\n",
            "         3.6369e-03,  2.6813e-02,  3.7742e-02,  5.3021e-03, -2.6763e-02,\n",
            "        -1.9912e-02,  7.5408e-02, -1.4839e-01,  5.4851e-02, -2.8359e-02,\n",
            "        -8.4575e-02, -7.1482e-02, -3.5255e-02,  4.3687e-02, -7.3170e-03,\n",
            "        -1.0373e-01,  1.8857e-02, -1.6432e-03,  4.4577e-02, -1.0060e-01,\n",
            "         4.6177e-02,  5.7075e-02,  1.3574e-01, -3.1414e-02, -1.5419e-02,\n",
            "         6.4323e-02, -1.9411e-02,  9.2282e-02,  5.1881e-02, -2.1687e-02,\n",
            "        -5.2754e-03, -1.7284e-02, -7.6404e-02, -7.7567e-03,  3.9789e-03,\n",
            "        -6.3117e-02, -7.0600e-03,  8.3449e-02,  4.4994e-02, -6.5175e-04,\n",
            "        -1.2513e-02,  2.4039e-02, -1.7452e-02,  8.6102e-02,  9.3158e-03,\n",
            "         1.5504e-02,  1.6610e-02,  3.5816e-02,  5.1665e-02, -2.8690e-03,\n",
            "        -5.9123e-02,  1.6995e-02,  2.7976e-02,  9.5845e-02, -2.8796e-02,\n",
            "        -3.9649e-02,  2.7449e-02,  4.1449e-02,  2.4183e-02, -5.3613e-02,\n",
            "        -1.2956e-02,  7.5293e-02,  2.5709e-02, -4.0720e-02,  1.0458e-03,\n",
            "        -2.5551e-02,  2.9039e-02,  1.0784e-02,  1.2268e-01,  3.7313e-02,\n",
            "        -2.2168e-02,  8.9978e-02,  4.8200e-02, -1.3578e-02,  2.6823e-02,\n",
            "         1.4709e-02, -1.7293e-02,  7.1181e-02, -2.5049e-02,  6.0888e-02,\n",
            "        -1.3517e-02, -1.1441e-02, -4.5996e-02, -8.6260e-02,  5.1932e-02,\n",
            "         1.0900e-01, -5.6022e-02, -7.5737e-03, -1.9261e-02,  3.7798e-02,\n",
            "         2.1514e-02,  4.5055e-02,  3.1432e-02, -4.1821e-02,  5.2410e-02,\n",
            "        -4.6340e-02, -5.2976e-02, -1.7671e-02,  6.4157e-02, -1.4843e-01,\n",
            "         8.2264e-03,  4.8591e-03, -5.7576e-02,  1.2325e-02,  2.7476e-02,\n",
            "        -1.5675e-02, -3.9709e-03,  1.9007e-02,  9.8442e-03, -2.1851e-02,\n",
            "         9.1209e-03, -4.7204e-02,  6.9091e-02,  4.1224e-02, -5.7185e-03,\n",
            "        -2.4363e-02,  4.1596e-02,  6.0440e-02,  6.6958e-02, -1.5571e-02,\n",
            "        -6.2780e-02,  2.3440e-02, -6.9045e-02, -9.7530e-02,  8.4682e-04,\n",
            "        -1.2317e-01, -2.8132e-02,  1.5301e-02,  6.0423e-03,  4.2853e-02,\n",
            "         3.2683e-02,  3.7758e-03,  1.2861e-02,  3.9315e-02, -5.9100e-02,\n",
            "        -2.7530e-02, -1.8280e-02,  1.0072e-02,  3.5076e-02, -6.3968e-02,\n",
            "        -5.9066e-02, -1.2943e-01, -8.5690e-03, -3.7181e-02,  4.0224e-02,\n",
            "        -2.3463e-02,  2.0499e-02, -7.8721e-02,  1.6681e-02,  9.0797e-02,\n",
            "         9.1372e-02, -1.5501e-04, -2.4964e-02, -2.6653e-02,  7.8210e-02,\n",
            "         5.0348e-02,  6.5885e-03,  7.9732e-02, -5.8013e-02,  4.9909e-02,\n",
            "         5.5382e-02, -2.6498e-02,  5.4312e-02,  2.9646e-02, -1.8378e-02,\n",
            "        -1.3659e-02,  3.9050e-02,  3.0359e-02,  6.9679e-02, -2.7579e-02,\n",
            "        -5.6372e-02, -9.0979e-02, -8.0618e-03, -7.0255e-02,  1.0123e-02,\n",
            "         7.2885e-02, -5.6820e-02, -6.6698e-02,  9.6192e-02, -6.9482e-02,\n",
            "         6.3897e-02,  5.1526e-02, -6.7743e-02,  3.1714e-02,  5.6532e-02,\n",
            "         7.8241e-02, -1.0519e-02,  7.6105e-02,  1.2452e-01,  8.5340e-02,\n",
            "        -3.3119e-02, -1.7505e-02, -6.9945e-02,  6.2715e-02, -2.0161e-02,\n",
            "         8.1423e-03, -2.7961e-03,  6.6570e-02,  1.3823e-01, -4.0674e-02,\n",
            "        -3.4993e-02,  2.4117e-02,  1.5082e-02,  6.2326e-02,  7.5106e-02,\n",
            "         4.3886e-03, -6.8087e-02, -4.5146e-02, -9.0466e-04, -4.9335e-02,\n",
            "         1.3295e-03, -6.4656e-03,  2.4747e-02,  3.9950e-02,  6.8273e-02,\n",
            "        -3.3840e-02,  3.3392e-02,  9.5674e-02,  2.1115e-02, -8.5006e-03,\n",
            "         1.8209e-02, -4.2123e-02, -5.6538e-02, -2.2504e-02,  1.8652e-02,\n",
            "        -2.2379e-02, -3.0439e-02, -9.4725e-02, -8.1087e-02, -2.8831e-02,\n",
            "         2.8818e-02,  5.2690e-02, -1.8964e-01, -5.8527e-02, -9.2450e-02,\n",
            "         9.2485e-03, -4.2063e-02,  8.4913e-02,  2.1387e-02, -4.8710e-02,\n",
            "        -4.6535e-02,  1.5636e-02,  3.8541e-02, -5.6517e-05, -6.6900e-02,\n",
            "         5.6160e-02,  3.0651e-02,  4.5752e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.encoder.layer.11.output.LayerNorm.weight', Parameter containing:\n",
            "tensor([0.6384, 0.6300, 0.6657, 0.6127, 0.6376, 0.6371, 0.6570, 0.6331, 0.6264,\n",
            "        0.6274, 0.6279, 0.6530, 0.6055, 0.6508, 0.6297, 0.6216, 0.6509, 0.6025,\n",
            "        0.6437, 0.6300, 0.6269, 0.6288, 0.6218, 0.6324, 0.6286, 0.6211, 0.6202,\n",
            "        0.6334, 0.6525, 0.6213, 0.6393, 0.6333, 0.6316, 0.6377, 0.6364, 0.6289,\n",
            "        0.6342, 0.6112, 0.6209, 0.6371, 0.6302, 0.6346, 0.6272, 0.6218, 0.6665,\n",
            "        0.6390, 0.6836, 0.6314, 0.6834, 0.6381, 0.6403, 0.6628, 0.6905, 0.6079,\n",
            "        0.6166, 0.6367, 0.6341, 0.6005, 0.6061, 0.6538, 0.6669, 0.6125, 0.6143,\n",
            "        0.6157, 0.6322, 0.6397, 0.6320, 0.6501, 0.6304, 0.6317, 0.6289, 0.6107,\n",
            "        0.6380, 0.6405, 0.6143, 0.6522, 0.6047, 0.6076, 0.6453, 0.6347, 0.6345,\n",
            "        0.6315, 0.6294, 0.6553, 0.6169, 0.6251, 0.6510, 0.6221, 0.6401, 0.6853,\n",
            "        0.6214, 0.6307, 0.6428, 0.6185, 0.6171, 0.6550, 0.6437, 0.6276, 0.6232,\n",
            "        0.6212, 0.6536, 0.6362, 0.6159, 0.6311, 0.6680, 0.6267, 0.6330, 0.6212,\n",
            "        0.6301, 0.5318, 0.6436, 0.6386, 0.6399, 0.6273, 0.6604, 0.6951, 0.6258,\n",
            "        0.6358, 0.6286, 0.6141, 0.6586, 0.6308, 0.6431, 0.6485, 0.6303, 0.6178,\n",
            "        0.6410, 0.6356, 0.6644, 0.6729, 0.6194, 0.6183, 0.6338, 0.6260, 0.6120,\n",
            "        0.6670, 0.6350, 0.6337, 0.6329, 0.6186, 0.6261, 0.6323, 0.5352, 0.6262,\n",
            "        0.6402, 0.6480, 0.6291, 0.6221, 0.6568, 0.6198, 0.6312, 0.6352, 0.6188,\n",
            "        0.6464, 0.6209, 0.6688, 0.6608, 0.6192, 0.6645, 0.6005, 0.6289, 0.6210,\n",
            "        0.6187, 0.6294, 0.6319, 0.6159, 0.6647, 0.6350, 0.6330, 0.6304, 0.6271,\n",
            "        0.6352, 0.6252, 0.6280, 0.6691, 0.6531, 0.7134, 0.6341, 0.6153, 0.6190,\n",
            "        0.4560, 0.6633, 0.6192, 0.6314, 0.6253, 0.6157, 0.6236, 0.6331, 0.6351,\n",
            "        0.6228, 0.6346, 0.6620, 0.6803, 0.6246, 0.6366, 0.6230, 0.6173, 0.7476,\n",
            "        0.6356, 0.6460, 0.6094, 0.6203, 0.6758, 0.6076, 0.6733, 0.5315, 0.6364,\n",
            "        0.6397, 0.6674, 0.6287, 0.6353, 0.6318, 0.6663, 0.6376, 0.6200, 0.6303,\n",
            "        0.6224, 0.6362, 0.6235, 0.6183, 0.6421, 0.5906, 0.6244, 0.6254, 0.6406,\n",
            "        0.6402, 0.6670, 0.6475, 0.6141, 0.6419, 0.6289, 0.6276, 0.6012, 0.6322,\n",
            "        0.6502, 0.6098, 0.6163, 0.6439, 0.6441, 0.6198, 0.6096, 0.6438, 0.6417,\n",
            "        0.6207, 0.6221, 0.6314, 0.6346, 0.6424, 0.6505, 0.6197, 0.6626, 0.6280,\n",
            "        0.6274, 0.6147, 0.6576, 0.6521, 0.6229, 0.6307, 0.6542, 0.5951, 0.6398,\n",
            "        0.6278, 0.6261, 0.6218, 0.6329, 0.6784, 0.6857, 0.6377, 0.6338, 0.6250,\n",
            "        0.6533, 0.6379, 0.6497, 0.4800, 0.6291, 0.6451, 0.6843, 0.6271, 0.6006,\n",
            "        0.6481, 0.6145, 0.6409, 0.6265, 0.6621, 0.6242, 0.6424, 0.6176, 0.6414,\n",
            "        0.6487, 0.6233, 0.6266, 0.6764, 0.6153, 0.6282, 0.6213, 0.6218, 0.6077,\n",
            "        0.6760, 0.6448, 0.6194, 0.6133, 0.6627, 0.6620, 0.6123, 0.6447, 0.6276,\n",
            "        0.6074, 0.6355, 0.2733, 0.6234, 0.6448, 0.6281, 0.6408, 0.6209, 0.6348,\n",
            "        0.5948, 0.6265, 0.6288, 0.6523, 0.6324, 0.6185, 0.6138, 0.6167, 0.6319,\n",
            "        0.6588, 0.6490, 0.6170, 0.6269, 0.6300, 0.6341, 0.6205, 0.6115, 0.6197,\n",
            "        0.6779, 0.6323, 0.6168, 0.6396, 0.6188, 0.6363, 0.6000, 0.6344, 0.6188,\n",
            "        0.6309, 0.6176, 0.6522, 0.6386, 0.6353, 0.6329, 0.6167, 0.6441, 0.6369,\n",
            "        0.6345, 0.6275, 0.6363, 0.6353, 0.6486, 0.6287, 0.6268, 0.6261, 0.6385,\n",
            "        0.5896, 0.6288, 0.6216, 0.6141, 0.6393, 0.6453, 0.6181, 0.6181, 0.6344,\n",
            "        0.6263, 0.6077, 0.6241, 0.6399, 0.6224, 0.6326, 0.6354, 0.6133, 0.6352,\n",
            "        0.6321, 0.6392, 0.6411, 0.5365, 0.6179, 0.6134, 0.6166, 0.6293, 0.6133,\n",
            "        0.6500, 0.6387, 0.6225, 0.6784, 0.6560, 0.6243, 0.6406, 0.6288, 0.6380,\n",
            "        0.6308, 0.6276, 0.6754, 0.6355, 0.6390, 0.6337, 0.6624, 0.6925, 0.6277,\n",
            "        0.6035, 0.6296, 0.6262, 0.7089, 0.6599, 0.6311, 0.6430, 0.6382, 0.6258,\n",
            "        0.6512, 0.6338, 0.6338, 0.6218, 0.6266, 0.6353, 0.6361, 0.7031, 0.6023,\n",
            "        0.6456, 0.6385, 0.6299, 0.6390, 0.6515, 0.6246, 0.6396, 0.6430, 0.6355,\n",
            "        0.7782, 0.6255, 0.6211, 0.6263, 0.6885, 0.6084, 0.6313, 0.6296, 0.6192,\n",
            "        0.5809, 0.6468, 0.6258, 0.6383, 0.6193, 0.6281, 0.6406, 0.6237, 0.6258,\n",
            "        0.6268, 0.5954, 0.6637, 0.6102, 0.6271, 0.6270, 0.6300, 0.6493, 0.6270,\n",
            "        0.6391, 0.6346, 0.6552, 0.6556, 0.6211, 0.6292, 0.6225, 0.6489, 0.6339,\n",
            "        0.6224, 0.6588, 0.6057, 0.6201, 0.6219, 0.7086, 0.6325, 0.6385, 0.6228,\n",
            "        0.6193, 0.6111, 0.6333, 0.6416, 0.6281, 0.6128, 0.6520, 0.6238, 0.6236,\n",
            "        0.6303, 0.6492, 0.6139, 0.6293, 0.6339, 0.6194, 0.6279, 0.6479, 0.6316,\n",
            "        0.6276, 0.6436, 0.6213, 0.6427, 0.6065, 0.6271, 0.6237, 0.6408, 0.6397,\n",
            "        0.6151, 0.7299, 0.6305, 0.6353, 0.6369, 0.6307, 0.6365, 0.6394, 0.6501,\n",
            "        0.6402, 0.6174, 0.6086, 0.6521, 0.6170, 0.6230, 0.6358, 0.6497, 0.6148,\n",
            "        0.6285, 0.6630, 0.6186, 0.6260, 0.6180, 0.6201, 0.6336, 0.6081, 0.6540,\n",
            "        0.6130, 0.6145, 0.6939, 0.6317, 0.6149, 0.7039, 0.6160, 0.6272, 0.6087,\n",
            "        0.6282, 0.6575, 0.6640, 0.6269, 0.6428, 0.6156, 0.6506, 0.6301, 0.6189,\n",
            "        0.6474, 0.6188, 0.6614, 0.6475, 0.6764, 0.6282, 0.6363, 0.6255, 0.6093,\n",
            "        0.6269, 0.6226, 0.6362, 0.6387, 0.6603, 0.6926, 0.6305, 0.6326, 0.5934,\n",
            "        0.6091, 0.5922, 0.6331, 0.6759, 0.6226, 0.6200, 0.6283, 0.6323, 0.6834,\n",
            "        0.6318, 0.6485, 0.6731, 0.6307, 0.6407, 0.6428, 0.6478, 0.6101, 0.6382,\n",
            "        0.6248, 0.6563, 0.6270, 0.5176, 0.6143, 0.6567, 0.6277, 0.6234, 0.6273,\n",
            "        0.6230, 0.6163, 0.6253, 0.6246, 0.6219, 0.6437, 0.6163, 0.6394, 0.6223,\n",
            "        0.6381, 0.6711, 0.6193, 0.6341, 0.6202, 0.6306, 0.6855, 0.6169, 0.6608,\n",
            "        0.6413, 0.6073, 0.6099, 0.6886, 0.6545, 0.6303, 0.6289, 0.6170, 0.6146,\n",
            "        0.6222, 0.6569, 0.6391, 0.6498, 0.6231, 0.6311, 0.6149, 0.6380, 0.6652,\n",
            "        0.6179, 0.6393, 0.6344, 0.6307, 0.6375, 0.6570, 0.6364, 0.6536, 0.6366,\n",
            "        0.6939, 0.6344, 0.6406, 0.6145, 0.6104, 0.6540, 0.6684, 0.6228, 0.6354,\n",
            "        0.6247, 0.6329, 0.6069, 0.6222, 0.6401, 0.6184, 0.6320, 0.6253, 0.6561,\n",
            "        0.6311, 0.6098, 0.6296, 0.6345, 0.6655, 0.6368, 0.6417, 0.6204, 0.6167,\n",
            "        0.6496, 0.5092, 0.6322, 0.6467, 0.6442, 0.6169, 0.6217, 0.6171, 0.6479,\n",
            "        0.6395, 0.6242, 0.6299, 0.6305, 0.6158, 0.6503, 0.6288, 0.6161, 0.6538,\n",
            "        0.6713, 0.6220, 0.6196, 0.6965, 0.6285, 0.6369, 0.6248, 0.6180, 0.6288,\n",
            "        0.6421, 0.6326, 0.6461, 0.6799, 0.6320, 0.6065, 0.6553, 0.6395, 0.6489,\n",
            "        0.6506, 0.6355, 0.6255, 0.6291, 0.6445, 0.6394, 0.6475, 0.6315, 0.6116,\n",
            "        0.6204, 0.6243, 0.6288, 0.6222, 0.6340, 0.6689, 0.6636, 0.6397, 0.6240,\n",
            "        0.6251, 0.6379, 0.6349, 0.6577, 0.6313, 0.6256, 0.6191, 0.6826, 0.6339,\n",
            "        0.6188, 0.6248, 0.6788, 0.6242, 0.6443, 0.6252, 0.6199, 0.6232, 0.6255,\n",
            "        0.6361, 0.6255, 0.6459, 0.6494, 0.6014, 0.7048, 0.6307, 0.6436, 0.6340,\n",
            "        0.6423, 0.6538, 0.6481, 0.6516, 0.6327, 0.5313, 0.6531, 0.6167, 0.6196,\n",
            "        0.6541, 0.6369, 0.6306, 0.6473, 0.6102, 0.6082, 0.6307, 0.6290, 0.6272,\n",
            "        0.6281, 0.6218, 0.6398], device='cuda:0', requires_grad=True))\n",
            "('module.bert.encoder.layer.11.output.LayerNorm.bias', Parameter containing:\n",
            "tensor([-6.3461e-03, -1.9894e-02,  4.6285e-02,  1.5851e-02, -4.2569e-02,\n",
            "        -1.3283e-02, -9.1369e-03, -3.3534e-02,  2.5846e-02, -3.6451e-02,\n",
            "        -6.1155e-02,  3.0418e-02,  1.7200e-02,  1.2180e-02, -3.3043e-03,\n",
            "        -4.8502e-02,  5.4115e-04, -9.2394e-02,  1.0380e-02,  1.1926e-02,\n",
            "        -5.8754e-02, -6.8212e-03,  4.3755e-03, -4.5268e-02,  5.5810e-03,\n",
            "        -4.1087e-02, -4.8185e-02, -5.1641e-02, -9.1535e-02, -4.0457e-02,\n",
            "        -2.7150e-02, -4.6665e-02, -1.1590e-02, -7.6277e-02, -4.2056e-02,\n",
            "        -5.7515e-02, -2.7733e-02, -7.7364e-02,  6.9931e-03, -5.9466e-02,\n",
            "         5.3589e-02, -2.0961e-02, -4.5835e-02, -6.9220e-02, -1.0842e-01,\n",
            "        -1.9135e-02, -1.3539e-01, -1.7333e-03, -1.0963e-01, -1.1798e-01,\n",
            "        -2.1245e-02,  4.4968e-02,  3.4347e-02, -2.5082e-02, -8.8453e-02,\n",
            "        -8.9132e-03, -2.2133e-03, -6.2073e-02, -5.2548e-04, -6.9640e-02,\n",
            "         4.3890e-02, -8.3953e-02,  2.0464e-02,  5.0537e-02, -4.3895e-02,\n",
            "        -4.7470e-02, -9.3629e-04, -5.9144e-02,  3.3087e-02,  9.0344e-04,\n",
            "         2.1959e-02, -2.3260e-02, -6.6600e-02, -5.4183e-02, -1.2760e-02,\n",
            "        -8.5299e-02,  4.8837e-02, -4.9116e-02,  4.0889e-02,  3.1605e-02,\n",
            "         3.4653e-02, -3.4898e-02, -5.1033e-02,  1.9560e-02, -5.7742e-02,\n",
            "        -3.7249e-02,  2.8692e-02, -1.3316e-02, -4.7234e-02,  2.9975e-02,\n",
            "         1.4817e-02, -7.7454e-02,  5.3532e-02, -1.5101e-02, -4.1975e-02,\n",
            "        -1.1653e-01, -4.0040e-02, -1.4511e-02,  2.9632e-02, -9.7741e-03,\n",
            "        -8.1774e-02, -7.5635e-02, -9.0471e-02, -2.8863e-02, -1.0929e-01,\n",
            "        -1.9059e-02, -2.8311e-02,  4.8349e-02, -5.1201e-02, -2.3034e-02,\n",
            "        -3.5799e-02, -3.8083e-02,  1.2375e-02,  2.0018e-02, -2.0495e-02,\n",
            "         3.5751e-02, -2.9506e-03,  3.5649e-02, -2.2652e-03, -7.3649e-03,\n",
            "         3.4992e-02,  1.1655e-01, -3.2204e-03, -3.2992e-02,  1.1133e-02,\n",
            "         5.0059e-02,  1.0109e-02, -3.6496e-03, -3.4484e-02, -1.2807e-01,\n",
            "        -6.3418e-03, -1.3855e-03, -6.3614e-03, -3.4011e-02,  6.4414e-02,\n",
            "        -1.1152e-01, -4.5309e-02, -2.4458e-03,  3.8958e-02, -3.5377e-02,\n",
            "        -1.1892e-02, -4.7690e-03,  3.9924e-02, -2.8776e-02, -1.5479e-02,\n",
            "        -8.6420e-02, -3.7144e-02,  2.9788e-02, -6.6552e-02,  3.1951e-02,\n",
            "         5.6070e-03,  5.7415e-02,  7.2993e-02, -7.0931e-02,  1.5957e-02,\n",
            "         8.3283e-02, -1.4200e-02, -7.0544e-02, -1.1789e-01, -1.1684e-02,\n",
            "         2.3768e-02,  5.1856e-02, -1.8541e-02,  5.1044e-02, -6.6014e-02,\n",
            "        -7.0924e-02,  3.6743e-03, -9.4640e-02, -5.0436e-02, -5.2546e-02,\n",
            "        -2.4143e-02, -1.7134e-02,  1.8747e-02, -5.1428e-03, -7.9001e-02,\n",
            "         5.7671e-02,  9.3698e-02, -3.3379e-02, -5.0376e-02, -7.9511e-02,\n",
            "         1.3052e-01,  1.4976e-02, -1.7266e-02, -3.8186e-02, -1.4686e-02,\n",
            "        -6.0080e-02, -6.1968e-02,  6.4466e-03,  3.3904e-02,  9.1657e-03,\n",
            "        -3.3732e-02, -1.1071e-01, -1.0519e-02, -6.0141e-04,  4.4024e-02,\n",
            "        -5.1106e-02,  1.0253e-02,  1.2218e-01, -7.0053e-03,  2.8158e-02,\n",
            "        -4.4821e-02, -2.7442e-02,  3.9209e-02,  5.1164e-02, -8.5783e-02,\n",
            "         4.3804e-02,  4.2881e-02, -6.2721e-02,  4.4046e-02, -6.1240e-02,\n",
            "        -1.3413e-02, -6.9396e-02,  4.2974e-02, -8.8764e-02, -4.5121e-03,\n",
            "        -2.9662e-02,  5.5994e-02, -6.8597e-02, -4.9121e-02,  4.4877e-03,\n",
            "         3.2838e-02, -6.2602e-02, -1.9148e-02,  5.5441e-03, -7.5632e-02,\n",
            "        -1.1891e-01,  3.6709e-02, -1.0153e-01, -3.3007e-02,  1.2996e-03,\n",
            "        -5.4093e-02, -4.5864e-02, -1.0291e-01,  1.7535e-03,  6.1727e-02,\n",
            "        -2.1266e-02, -8.1250e-03, -3.6417e-02,  4.2391e-02,  3.1332e-02,\n",
            "        -4.2671e-02, -1.9608e-03, -4.9377e-02,  1.9157e-02, -3.1282e-03,\n",
            "        -3.2789e-02,  4.4702e-02, -7.5857e-02, -7.6272e-02, -4.3770e-02,\n",
            "        -5.8095e-02, -9.6655e-02, -2.2400e-02, -2.8436e-02,  7.7737e-02,\n",
            "        -8.0331e-02, -4.7472e-02,  7.3887e-03,  2.6763e-02, -5.7786e-02,\n",
            "        -6.0987e-02, -3.2469e-02, -3.1629e-02, -1.2727e-01, -5.1392e-02,\n",
            "        -6.6688e-02, -1.9219e-01, -5.1794e-02,  2.4553e-03,  2.9904e-02,\n",
            "         8.0934e-02, -3.0436e-02,  1.5577e-02, -7.2660e-02, -3.3336e-02,\n",
            "        -5.3065e-02,  1.1671e-01,  1.0026e-04,  1.9790e-03, -6.5624e-02,\n",
            "        -4.3014e-02, -1.0482e-02,  4.8351e-02, -8.9676e-02,  6.5467e-02,\n",
            "        -2.4850e-02,  1.4410e-02, -9.7899e-02, -1.1841e-01,  1.1648e-02,\n",
            "         1.5642e-02,  4.3354e-02, -2.9421e-02, -5.1757e-02, -1.1092e-02,\n",
            "        -3.3445e-02, -2.2702e-02,  8.9328e-02, -6.5749e-02,  1.3442e-02,\n",
            "         3.7518e-02,  3.1988e-02, -9.5302e-02, -4.4028e-02, -1.5638e-02,\n",
            "        -3.1500e-02, -1.3504e-02,  2.7924e-03,  2.2506e-01, -5.3851e-02,\n",
            "        -2.0857e-02,  4.1186e-03, -2.5374e-02, -2.3853e-02,  6.7087e-02,\n",
            "        -1.5815e-02, -3.7528e-03,  3.4893e-02, -2.6880e-02,  4.4248e-02,\n",
            "        -3.1470e-02, -3.4574e-02, -9.8893e-03,  6.6473e-02, -7.7952e-02,\n",
            "        -3.0651e-02, -4.4723e-02,  1.6922e-02, -8.1135e-02, -3.1212e-02,\n",
            "        -2.7336e-02, -4.2488e-02, -9.1422e-02,  6.7794e-02, -3.8465e-02,\n",
            "        -8.5077e-04, -4.9080e-02, -1.6648e-02, -5.0307e-02,  7.3965e-02,\n",
            "        -2.5469e-02, -1.5340e-02, -1.3841e-02, -5.3395e-02,  3.1235e-02,\n",
            "        -4.4601e-02, -3.0874e-02,  1.3887e-02, -1.5264e-02, -4.4242e-02,\n",
            "        -8.1788e-02,  3.6415e-02, -1.0788e-01, -6.9351e-02, -2.4133e-02,\n",
            "        -1.9062e-02,  3.4992e-02, -6.9762e-02, -5.6817e-02, -1.0014e-02,\n",
            "        -5.2109e-02,  6.1807e-02, -9.6213e-02, -3.5930e-02, -9.8491e-02,\n",
            "        -8.2680e-03,  4.1227e-03, -9.1548e-02, -3.5262e-02, -7.2005e-03,\n",
            "        -3.6008e-02, -3.1894e-02, -9.3911e-02,  1.2452e-02,  3.7334e-02,\n",
            "        -3.6833e-02,  1.2549e-02,  6.3336e-02, -2.8293e-02, -6.2196e-02,\n",
            "        -5.1179e-02, -2.1169e-02, -2.2330e-02,  1.7721e-02, -5.7899e-02,\n",
            "        -1.0315e-01, -8.9405e-03, -5.2078e-03, -5.5555e-02,  1.6670e-02,\n",
            "         1.2780e-01,  5.6130e-02,  6.1353e-03, -5.0181e-02,  5.3761e-02,\n",
            "        -1.5701e-03, -2.8827e-02,  5.8172e-02,  2.3896e-02, -7.4967e-02,\n",
            "        -5.9917e-02, -1.2450e-02,  2.4290e-02,  1.0785e-01, -1.6169e-02,\n",
            "        -6.5232e-02,  2.7714e-02, -6.4520e-02,  8.3731e-02, -4.5130e-02,\n",
            "        -1.9344e-03, -4.9180e-02, -2.6595e-02, -2.3764e-02, -5.1821e-02,\n",
            "        -5.2828e-02,  5.9051e-03, -3.1387e-02,  1.5917e-02, -6.3710e-03,\n",
            "        -9.6739e-03, -1.7171e-01, -1.5133e-02, -3.4919e-02, -6.6834e-02,\n",
            "        -2.9504e-02,  7.5407e-03, -5.1922e-03, -7.4846e-02, -1.5151e-02,\n",
            "        -7.9631e-02, -2.6341e-02, -1.5012e-01,  2.2460e-02,  1.0107e-02,\n",
            "        -2.5778e-02, -2.6850e-05, -2.0127e-02, -3.9769e-02, -1.5528e-02,\n",
            "        -2.5348e-02, -6.8829e-02,  3.4717e-02, -2.0053e-03, -5.0096e-02,\n",
            "         5.8816e-02,  4.7402e-02, -1.0247e-01, -2.6462e-02, -4.4910e-02,\n",
            "        -2.1627e-02, -4.0818e-02, -1.4128e-02, -9.3792e-04, -6.7596e-02,\n",
            "        -6.2011e-02, -4.6467e-02, -6.8064e-03,  7.3054e-02, -4.7341e-02,\n",
            "        -1.4687e-02, -3.7106e-02, -1.4129e-01, -9.2913e-02, -1.1632e-02,\n",
            "        -5.4049e-02, -5.8600e-02,  5.4550e-02, -3.9708e-02, -9.0867e-02,\n",
            "        -5.5624e-02, -2.0226e-02, -1.3310e-02,  4.2342e-02, -2.7498e-02,\n",
            "        -1.5261e-02, -5.2250e-02, -7.4362e-02,  5.7417e-02, -6.4251e-02,\n",
            "        -5.5464e-02,  5.9414e-03,  1.3054e-02,  3.2055e-02,  9.2786e-03,\n",
            "         9.6044e-04,  9.5211e-02,  3.8422e-02, -1.6226e-03, -2.5338e-02,\n",
            "        -6.2656e-03, -8.6946e-03, -4.5840e-02, -5.9405e-02, -2.9124e-02,\n",
            "         1.0008e-02, -4.2456e-02,  6.5832e-02,  4.8613e-02, -4.0501e-02,\n",
            "        -4.6531e-02, -1.5976e-02, -4.6823e-02, -1.7073e-02, -1.7184e-04,\n",
            "         9.7579e-02,  1.2385e-02, -1.8580e-02, -2.3771e-02,  5.3708e-03,\n",
            "        -6.2866e-02, -6.6282e-02,  3.2546e-02, -5.4184e-02,  9.1790e-02,\n",
            "        -1.0412e-01, -9.0681e-02,  5.6900e-03, -3.8840e-02, -7.4245e-02,\n",
            "        -4.6258e-02, -1.0141e-02,  2.7948e-03, -9.8402e-02, -5.7454e-02,\n",
            "         8.9371e-03, -4.7653e-02, -3.5983e-05,  3.6370e-02, -6.8060e-03,\n",
            "        -1.2731e-01, -2.8675e-02,  2.9252e-02,  7.4892e-02, -1.2447e-02,\n",
            "        -2.8557e-02,  1.0087e-01,  1.0119e-02,  3.5378e-02,  4.1354e-02,\n",
            "        -3.4999e-02,  6.8073e-02, -8.3200e-02,  1.2770e-03, -5.2315e-02,\n",
            "        -7.4785e-03, -6.4946e-02,  1.3443e-02, -3.5037e-02,  2.9521e-03,\n",
            "        -3.4377e-02, -6.3584e-02, -5.1472e-02,  2.8725e-02, -7.8538e-03,\n",
            "        -1.0991e-03,  1.7426e-02, -6.5711e-02, -1.6871e-02,  2.3383e-02,\n",
            "         2.2133e-02, -4.0960e-02,  1.2284e-02, -8.1919e-02, -2.2731e-02,\n",
            "        -4.7884e-02, -4.9796e-02, -4.8432e-02, -3.7191e-02,  8.0479e-03,\n",
            "        -4.7701e-02,  1.8650e-02, -1.0814e-01,  7.4144e-02, -3.5186e-02,\n",
            "        -1.3576e-01, -6.6604e-02,  3.8105e-02,  1.3714e-02,  7.7252e-03,\n",
            "         5.0489e-02,  4.1594e-02,  5.2363e-02, -5.8934e-02, -2.5082e-02,\n",
            "         1.9394e-02, -6.3943e-03, -3.7295e-03,  3.8858e-02,  1.0877e-02,\n",
            "        -7.3624e-02, -4.5368e-02, -4.7722e-02, -1.0537e-02,  1.5779e-02,\n",
            "        -9.4885e-02,  1.1391e-02,  4.5534e-02, -2.0946e-02,  2.2194e-02,\n",
            "        -2.3659e-02, -2.0593e-02, -9.2396e-02, -5.1322e-02,  3.7030e-02,\n",
            "        -2.6163e-02, -5.7035e-03, -1.9519e-02, -7.4590e-03,  5.5139e-02,\n",
            "        -5.2928e-02, -9.3105e-02, -9.0368e-02, -5.1391e-02,  4.1980e-03,\n",
            "        -1.1540e-01, -2.4661e-02, -1.2784e-02,  3.7718e-02,  9.8530e-03,\n",
            "        -5.9503e-02, -1.6615e-02, -1.0033e-01, -1.2805e-01,  3.3951e-02,\n",
            "        -6.6903e-02, -1.0696e-01, -2.2511e-02,  1.0136e-02, -8.9295e-02,\n",
            "        -5.8106e-02, -1.8917e-02,  3.0799e-02, -8.4209e-02, -4.7490e-02,\n",
            "         3.4659e-02, -2.3828e-02,  1.0109e-02, -2.2014e-02,  1.3713e-01,\n",
            "        -8.5138e-02,  4.2718e-03,  3.1069e-03,  2.6038e-02, -1.7890e-02,\n",
            "        -3.1987e-02, -7.9456e-02, -2.1475e-02, -9.4370e-02,  8.7895e-03,\n",
            "        -2.8219e-02, -4.0465e-02, -1.3446e-02, -5.4235e-02,  3.6914e-02,\n",
            "        -5.9405e-02, -4.1999e-02,  7.6936e-03, -2.3912e-02,  2.7201e-02,\n",
            "        -8.1045e-02, -6.2674e-02,  6.0316e-03, -8.3956e-03, -9.7366e-02,\n",
            "        -2.8905e-02, -6.4466e-02, -4.1084e-02, -1.5809e-02, -3.3628e-02,\n",
            "        -7.5521e-04,  1.1060e-02, -3.0861e-02, -2.7453e-02, -2.2560e-02,\n",
            "        -4.0385e-02,  1.1128e-02,  3.2616e-02,  1.9923e-02, -5.2315e-02,\n",
            "        -6.3481e-02,  2.8915e-02, -4.1004e-02,  2.9585e-02,  7.3443e-02,\n",
            "        -3.7008e-02, -1.3618e-02,  7.0967e-02,  6.3854e-02, -3.6539e-02,\n",
            "         5.6841e-03, -2.3552e-02, -2.3075e-02, -1.0181e-01, -3.1658e-02,\n",
            "         7.5154e-02, -6.8950e-02,  1.7453e-02, -4.2651e-02, -8.9301e-02,\n",
            "        -1.1395e-01, -5.5166e-02, -6.5374e-02, -1.0342e-01, -1.0614e-01,\n",
            "        -2.7443e-03, -2.7233e-02, -7.5034e-03,  1.6079e-02, -3.1732e-02,\n",
            "         2.1911e-02, -3.8058e-02,  1.2087e-02,  1.9566e-02, -2.1590e-02,\n",
            "        -1.8401e-02, -5.6781e-02,  6.9947e-02,  3.7185e-02, -6.0096e-03,\n",
            "        -5.5889e-02, -7.1291e-04, -2.1571e-02, -1.1587e-01, -1.2091e-02,\n",
            "         1.6871e-02,  2.5919e-02, -1.1262e-01, -9.1336e-02, -4.0920e-02,\n",
            "        -8.1354e-03,  6.3578e-02, -1.0269e-02,  6.2887e-02, -7.6906e-02,\n",
            "        -2.2190e-02, -3.0619e-04, -1.7264e-02,  1.4001e-02, -6.2175e-02,\n",
            "        -3.0614e-02, -7.7598e-03,  7.8681e-02, -1.1139e-01, -9.4248e-02,\n",
            "        -5.5925e-02,  1.9979e-02, -4.3891e-02, -5.6174e-02, -7.6827e-02,\n",
            "         5.8949e-02,  2.0870e-02, -6.9111e-02, -7.9080e-02, -2.0177e-03,\n",
            "        -2.2291e-02, -1.0346e-01,  2.1730e-02,  7.4742e-02, -3.9678e-02,\n",
            "        -7.2509e-02, -1.7938e-02,  8.0589e-03, -4.5611e-02, -8.8949e-03,\n",
            "         1.6488e-02,  3.7573e-02, -1.7079e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.bert.pooler.dense.weight', Parameter containing:\n",
            "tensor([[-0.0013, -0.0381, -0.0158,  ...,  0.0244, -0.0008,  0.0240],\n",
            "        [ 0.0020,  0.0151,  0.0033,  ...,  0.0180, -0.0023,  0.0231],\n",
            "        [-0.0386,  0.0145,  0.0621,  ...,  0.0374, -0.0105, -0.0395],\n",
            "        ...,\n",
            "        [-0.0111,  0.0136,  0.0541,  ...,  0.0666,  0.0017, -0.0090],\n",
            "        [ 0.0001,  0.0024, -0.0125,  ...,  0.0046, -0.0014, -0.0079],\n",
            "        [ 0.0415,  0.0751,  0.0305,  ...,  0.0317,  0.0479,  0.0080]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.bert.pooler.dense.bias', Parameter containing:\n",
            "tensor([-3.5976e-02, -3.8954e-03,  5.1814e-02,  2.2247e-02, -4.9372e-03,\n",
            "        -1.1203e-03,  2.6339e-02,  9.0639e-03,  3.8723e-02, -9.6557e-02,\n",
            "         2.3311e-02, -2.0086e-02,  6.5490e-02, -5.1680e-02,  6.6156e-02,\n",
            "        -1.4721e-02, -4.5325e-03, -8.4059e-04,  1.3027e-02, -2.9770e-02,\n",
            "         4.3693e-02, -1.4292e-02,  4.5489e-02,  4.5346e-03,  7.6859e-03,\n",
            "        -4.0926e-02, -1.1412e-02,  6.6037e-02,  6.7376e-02,  4.2859e-02,\n",
            "        -2.6975e-02,  3.5281e-05, -8.1483e-02, -3.7569e-03,  4.0570e-02,\n",
            "        -5.7694e-02, -8.3285e-03, -2.6218e-02, -4.7924e-03, -8.2249e-03,\n",
            "        -6.4672e-02,  1.9579e-03,  7.2015e-02, -2.2552e-02,  3.4110e-03,\n",
            "        -1.4251e-02, -8.3488e-03,  3.8957e-03, -5.5111e-02, -4.8916e-02,\n",
            "        -4.0505e-02, -3.5423e-02,  6.1806e-03,  9.7887e-04,  2.5234e-03,\n",
            "         3.8691e-03, -7.2486e-03, -2.1082e-03,  1.9743e-03, -1.5894e-02,\n",
            "        -1.4470e-02,  3.2000e-03,  2.3056e-02, -4.6876e-02, -3.6722e-02,\n",
            "        -7.3898e-02, -4.4587e-03,  5.0023e-03,  6.9043e-03,  3.3589e-03,\n",
            "         2.2775e-02, -3.1106e-03,  2.4767e-02, -4.8293e-02, -5.0632e-02,\n",
            "         3.7559e-03, -8.8531e-03,  1.7421e-02, -5.2642e-03, -7.4719e-02,\n",
            "        -3.0047e-02, -5.3653e-02,  8.8173e-03,  6.1905e-02, -6.8883e-02,\n",
            "        -2.1225e-02, -8.6340e-03, -1.7548e-03, -8.5851e-02,  5.9378e-03,\n",
            "         1.1967e-02,  2.0897e-03, -4.2413e-02,  8.7467e-03,  4.5203e-02,\n",
            "        -7.6098e-03,  1.8549e-04,  2.8614e-02,  2.0781e-03, -4.9508e-03,\n",
            "        -8.8196e-03, -3.4198e-03,  7.3986e-03, -1.7159e-03,  3.8593e-03,\n",
            "        -9.6255e-03, -1.3085e-02, -1.6207e-02, -4.1687e-02,  9.7658e-03,\n",
            "         8.9834e-03, -5.8610e-03, -1.4935e-03, -5.3537e-02,  5.5583e-03,\n",
            "        -1.0870e-04, -8.4393e-02, -2.0534e-02, -8.6620e-02,  3.6078e-02,\n",
            "         5.2120e-03, -1.4668e-02,  6.6004e-02,  3.5172e-02,  4.0267e-03,\n",
            "        -3.8512e-03,  4.1188e-02, -9.5374e-03,  8.4221e-03,  4.1511e-03,\n",
            "         3.0034e-02, -6.3468e-04, -7.1196e-02, -7.6416e-02,  1.1497e-02,\n",
            "         6.4726e-02,  2.7181e-03,  6.8666e-02, -4.0454e-03,  5.6817e-02,\n",
            "         4.0652e-02,  1.6854e-02, -3.1005e-02, -1.3064e-02,  2.8483e-03,\n",
            "         1.6498e-02, -3.0347e-02,  8.9853e-03, -7.7755e-03, -1.8113e-02,\n",
            "        -2.2885e-03, -5.0442e-03,  4.7131e-02, -6.0013e-02, -8.7461e-03,\n",
            "         5.9925e-02,  2.1879e-02,  6.8031e-02,  6.3506e-02, -4.2697e-03,\n",
            "        -1.4504e-02,  4.1974e-02,  9.1194e-03,  3.0824e-03,  2.6392e-02,\n",
            "         1.1135e-04, -6.1943e-02,  5.2578e-03, -3.5484e-02,  1.7566e-02,\n",
            "         3.5716e-03, -4.5841e-03,  4.8374e-02, -7.5280e-02, -4.6712e-03,\n",
            "         1.6970e-02,  8.2794e-02,  4.9445e-02,  4.4020e-03, -2.7126e-02,\n",
            "        -1.0707e-02, -2.4882e-02, -6.9837e-02,  7.4775e-02,  6.0947e-03,\n",
            "         1.8029e-03,  6.6490e-03, -1.5068e-02, -4.7108e-02, -4.8279e-02,\n",
            "         2.2051e-02,  1.0339e-02, -4.7205e-02, -4.5817e-03, -9.0287e-03,\n",
            "        -2.7196e-03,  3.0730e-03,  3.3383e-02,  5.3510e-03, -1.1000e-02,\n",
            "        -4.3553e-03,  6.9025e-02,  2.8795e-02,  3.3619e-02, -1.4701e-02,\n",
            "         1.4060e-02, -4.8320e-02, -3.1883e-02, -1.4480e-03,  6.0081e-03,\n",
            "         2.3593e-03,  8.4787e-02,  1.2406e-02,  2.5789e-03, -5.0564e-02,\n",
            "        -8.2032e-02, -9.4141e-04, -5.5241e-02, -7.6775e-03, -2.4861e-02,\n",
            "         9.4571e-03,  5.8841e-03, -4.7154e-02,  3.5717e-03, -4.3507e-02,\n",
            "        -4.6676e-02, -1.2599e-03, -8.0774e-03,  2.4329e-03, -7.5354e-03,\n",
            "         1.5403e-02, -3.6249e-02, -2.7785e-02,  2.7119e-02,  6.7980e-02,\n",
            "         3.1430e-02, -5.2584e-02,  3.7485e-02, -4.8816e-03,  3.0093e-02,\n",
            "        -1.1456e-02,  5.6422e-02, -1.2345e-02,  6.0983e-03, -6.2267e-02,\n",
            "         4.3605e-02, -3.8197e-02,  5.2499e-02,  1.6630e-04, -7.5969e-02,\n",
            "        -6.4865e-02,  1.3884e-02, -1.3189e-03,  6.7656e-02, -9.5227e-03,\n",
            "         3.3197e-02, -2.3533e-02, -7.0145e-02, -1.0432e-02,  2.8274e-02,\n",
            "        -8.1062e-02, -3.1947e-02, -2.7126e-03, -1.3334e-02, -2.8135e-03,\n",
            "        -1.0706e-02, -6.9299e-02,  3.4644e-02, -2.9170e-03,  4.5919e-02,\n",
            "        -8.9445e-03, -2.6880e-02,  1.2639e-02, -6.7561e-02, -7.4783e-04,\n",
            "         6.1452e-03,  5.0698e-02, -8.5238e-03, -5.8640e-02,  1.7157e-03,\n",
            "         2.5632e-02,  1.0620e-02,  5.4907e-02,  3.9250e-02,  1.3627e-02,\n",
            "         7.0662e-02,  5.1439e-02,  3.4894e-02,  1.0701e-02, -1.6566e-02,\n",
            "         1.0428e-01,  3.2651e-02, -1.1483e-02, -6.2328e-02, -1.2810e-02,\n",
            "         3.5336e-02, -1.5002e-02, -1.1252e-02,  1.3703e-04, -5.9601e-02,\n",
            "        -4.1940e-02,  6.4488e-02,  3.5795e-02, -1.1073e-02,  4.2960e-02,\n",
            "         6.6363e-02, -2.1350e-02, -2.8048e-02, -7.8367e-03,  5.9328e-02,\n",
            "        -1.1968e-02,  1.9762e-02,  1.6436e-03, -1.9586e-04,  4.5457e-02,\n",
            "        -3.3673e-02,  2.8175e-02,  3.0951e-02, -8.4503e-03,  4.3550e-03,\n",
            "        -1.8311e-02, -6.0869e-02,  7.6723e-03, -8.1489e-03, -2.4181e-02,\n",
            "        -6.5763e-02, -3.1222e-03, -2.7059e-02,  5.2479e-03,  2.2024e-03,\n",
            "         2.2695e-03, -3.7777e-02,  2.9974e-05, -5.2179e-02,  8.6420e-03,\n",
            "         1.5367e-02, -2.6896e-02, -2.3252e-02,  1.3346e-02, -2.8418e-02,\n",
            "         5.2154e-02, -6.8118e-02,  6.3350e-02, -2.9627e-03, -3.7981e-02,\n",
            "         2.4390e-02, -2.4529e-03, -5.2262e-02,  4.4196e-03,  4.0819e-03,\n",
            "        -5.2573e-04,  1.5752e-02, -4.7064e-03, -7.4060e-02, -1.7793e-02,\n",
            "        -1.8469e-02,  4.7606e-03, -4.6088e-03,  7.6716e-02, -1.5433e-04,\n",
            "         3.2812e-02,  9.1863e-03,  7.8530e-02, -7.5982e-02, -3.3673e-02,\n",
            "        -4.5526e-02, -7.1196e-02,  6.6536e-02,  6.6131e-02,  3.4918e-03,\n",
            "        -4.2715e-02, -7.3834e-04,  5.9887e-02, -9.4803e-03, -5.1243e-02,\n",
            "         4.2864e-03,  2.6319e-02, -9.7941e-03,  5.9664e-02, -2.4174e-02,\n",
            "        -1.1204e-02, -7.2384e-03,  4.9288e-03,  1.6533e-02, -2.4369e-02,\n",
            "         6.4933e-03, -1.3649e-02, -2.4182e-03, -6.2799e-03, -1.7210e-02,\n",
            "        -7.2431e-02, -7.8740e-03,  1.2792e-02,  2.0920e-02, -1.2329e-02,\n",
            "         3.8972e-02, -7.1581e-03, -1.2284e-02,  1.9917e-03,  7.4051e-03,\n",
            "        -1.4321e-02, -3.5394e-02, -2.4015e-02, -3.2418e-02, -7.5521e-02,\n",
            "         3.2948e-02,  1.5901e-03, -2.8555e-03,  2.7893e-03, -3.4312e-03,\n",
            "         2.5345e-03, -1.6758e-02, -4.9462e-02,  3.2866e-03,  1.2931e-02,\n",
            "        -5.2172e-02,  7.3928e-02, -4.0985e-03,  9.5864e-03,  3.0872e-02,\n",
            "         5.2427e-02, -1.4434e-02, -2.9415e-03, -4.2015e-03, -6.0903e-02,\n",
            "         5.1717e-03, -6.3032e-02,  6.9126e-02, -3.7929e-02,  7.9866e-03,\n",
            "         6.0435e-03, -8.5405e-03,  1.0294e-02, -9.4331e-03,  2.0938e-02,\n",
            "        -8.2480e-03,  3.0896e-02,  1.7897e-02, -2.8527e-02, -1.5033e-02,\n",
            "         3.5082e-03,  5.9243e-02, -3.4988e-03, -1.2596e-03, -6.2648e-02,\n",
            "        -5.5414e-02, -1.9697e-02, -4.5624e-02, -7.3769e-02,  3.9488e-02,\n",
            "         2.1077e-02,  7.9621e-03,  8.1903e-03, -6.0914e-03, -1.0476e-02,\n",
            "        -2.5105e-02, -4.3828e-03, -6.4321e-02,  4.9943e-02, -1.9390e-02,\n",
            "         1.1147e-02, -4.4764e-03,  6.2568e-03, -6.4056e-02,  7.2374e-02,\n",
            "         1.0556e-02,  1.0156e-03,  7.9476e-03, -2.9068e-02,  1.5874e-02,\n",
            "        -2.6810e-02,  2.9566e-02, -5.4999e-03,  1.3515e-02, -1.0681e-02,\n",
            "        -5.0631e-02,  4.0818e-02,  2.0823e-02,  3.9909e-03,  7.8542e-03,\n",
            "        -4.6300e-02,  2.4748e-03,  6.9978e-02,  4.5245e-02, -1.6835e-02,\n",
            "        -1.0396e-02,  1.5637e-02, -5.6454e-03, -7.2900e-02,  5.3020e-02,\n",
            "         1.3894e-02, -1.8712e-05, -3.6843e-04,  5.5773e-03,  4.1098e-02,\n",
            "        -8.0191e-04, -2.4093e-03, -2.5919e-02,  1.2519e-03, -2.3039e-03,\n",
            "        -8.6203e-03,  1.2129e-02,  1.0854e-04, -1.5088e-03, -7.4610e-02,\n",
            "         2.2519e-02, -3.8466e-02,  1.0974e-02,  6.0302e-02, -5.1620e-02,\n",
            "         1.3340e-02, -8.0559e-03,  4.1242e-04,  1.9552e-02, -7.9269e-03,\n",
            "        -8.1603e-03,  1.7111e-03,  6.7728e-03,  6.4310e-02, -1.3457e-02,\n",
            "        -7.4335e-02, -3.4480e-02, -7.3911e-04, -6.2072e-02, -2.2311e-02,\n",
            "        -1.7109e-03,  5.9918e-03, -1.9664e-03, -7.1251e-03,  2.2825e-02,\n",
            "        -4.8720e-03, -6.5003e-02, -2.1775e-03, -1.8444e-03,  7.3265e-02,\n",
            "         5.2221e-04, -1.2954e-02, -5.3259e-02, -3.7084e-02, -3.8334e-03,\n",
            "         4.3280e-02, -6.0462e-02,  7.0455e-02, -6.5359e-02,  1.0842e-03,\n",
            "         1.0533e-02,  6.1542e-03, -3.7020e-02,  1.8747e-04, -2.1195e-02,\n",
            "        -4.4640e-04,  1.5528e-03,  1.6410e-02, -6.1031e-02, -2.0254e-03,\n",
            "        -6.1408e-03,  5.8679e-03, -2.9146e-03, -1.9892e-03,  4.3533e-02,\n",
            "         1.5581e-03, -1.3258e-02, -3.4477e-03, -3.1063e-03,  1.6558e-03,\n",
            "         1.2330e-02, -1.3658e-02,  1.4830e-05,  2.3483e-03, -2.5652e-03,\n",
            "        -4.6861e-02, -3.8526e-03,  1.4891e-04,  3.5026e-03,  2.9304e-02,\n",
            "        -1.2515e-02, -1.6651e-02, -6.5505e-02, -7.0729e-03,  5.6096e-02,\n",
            "         1.6492e-02, -1.0102e-02, -5.1001e-02,  4.6134e-02,  5.9714e-02,\n",
            "         2.7749e-02, -3.9069e-03,  2.8098e-02, -4.3307e-02,  2.9125e-03,\n",
            "        -1.1555e-03, -5.4332e-04,  1.7716e-02,  3.9426e-02, -1.1650e-02,\n",
            "         5.4143e-03,  7.6668e-04, -1.4501e-02, -3.2781e-02, -1.7109e-04,\n",
            "        -4.5489e-03,  1.3235e-02, -3.1242e-02, -6.5027e-02,  4.4712e-03,\n",
            "        -8.8002e-03, -4.7278e-02,  1.5585e-02, -1.2312e-03, -7.9780e-03,\n",
            "         3.0320e-02,  5.8618e-02,  2.9539e-02, -1.0512e-02,  1.3233e-02,\n",
            "        -9.1499e-03, -1.4123e-02,  4.5389e-03, -4.5021e-02,  8.7648e-02,\n",
            "         1.5873e-02,  3.4661e-02,  2.9271e-03,  8.5943e-04,  6.3336e-02,\n",
            "         3.6434e-03,  2.7110e-02,  3.9728e-03,  1.6938e-02,  7.7805e-03,\n",
            "        -5.5892e-02,  1.3069e-02, -6.5158e-02, -1.4633e-03, -5.5059e-02,\n",
            "         7.2858e-03,  5.3833e-03,  6.2287e-02, -2.0552e-03,  5.5169e-02,\n",
            "         4.2386e-02, -6.4451e-03,  1.9792e-02,  5.1887e-02,  1.0740e-03,\n",
            "        -6.3414e-02, -7.8450e-02, -8.0298e-02,  6.7308e-03, -4.0420e-03,\n",
            "        -7.0452e-04,  1.4065e-02,  6.0994e-03,  4.5103e-03,  1.0812e-02,\n",
            "        -2.0521e-02,  6.0328e-02,  5.4586e-03, -6.6997e-02,  7.8335e-02,\n",
            "        -1.5540e-02,  5.5494e-03,  1.2440e-02, -7.8808e-02, -4.0115e-02,\n",
            "         3.3492e-04,  1.2926e-03,  2.9311e-02,  1.7346e-02,  5.1262e-02,\n",
            "        -1.8666e-03, -8.3860e-03, -1.1787e-02,  2.8252e-02, -1.7593e-02,\n",
            "        -8.7817e-02,  5.2170e-03,  4.4334e-02, -3.3100e-02,  7.3799e-02,\n",
            "        -4.1801e-02, -4.6398e-03,  5.7726e-02,  2.2281e-02,  3.2628e-02,\n",
            "         3.7614e-02,  2.2902e-02, -5.2283e-04,  2.9742e-02,  5.6447e-02,\n",
            "         4.1772e-02,  7.6728e-02,  4.9035e-02,  2.5668e-02,  2.9138e-02,\n",
            "         4.4800e-03,  1.5523e-02, -6.5518e-02, -1.5646e-03,  7.2878e-03,\n",
            "         3.4111e-02, -9.0368e-04, -3.1689e-04, -3.5873e-02,  2.4688e-02,\n",
            "        -5.3862e-04,  1.4132e-02, -1.1777e-02,  7.0576e-03, -1.1229e-02,\n",
            "        -1.0256e-02, -3.0035e-02, -3.0638e-03,  1.3067e-02,  1.0861e-02,\n",
            "         5.8566e-02, -7.6218e-03, -5.1887e-03, -4.6288e-03,  5.5355e-03,\n",
            "         7.2679e-02, -5.8685e-02,  1.8479e-02, -5.3238e-03,  6.6855e-02,\n",
            "        -1.4153e-02, -1.0367e-02,  1.9953e-02, -4.0999e-02, -2.1670e-03,\n",
            "        -4.4959e-03, -3.5326e-02,  3.8980e-02, -1.1234e-02, -3.1199e-03,\n",
            "        -1.5884e-02,  2.0611e-02,  2.4461e-03, -2.1509e-03,  5.7122e-02,\n",
            "         5.6231e-02, -6.3469e-03,  3.8378e-03,  7.6445e-03, -3.4256e-03,\n",
            "        -1.8695e-02, -4.3274e-04,  2.8806e-02, -4.9924e-02,  3.1271e-02,\n",
            "        -4.2970e-02,  2.0461e-02, -5.6234e-02,  2.5810e-05, -1.4020e-03,\n",
            "        -1.7969e-02, -1.1418e-02,  8.4572e-03,  5.4102e-03,  2.1549e-02,\n",
            "        -3.1038e-02,  4.6863e-02, -1.0834e-02,  5.6062e-02,  1.6208e-02,\n",
            "         1.5080e-02, -1.9071e-02,  5.0027e-02], device='cuda:0',\n",
            "       requires_grad=True))\n",
            "('module.classifier.weight', Parameter containing:\n",
            "tensor([[ 0.0065, -0.0148,  0.0100,  ...,  0.0008,  0.0157, -0.0075],\n",
            "        [-0.0021, -0.0106, -0.0096,  ..., -0.0100, -0.0068, -0.0022]],\n",
            "       device='cuda:0', requires_grad=True))\n",
            "('module.classifier.bias', Parameter containing:\n",
            "tensor([0., 0.], device='cuda:0', requires_grad=True))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Optimizer Grouped Parameters\n",
        "\n",
        "##This code is taken from: https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L102\n",
        "## Don't apply weight decay to any parameters whose names include these tokens.\n",
        "## (Here, the BERT doesn't have `gamma` or `beta` parameters, only `bias` terms)\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "'''\n",
        "for n,p in param_optimizer:\n",
        "    print(n)\n",
        "    print('------')\n",
        "    print(p)\n",
        "    print('/////////////////////////')\n",
        "'''\n",
        "\n",
        "## Separate the `weight` parameters from the `bias` parameters.\n",
        "## - For the `weight` parameters, this specifies a 'weight_decay_rate'= 0.01\n",
        "## - For the `bias` parameters, the 'weight_decay_rate'= 0.0\n",
        "## `optimizer_grouped_parameters` only includes the parameter values, not the names.\n",
        "optimizer_grouped_parameters = [\n",
        "    # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.1},\n",
        "\n",
        "    # Filter for parameters which *do* include those.\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]"
      ],
      "metadata": {
        "id": "i8n391gzQsGI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The Hyperparemeters for the Training Loop :\n",
        "\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps = 1e-8)\n",
        "\n",
        "## Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "## Total number of training steps is number of batches * number of epochs.\n",
        "## `train_dataloader` contains batched data so `len(train_dataloader)` gives the number of batches.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "## Create the learning rate scheduler.\n",
        "# from transformer import get_linear_schedule_with_warmup\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "##Creating the Accuracy Measurement Function\n",
        "## Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prQBs2-6AMYd",
        "outputId": "9bc51cd7-7522-4bd5-ba23-f68900925438"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# The Training Loop\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ARACfXlUFU9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = []\n",
        "\n",
        "## Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  model.train()\n",
        "\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "  ## Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    #print(f'the number of step is :{step}')>> 240 batch size with size 32\n",
        "    ## Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    ## Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    ## Forward pass\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    loss = outputs['loss']\n",
        "    train_loss_set.append(loss.item())\n",
        "    ## Backward pass\n",
        "    loss.backward()\n",
        "    ## Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    ## Update the learning rate.\n",
        "    scheduler.step()\n",
        "\n",
        "    ## Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "\n",
        "\n",
        "  ## Validation\n",
        "  model.eval()\n",
        "\n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  ## Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    ## Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "        ## Forward pass, calculate logit predictions\n",
        "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "    print(f' the logit is : {logits}')\n",
        "    ## Move logits and labels to CPU\n",
        "    logits = logits['logits'].detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4jmGeBIEvlo",
        "outputId": "3d88dbd3-df55-4b1c-aa2a-940177d55bf7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.5065358272479283\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 1.1496, -0.4891],\n",
            "        [-0.3212,  0.9449],\n",
            "        [-0.5814,  1.2905],\n",
            "        [-1.4652,  1.7141],\n",
            "        [-0.5965,  0.9719],\n",
            "        [-0.0207,  0.5743],\n",
            "        [-1.4689,  1.9538],\n",
            "        [ 0.7399, -0.1618],\n",
            "        [ 1.0658, -0.5665],\n",
            "        [-0.2707,  0.6933],\n",
            "        [-0.9951,  1.6187],\n",
            "        [-0.9176,  1.3990],\n",
            "        [-1.5336,  1.9408],\n",
            "        [-0.2321,  0.8497],\n",
            "        [-1.5718,  1.9757],\n",
            "        [-0.5059,  1.0435],\n",
            "        [ 0.4128,  0.2009],\n",
            "        [-0.8255,  1.4669],\n",
            "        [ 0.4359,  0.3106],\n",
            "        [-0.8598,  1.3869],\n",
            "        [-1.2646,  1.7048],\n",
            "        [-0.6959,  1.3138],\n",
            "        [-1.4680,  2.0384],\n",
            "        [-1.3028,  1.9741],\n",
            "        [ 1.1646, -0.6651],\n",
            "        [-0.5255,  1.2023],\n",
            "        [-0.8882,  1.4623],\n",
            "        [-1.1116,  1.7769],\n",
            "        [-0.5076,  1.0083],\n",
            "        [-1.5778,  1.9690],\n",
            "        [-0.7237,  1.3342],\n",
            "        [-1.4442,  2.0452]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.0953,  1.5580],\n",
            "        [-0.7593,  1.3619],\n",
            "        [-1.2342,  1.7200],\n",
            "        [-0.9770,  1.5904],\n",
            "        [-1.3319,  1.7735],\n",
            "        [-0.5208,  1.2195],\n",
            "        [-0.8541,  1.3093],\n",
            "        [-0.8887,  1.4132],\n",
            "        [ 0.5258,  0.2730],\n",
            "        [-1.3317,  1.8056],\n",
            "        [-1.4115,  1.8133],\n",
            "        [-0.2285,  0.6646],\n",
            "        [-0.7149,  1.1373],\n",
            "        [-1.5291,  1.9030],\n",
            "        [-0.9984,  1.5860],\n",
            "        [-1.2100,  1.7986],\n",
            "        [-1.2377,  1.8481],\n",
            "        [-1.1999,  1.7117],\n",
            "        [-0.9761,  1.5391],\n",
            "        [-1.0498,  1.5708],\n",
            "        [-0.7279,  1.3316],\n",
            "        [-0.0416,  0.5886],\n",
            "        [ 0.2384,  0.3455],\n",
            "        [-0.1367,  0.7844],\n",
            "        [-1.2696,  1.9776],\n",
            "        [-0.5038,  1.2338],\n",
            "        [-1.4897,  2.0066],\n",
            "        [-1.4648,  2.1337],\n",
            "        [-0.8608,  1.2195],\n",
            "        [-0.8969,  1.4352],\n",
            "        [-1.6244,  1.8925],\n",
            "        [ 0.2824,  0.2987]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.5633,  1.9126],\n",
            "        [-0.8341,  1.4641],\n",
            "        [ 0.8869, -0.5828],\n",
            "        [-1.3360,  1.9330],\n",
            "        [-0.7158,  1.4103],\n",
            "        [-0.8912,  1.4597],\n",
            "        [-1.1388,  1.5680],\n",
            "        [-0.1940,  0.8392],\n",
            "        [-1.2289,  1.6448],\n",
            "        [-1.0323,  1.6732],\n",
            "        [-1.1683,  1.7116],\n",
            "        [-1.5920,  1.9267],\n",
            "        [-0.3550,  0.8455],\n",
            "        [-0.8030,  1.3239],\n",
            "        [-0.6350,  1.1873],\n",
            "        [-1.2203,  1.8630],\n",
            "        [-1.4271,  1.8312],\n",
            "        [-1.5202,  1.7872],\n",
            "        [-1.1341,  1.7922],\n",
            "        [-0.9373,  1.4030],\n",
            "        [-0.9097,  1.6485],\n",
            "        [ 0.5182,  0.1476],\n",
            "        [-0.4656,  1.1018],\n",
            "        [-0.6723,  1.2393],\n",
            "        [-0.7095,  1.2254],\n",
            "        [-0.5708,  1.2266],\n",
            "        [ 1.1285, -0.6413],\n",
            "        [ 1.0328, -0.3826],\n",
            "        [ 0.4085,  0.2359],\n",
            "        [ 0.7152, -0.1938],\n",
            "        [ 0.3125,  0.0802],\n",
            "        [ 0.6757,  0.0761]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.0679,  1.7002],\n",
            "        [ 0.9240, -0.3139],\n",
            "        [ 0.2882,  0.2510],\n",
            "        [-0.7363,  1.1994],\n",
            "        [ 0.0233,  0.4226],\n",
            "        [ 0.9251, -0.2237],\n",
            "        [-0.3119,  1.0775],\n",
            "        [-0.4806,  1.0291],\n",
            "        [-0.5242,  1.2102],\n",
            "        [-0.2905,  0.7908],\n",
            "        [ 0.5275,  0.1801],\n",
            "        [ 0.0995,  0.4516],\n",
            "        [ 1.0439, -0.3607],\n",
            "        [-1.6114,  2.0617],\n",
            "        [-0.2730,  0.8656],\n",
            "        [-0.6851,  1.2789],\n",
            "        [-1.3744,  1.8442],\n",
            "        [-1.1652,  1.7361],\n",
            "        [ 0.6105, -0.0895],\n",
            "        [-1.3687,  2.0371],\n",
            "        [ 1.0001, -0.5167],\n",
            "        [-0.3722,  0.9587],\n",
            "        [-0.4728,  1.1824],\n",
            "        [-1.4596,  1.8517],\n",
            "        [-1.3533,  1.9237],\n",
            "        [-1.1382,  1.3696],\n",
            "        [-0.9005,  1.5062],\n",
            "        [-1.1760,  1.5799],\n",
            "        [-0.0131,  0.1678],\n",
            "        [ 0.8034, -0.1946],\n",
            "        [-1.2852,  1.7265],\n",
            "        [-1.3573,  1.9558]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.7443,  1.3048],\n",
            "        [-0.7920,  1.4988],\n",
            "        [-0.3991,  0.9381],\n",
            "        [-1.4765,  2.1069],\n",
            "        [-1.5013,  2.0318],\n",
            "        [ 0.2507,  0.4178],\n",
            "        [-0.9658,  1.5626],\n",
            "        [-1.5945,  2.0073],\n",
            "        [-1.6361,  2.0877],\n",
            "        [-1.0796,  1.7997],\n",
            "        [-1.0629,  1.5358],\n",
            "        [-0.8243,  1.3158],\n",
            "        [ 0.7645, -0.1833],\n",
            "        [-1.2645,  1.5755],\n",
            "        [-1.1846,  1.6963],\n",
            "        [-1.3200,  1.8356],\n",
            "        [-0.6620,  1.1707],\n",
            "        [ 1.0844, -0.3782],\n",
            "        [ 0.8284, -0.1621],\n",
            "        [-0.1606,  0.7676],\n",
            "        [-1.2710,  1.8955],\n",
            "        [-0.5608,  1.2175],\n",
            "        [-0.2958,  0.9312],\n",
            "        [ 0.8886, -0.2383],\n",
            "        [-1.4834,  1.9468],\n",
            "        [ 0.2165,  0.3282],\n",
            "        [ 0.5174,  0.2440],\n",
            "        [-1.3861,  1.7956],\n",
            "        [-0.6785,  1.2344],\n",
            "        [-1.3324,  1.8126],\n",
            "        [-1.2221,  1.5544],\n",
            "        [ 0.5989,  0.0091]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 0.8214, -0.0455],\n",
            "        [ 0.9777, -0.3821],\n",
            "        [-0.8624,  1.4905],\n",
            "        [-0.9526,  1.3281],\n",
            "        [ 0.2070,  0.3232],\n",
            "        [-1.6538,  1.9192],\n",
            "        [-0.8027,  1.2345],\n",
            "        [-0.8650,  1.4076],\n",
            "        [-0.7855,  1.3104],\n",
            "        [ 0.7445, -0.0644],\n",
            "        [-1.0658,  1.7486],\n",
            "        [-1.1515,  1.8177],\n",
            "        [-1.3593,  1.9255],\n",
            "        [-1.0209,  1.5212],\n",
            "        [-1.3220,  1.9764],\n",
            "        [-0.6929,  1.2989],\n",
            "        [-0.8819,  1.4121],\n",
            "        [-0.8025,  1.3968],\n",
            "        [ 0.4763,  0.1725],\n",
            "        [ 1.0457, -0.4816],\n",
            "        [-0.2297,  0.8386],\n",
            "        [-1.5719,  1.7116],\n",
            "        [-0.1190,  0.6439],\n",
            "        [-0.1457,  0.7282],\n",
            "        [-1.1472,  1.6736],\n",
            "        [-1.1348,  1.6227],\n",
            "        [-1.2213,  1.8287],\n",
            "        [-0.7636,  1.4067],\n",
            "        [-0.2109,  0.6932],\n",
            "        [ 0.1719,  0.3779],\n",
            "        [-0.3156,  0.7451],\n",
            "        [-0.3182,  0.8050]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.9095,  1.6845],\n",
            "        [-0.8391,  1.4676],\n",
            "        [-1.1990,  1.6645],\n",
            "        [-0.7431,  1.4846],\n",
            "        [-0.0491,  0.6272],\n",
            "        [-1.1311,  1.7422],\n",
            "        [ 0.9395, -0.3494],\n",
            "        [ 0.4488, -0.0622],\n",
            "        [ 0.0598,  0.5836],\n",
            "        [-1.6944,  1.9523],\n",
            "        [ 0.2101,  0.3235],\n",
            "        [-1.1910,  1.8150],\n",
            "        [-1.3602,  1.9084],\n",
            "        [-0.3636,  0.8835],\n",
            "        [-1.0331,  1.8046],\n",
            "        [ 0.9139, -0.2518],\n",
            "        [-1.1003,  1.7767],\n",
            "        [ 0.9641, -0.4444],\n",
            "        [-1.0488,  1.6458],\n",
            "        [-0.8912,  1.5005],\n",
            "        [-1.4170,  1.9629],\n",
            "        [-0.9920,  1.6558],\n",
            "        [-0.2129,  0.7044],\n",
            "        [ 0.2673,  0.2603],\n",
            "        [-0.7246,  1.3980],\n",
            "        [-0.5575,  1.2042],\n",
            "        [-1.6788,  1.9919],\n",
            "        [-1.0539,  1.5183],\n",
            "        [-0.9315,  1.6267],\n",
            "        [-1.3561,  1.9954],\n",
            "        [-1.0704,  1.7017],\n",
            "        [-1.3799,  1.9509]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-8.2224e-01,  1.3951e+00],\n",
            "        [-8.6747e-01,  1.5544e+00],\n",
            "        [ 3.1245e-01,  2.6668e-01],\n",
            "        [-8.3091e-01,  1.3026e+00],\n",
            "        [ 1.1242e+00, -4.7192e-01],\n",
            "        [ 1.2093e-01,  3.2895e-01],\n",
            "        [-2.1016e-01,  8.3271e-01],\n",
            "        [-7.6279e-02,  6.4472e-01],\n",
            "        [-5.1727e-01,  1.2067e+00],\n",
            "        [ 9.1279e-01, -2.0530e-01],\n",
            "        [-1.2360e-01,  6.3639e-01],\n",
            "        [-1.5582e+00,  1.9485e+00],\n",
            "        [ 1.1687e+00, -6.4595e-01],\n",
            "        [-1.2616e+00,  1.7979e+00],\n",
            "        [-8.5243e-01,  1.4246e+00],\n",
            "        [ 3.6297e-04,  4.8615e-01],\n",
            "        [ 5.6009e-01, -8.5635e-02],\n",
            "        [-4.0983e-01,  1.0570e+00],\n",
            "        [-1.4224e+00,  2.1588e+00],\n",
            "        [ 4.0540e-01,  2.3672e-01],\n",
            "        [-1.2794e+00,  1.7925e+00],\n",
            "        [-1.4614e+00,  2.0228e+00],\n",
            "        [ 1.2067e-01,  7.1672e-01],\n",
            "        [ 5.8364e-01,  1.8632e-01],\n",
            "        [-2.4608e-01,  7.2445e-01],\n",
            "        [ 1.0890e+00, -3.6596e-01],\n",
            "        [-1.3490e+00,  1.9703e+00],\n",
            "        [-7.6711e-01,  1.6119e+00],\n",
            "        [-8.9512e-01,  1.4971e+00],\n",
            "        [-9.4473e-01,  1.4931e+00],\n",
            "        [-3.4730e-01,  8.2446e-01],\n",
            "        [-9.2457e-01,  1.3815e+00]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.8902,  1.4307],\n",
            "        [-0.8790,  1.6045],\n",
            "        [-1.4466,  1.9707],\n",
            "        [-1.3284,  1.9232],\n",
            "        [-0.8783,  1.6163],\n",
            "        [-1.3104,  1.9293],\n",
            "        [ 0.9975, -0.5309],\n",
            "        [-1.1430,  1.8723],\n",
            "        [-0.3433,  0.9341],\n",
            "        [-0.1289,  0.7409],\n",
            "        [-1.2267,  1.6519],\n",
            "        [-1.1252,  1.8471],\n",
            "        [ 0.8291, -0.1704],\n",
            "        [-1.5251,  1.8868],\n",
            "        [-0.8919,  1.6159],\n",
            "        [-0.9543,  1.6191],\n",
            "        [-1.4846,  1.9765],\n",
            "        [-1.6143,  2.0483],\n",
            "        [-1.0887,  1.5336],\n",
            "        [ 0.1336,  0.4913],\n",
            "        [-0.6117,  0.8938],\n",
            "        [ 0.3661,  0.1897],\n",
            "        [-1.0081,  1.5486],\n",
            "        [ 0.5964,  0.0493],\n",
            "        [ 0.1162,  0.4604],\n",
            "        [-0.4264,  0.9487],\n",
            "        [-0.8850,  1.3792],\n",
            "        [-1.3442,  1.9296],\n",
            "        [-0.8529,  1.4967],\n",
            "        [-0.4872,  0.9206],\n",
            "        [-1.3646,  1.9691],\n",
            "        [-0.3670,  1.0191]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-4.4075e-01,  9.9554e-01],\n",
            "        [-9.1658e-01,  1.5920e+00],\n",
            "        [ 4.0949e-01,  2.7042e-01],\n",
            "        [ 6.9465e-01, -8.0465e-02],\n",
            "        [ 1.6836e-01,  4.2164e-01],\n",
            "        [-8.9597e-01,  1.3771e+00],\n",
            "        [-1.3050e+00,  1.9122e+00],\n",
            "        [-1.4749e+00,  1.9947e+00],\n",
            "        [-1.2917e+00,  1.8807e+00],\n",
            "        [ 2.3526e-03,  5.2521e-01],\n",
            "        [ 2.4748e-01,  2.6328e-01],\n",
            "        [ 1.4431e-03,  5.9929e-01],\n",
            "        [-6.1724e-01,  1.2974e+00],\n",
            "        [-7.9587e-01,  1.2831e+00],\n",
            "        [-5.7973e-01,  1.1444e+00],\n",
            "        [-1.5693e+00,  1.9902e+00],\n",
            "        [ 4.6407e-01,  2.4654e-02],\n",
            "        [-1.6258e+00,  1.9838e+00],\n",
            "        [-1.5132e+00,  2.0422e+00],\n",
            "        [ 1.3081e-02,  6.2705e-01],\n",
            "        [-2.6774e-01,  6.8568e-01],\n",
            "        [-1.2507e+00,  1.8722e+00],\n",
            "        [-4.6784e-01,  9.2535e-01],\n",
            "        [-1.1952e+00,  1.6066e+00],\n",
            "        [-8.9424e-01,  1.3313e+00],\n",
            "        [-1.8765e-01,  7.2086e-01],\n",
            "        [-6.4035e-01,  1.3457e+00],\n",
            "        [ 1.1874e+00, -6.8918e-01],\n",
            "        [-4.8053e-01,  1.0006e+00],\n",
            "        [ 7.0443e-01, -1.1227e-01],\n",
            "        [-3.4524e-01,  8.9218e-01],\n",
            "        [-1.3019e+00,  1.8534e+00]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.3467,  0.9913],\n",
            "        [-0.1555,  0.7946],\n",
            "        [-0.7374,  1.1094],\n",
            "        [-1.5081,  1.9586],\n",
            "        [-1.6113,  1.9513],\n",
            "        [ 0.8096, -0.1823],\n",
            "        [-0.9679,  1.6535],\n",
            "        [ 0.5665,  0.2441],\n",
            "        [-1.6050,  1.9712],\n",
            "        [-1.0391,  1.5859],\n",
            "        [-1.6380,  1.8772],\n",
            "        [-1.1207,  1.6105],\n",
            "        [-0.0285,  0.5844],\n",
            "        [-1.4712,  2.1171],\n",
            "        [-0.6380,  1.1636],\n",
            "        [-1.6740,  2.0495],\n",
            "        [-0.8363,  1.5373],\n",
            "        [ 0.4549,  0.0444],\n",
            "        [-0.6604,  1.1803],\n",
            "        [ 0.5752,  0.1535],\n",
            "        [-1.6370,  1.9549],\n",
            "        [ 0.3270,  0.2025],\n",
            "        [ 0.7449, -0.1521],\n",
            "        [-1.1716,  1.7822],\n",
            "        [ 0.9563, -0.3241],\n",
            "        [-1.6100,  2.0178],\n",
            "        [-1.5270,  1.9524],\n",
            "        [-0.7489,  1.4249],\n",
            "        [-0.7207,  1.2657],\n",
            "        [-0.7793,  1.5248],\n",
            "        [-0.2693,  0.9468],\n",
            "        [ 0.0362,  0.4968]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.9913,  1.6653],\n",
            "        [-0.0047,  0.5427],\n",
            "        [-1.1684,  1.7173],\n",
            "        [-1.4910,  1.9568],\n",
            "        [-1.0162,  1.4917],\n",
            "        [ 0.8569, -0.0728],\n",
            "        [-1.3897,  1.8454],\n",
            "        [-1.3757,  1.8806],\n",
            "        [-0.9731,  1.5284],\n",
            "        [ 0.0878,  0.3070],\n",
            "        [-1.1208,  1.6331],\n",
            "        [-0.2240,  0.7834],\n",
            "        [-0.6467,  1.2086],\n",
            "        [ 0.4827,  0.1546],\n",
            "        [-1.5323,  1.9149],\n",
            "        [-0.6595,  1.2169],\n",
            "        [-1.4312,  1.8391],\n",
            "        [-0.9035,  1.4425],\n",
            "        [-0.9347,  1.4074],\n",
            "        [-0.5216,  1.0842],\n",
            "        [-0.3610,  0.8856],\n",
            "        [-1.2121,  1.8092],\n",
            "        [-0.8747,  1.6418],\n",
            "        [-1.5788,  2.0008],\n",
            "        [ 0.2845,  0.4580],\n",
            "        [-0.9137,  1.5537],\n",
            "        [-1.3951,  1.9653],\n",
            "        [-0.0091,  0.7167],\n",
            "        [ 0.0620,  0.5312],\n",
            "        [-1.2910,  1.8985],\n",
            "        [-1.5118,  1.8584],\n",
            "        [-0.6230,  1.3185]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.4157,  1.9246],\n",
            "        [ 0.1421,  0.5309],\n",
            "        [-0.6068,  1.1543],\n",
            "        [-1.3743,  1.8047],\n",
            "        [-1.3560,  1.7361],\n",
            "        [-0.4415,  1.0022],\n",
            "        [ 0.4026,  0.2615],\n",
            "        [-1.4116,  2.0037],\n",
            "        [-0.3896,  0.9423],\n",
            "        [-0.5327,  1.0847],\n",
            "        [ 0.0709,  0.3713],\n",
            "        [-0.7626,  1.4627],\n",
            "        [-1.4161,  1.9353],\n",
            "        [ 0.1188,  0.4793],\n",
            "        [-0.1403,  0.6298],\n",
            "        [-1.2876,  1.9101],\n",
            "        [-0.9887,  1.5141],\n",
            "        [-1.0354,  1.6313],\n",
            "        [ 0.8253, -0.1547],\n",
            "        [-1.3089,  1.9608],\n",
            "        [-0.7523,  1.4476],\n",
            "        [-0.7731,  1.4843],\n",
            "        [ 0.1537,  0.3576],\n",
            "        [-0.6377,  1.1424],\n",
            "        [ 1.0016, -0.4155],\n",
            "        [-1.3155,  1.8521],\n",
            "        [-0.3629,  0.9945],\n",
            "        [-0.2428,  0.8094],\n",
            "        [ 0.2380,  0.1462],\n",
            "        [-0.4217,  0.9122],\n",
            "        [-0.3706,  0.8441],\n",
            "        [ 1.1142, -0.7231]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.2031,  1.9135],\n",
            "        [-1.0877,  1.6992],\n",
            "        [-1.3719,  2.0945],\n",
            "        [ 0.9104, -0.3921],\n",
            "        [-1.1322,  1.5261],\n",
            "        [ 0.1183,  0.3911],\n",
            "        [ 1.0582, -0.4569],\n",
            "        [-1.0378,  1.5911],\n",
            "        [-1.2649,  1.7992],\n",
            "        [-1.5408,  2.0483],\n",
            "        [-1.4019,  2.0083],\n",
            "        [-0.0828,  0.8058],\n",
            "        [-1.3600,  1.7881],\n",
            "        [-0.5414,  1.2579],\n",
            "        [ 0.2597,  0.3188],\n",
            "        [ 1.1292, -0.5597],\n",
            "        [-1.0420,  1.6890],\n",
            "        [ 0.9094, -0.2810],\n",
            "        [-0.9019,  1.3723],\n",
            "        [-0.8410,  1.6802],\n",
            "        [-0.8177,  1.4527],\n",
            "        [-1.1081,  1.6015],\n",
            "        [-1.3419,  1.8082],\n",
            "        [-1.6547,  1.9498],\n",
            "        [-1.3396,  1.7266],\n",
            "        [-0.1141,  0.7139],\n",
            "        [-0.9814,  1.3537],\n",
            "        [-0.5374,  1.2965],\n",
            "        [ 0.0721,  0.4981],\n",
            "        [-1.0753,  1.3754],\n",
            "        [ 1.0335, -0.5164],\n",
            "        [ 0.1420,  0.4649]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.6739,  1.1817],\n",
            "        [ 0.8927, -0.3648],\n",
            "        [ 0.9221, -0.2891],\n",
            "        [ 0.8211, -0.2197],\n",
            "        [ 0.4731,  0.1275],\n",
            "        [-0.3986,  1.0213],\n",
            "        [ 0.5487, -0.0728],\n",
            "        [-1.5480,  2.0244],\n",
            "        [ 0.7593, -0.1895],\n",
            "        [-1.1782,  1.7700],\n",
            "        [-0.5601,  1.0762],\n",
            "        [-1.2542,  2.0148],\n",
            "        [-1.5209,  1.8731],\n",
            "        [ 0.9036, -0.1996],\n",
            "        [-1.6574,  2.0223],\n",
            "        [-1.0825,  1.5668],\n",
            "        [-0.3888,  1.0402],\n",
            "        [-1.5878,  1.9828],\n",
            "        [-1.3035,  1.6727],\n",
            "        [-0.7263,  1.5758],\n",
            "        [-0.8899,  1.5861],\n",
            "        [ 0.0687,  0.4296],\n",
            "        [-1.0884,  1.7561],\n",
            "        [ 0.1512,  0.4732],\n",
            "        [ 0.3857,  0.2423],\n",
            "        [-1.6682,  1.8588],\n",
            "        [ 0.0969,  0.5278],\n",
            "        [ 0.6510,  0.0066],\n",
            "        [-0.2152,  0.6731],\n",
            "        [-1.0047,  1.7529],\n",
            "        [-0.6623,  1.3003],\n",
            "        [ 0.0999,  0.5034]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.4473,  1.8037],\n",
            "        [-0.6762,  1.2301],\n",
            "        [ 0.8496, -0.2755],\n",
            "        [-1.0349,  1.7728],\n",
            "        [ 0.5930,  0.0685],\n",
            "        [-0.0421,  0.5520],\n",
            "        [-0.8773,  1.4282],\n",
            "        [-1.5762,  2.0178],\n",
            "        [-0.3990,  0.8463],\n",
            "        [-0.4919,  0.9653],\n",
            "        [ 0.1749,  0.4430],\n",
            "        [ 0.0385,  0.6420],\n",
            "        [ 0.9174, -0.4461],\n",
            "        [-0.3786,  1.0501],\n",
            "        [-0.5344,  1.0948],\n",
            "        [-1.1344,  1.6709],\n",
            "        [-0.3834,  1.0360],\n",
            "        [ 0.7106, -0.1946],\n",
            "        [-0.7324,  1.2972],\n",
            "        [-0.0169,  0.5794],\n",
            "        [ 0.6197,  0.0048],\n",
            "        [ 0.8270, -0.1565],\n",
            "        [ 1.1379, -0.6731],\n",
            "        [ 1.1396, -0.6122],\n",
            "        [-1.6329,  1.9291],\n",
            "        [-1.5030,  1.9514],\n",
            "        [-1.0213,  1.4921],\n",
            "        [-1.2601,  1.8274],\n",
            "        [-1.2989,  1.6943],\n",
            "        [-0.1506,  0.5616],\n",
            "        [-0.1489,  0.7476],\n",
            "        [ 0.0251,  0.6114]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.1325,  1.4799],\n",
            "        [-0.2417,  0.9478],\n",
            "        [-0.9428,  1.5761],\n",
            "        [-0.6437,  1.1911],\n",
            "        [-0.9309,  1.3630],\n",
            "        [-0.7427,  1.4011],\n",
            "        [-0.9445,  1.6160],\n",
            "        [-1.1791,  1.7636],\n",
            "        [-1.5944,  1.9421],\n",
            "        [-0.6679,  1.2527],\n",
            "        [-0.7542,  1.4317],\n",
            "        [-1.3808,  1.9777],\n",
            "        [-1.1673,  1.8975],\n",
            "        [ 0.4748,  0.1505],\n",
            "        [ 0.8011, -0.2637],\n",
            "        [-0.7755,  1.0964],\n",
            "        [-0.7456,  1.1480],\n",
            "        [ 1.0047, -0.4073],\n",
            "        [-0.7079,  1.1109],\n",
            "        [ 0.5575,  0.0403],\n",
            "        [-0.6123,  1.1691],\n",
            "        [-0.3966,  1.0086],\n",
            "        [ 1.0448, -0.5246],\n",
            "        [ 0.7370, -0.0478],\n",
            "        [-0.7090,  1.2685],\n",
            "        [-1.4221,  1.9873],\n",
            "        [-1.3296,  2.0622],\n",
            "        [-0.6085,  1.1343],\n",
            "        [-0.2958,  0.7920],\n",
            "        [ 0.2252,  0.3600],\n",
            "        [ 1.1407, -0.5747],\n",
            "        [-1.3311,  2.0451]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.0110,  0.6513],\n",
            "        [-1.3029,  1.8623],\n",
            "        [-0.9115,  1.5799],\n",
            "        [-0.1297,  0.6950],\n",
            "        [-1.1065,  1.6394],\n",
            "        [-0.4598,  1.0686],\n",
            "        [-1.6180,  1.9722],\n",
            "        [-1.4213,  1.8241],\n",
            "        [ 1.0495, -0.3178],\n",
            "        [-1.4238,  1.9668],\n",
            "        [ 0.7125, -0.0590],\n",
            "        [-1.0549,  1.6757],\n",
            "        [ 0.5394,  0.0546],\n",
            "        [-0.5548,  1.0556],\n",
            "        [-0.7588,  1.5741],\n",
            "        [-1.6166,  2.1150],\n",
            "        [-0.7234,  1.3238],\n",
            "        [-1.6505,  1.8870],\n",
            "        [ 0.8467, -0.2499],\n",
            "        [ 0.3971,  0.0356],\n",
            "        [-0.1619,  0.7827],\n",
            "        [ 0.1764,  0.3566],\n",
            "        [-0.5616,  1.2837],\n",
            "        [-0.5214,  0.8967],\n",
            "        [-0.2028,  0.7167],\n",
            "        [ 0.9070, -0.4003],\n",
            "        [-0.7145,  1.3023],\n",
            "        [ 1.0360, -0.2949],\n",
            "        [-0.0829,  0.6838],\n",
            "        [-0.9566,  1.3067],\n",
            "        [-0.9811,  1.4827],\n",
            "        [-0.8775,  1.4634]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6563,  0.0052],\n",
            "        [-0.2613,  0.7717],\n",
            "        [-1.4340,  1.9624],\n",
            "        [-1.5726,  1.9356],\n",
            "        [ 0.9229, -0.3201],\n",
            "        [ 0.8810, -0.1808],\n",
            "        [-0.4011,  1.0022],\n",
            "        [-0.4180,  0.9272],\n",
            "        [-1.0116,  1.6517],\n",
            "        [-1.1927,  1.6609],\n",
            "        [-0.4589,  1.0980],\n",
            "        [-1.1011,  1.5680],\n",
            "        [-0.9165,  1.3989],\n",
            "        [ 0.1840,  0.5089],\n",
            "        [ 1.0120, -0.4359],\n",
            "        [-0.6560,  1.3642],\n",
            "        [-0.5953,  1.0960],\n",
            "        [ 0.7048, -0.1366],\n",
            "        [-1.3498,  1.7446],\n",
            "        [-0.8475,  1.5267],\n",
            "        [-0.7194,  1.2718],\n",
            "        [-0.1573,  0.6498],\n",
            "        [ 0.2946,  0.4353],\n",
            "        [-1.1706,  1.7887],\n",
            "        [ 1.1449, -0.6948],\n",
            "        [ 0.7405, -0.0884],\n",
            "        [-0.2223,  0.7441],\n",
            "        [-0.6752,  1.3869],\n",
            "        [-0.4643,  1.0549],\n",
            "        [-0.9433,  1.6319],\n",
            "        [-1.1255,  1.7093],\n",
            "        [-1.1010,  1.7373]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.1356,  1.6278],\n",
            "        [-0.8926,  1.4762],\n",
            "        [-0.6069,  1.2645],\n",
            "        [-1.3726,  1.7562],\n",
            "        [-1.2531,  1.7366],\n",
            "        [-1.4470,  1.9768],\n",
            "        [-1.3189,  1.8721],\n",
            "        [-1.2775,  1.7316],\n",
            "        [ 0.2339,  0.3726],\n",
            "        [-0.9998,  1.5418],\n",
            "        [-1.1659,  1.7942],\n",
            "        [-0.8969,  1.4971],\n",
            "        [ 1.1161, -0.6690],\n",
            "        [-1.2179,  1.7128],\n",
            "        [-1.3709,  1.9995],\n",
            "        [-1.5111,  1.8821],\n",
            "        [-1.2146,  1.6987],\n",
            "        [-1.0341,  1.6220],\n",
            "        [ 1.1150, -0.5660],\n",
            "        [-1.3044,  1.7955],\n",
            "        [ 0.8837, -0.2768],\n",
            "        [ 0.0425,  0.4049],\n",
            "        [ 0.7219,  0.0987],\n",
            "        [-0.2210,  0.7960],\n",
            "        [-1.3757,  1.8809],\n",
            "        [ 1.0629, -0.5048],\n",
            "        [ 0.9257, -0.1365],\n",
            "        [ 0.1797,  0.4278],\n",
            "        [ 1.0588, -0.5452],\n",
            "        [-1.0700,  1.6037],\n",
            "        [-0.3652,  0.8023],\n",
            "        [-1.3573,  1.7737]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 0.8728, -0.2957],\n",
            "        [ 0.8813, -0.1495],\n",
            "        [-0.6144,  1.1763],\n",
            "        [-0.6463,  1.2928],\n",
            "        [-0.8022,  1.5314],\n",
            "        [-1.1420,  1.6408],\n",
            "        [-1.3398,  1.8630],\n",
            "        [-0.1970,  0.8150],\n",
            "        [-0.9746,  1.7450],\n",
            "        [-0.6846,  1.2054],\n",
            "        [-0.7675,  1.4695],\n",
            "        [-0.8938,  1.3082],\n",
            "        [-0.6862,  1.0667],\n",
            "        [-1.0095,  1.6408],\n",
            "        [-0.8918,  1.4519],\n",
            "        [-1.4968,  1.9729],\n",
            "        [ 0.9053, -0.2869],\n",
            "        [ 0.5046,  0.0371],\n",
            "        [-1.5718,  2.0805],\n",
            "        [ 0.6598, -0.0022],\n",
            "        [-1.3400,  1.7659],\n",
            "        [-1.0611,  1.7060],\n",
            "        [-0.4293,  0.7283],\n",
            "        [ 0.4373,  0.0884],\n",
            "        [-0.5389,  1.1707],\n",
            "        [-1.5925,  1.9864],\n",
            "        [-0.6067,  1.1040],\n",
            "        [-0.5936,  1.1049],\n",
            "        [-1.0550,  1.7914],\n",
            "        [-1.0659,  1.4482],\n",
            "        [-0.6252,  1.1897],\n",
            "        [ 1.1038, -0.5641]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-9.2527e-01,  1.3762e+00],\n",
            "        [ 1.0343e+00, -4.6180e-01],\n",
            "        [-3.3871e-01,  9.8100e-01],\n",
            "        [-2.0379e-03,  3.5222e-01],\n",
            "        [-1.0475e+00,  1.6349e+00],\n",
            "        [-1.0133e+00,  1.5384e+00],\n",
            "        [-6.2021e-01,  1.2593e+00],\n",
            "        [-8.0399e-01,  1.1738e+00],\n",
            "        [-3.3000e-01,  8.5654e-01],\n",
            "        [-4.3304e-01,  9.9412e-01],\n",
            "        [ 7.1530e-02,  4.5729e-01],\n",
            "        [-6.3732e-01,  1.2516e+00],\n",
            "        [-5.8566e-01,  1.0960e+00],\n",
            "        [-5.8083e-01,  1.1223e+00],\n",
            "        [-1.5889e+00,  2.0989e+00],\n",
            "        [ 1.4373e-01,  4.7212e-01],\n",
            "        [-1.4450e+00,  1.8836e+00],\n",
            "        [-1.2684e+00,  1.6044e+00],\n",
            "        [-1.2437e+00,  1.8101e+00],\n",
            "        [-6.1786e-01,  1.2220e+00],\n",
            "        [-1.5550e+00,  2.1156e+00],\n",
            "        [-8.2258e-01,  1.4723e+00],\n",
            "        [-1.6730e+00,  1.9024e+00],\n",
            "        [-1.6279e+00,  2.0013e+00],\n",
            "        [-1.6272e-01,  7.4987e-01],\n",
            "        [-1.3777e+00,  1.8438e+00],\n",
            "        [-1.5266e+00,  1.9553e+00],\n",
            "        [ 1.9744e-01,  5.6084e-01],\n",
            "        [ 7.5113e-02,  5.9829e-01],\n",
            "        [ 8.3753e-01, -1.6650e-01],\n",
            "        [ 1.0489e+00, -3.9503e-01],\n",
            "        [-1.0353e+00,  1.5762e+00]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.1134,  1.6443],\n",
            "        [-0.9172,  1.4243],\n",
            "        [ 0.6407, -0.0543],\n",
            "        [ 0.9972, -0.2501],\n",
            "        [-0.9130,  1.4380],\n",
            "        [ 0.4654,  0.2094],\n",
            "        [-0.5504,  1.0663],\n",
            "        [-0.8483,  1.4210],\n",
            "        [ 0.2788,  0.3581],\n",
            "        [ 0.2298,  0.3592],\n",
            "        [-0.0067,  0.5320],\n",
            "        [-0.0891,  0.5584],\n",
            "        [-0.8485,  1.4302],\n",
            "        [-1.3996,  1.9062],\n",
            "        [ 1.0550, -0.5996],\n",
            "        [ 1.0899, -0.4212],\n",
            "        [ 1.1125, -0.5561],\n",
            "        [-0.8691,  1.3257],\n",
            "        [ 0.5952,  0.0542],\n",
            "        [-0.2825,  0.8708],\n",
            "        [-0.4879,  1.1506],\n",
            "        [-0.5996,  1.2571],\n",
            "        [ 1.0931, -0.6634],\n",
            "        [-1.4345,  1.9828],\n",
            "        [-0.7152,  1.2413],\n",
            "        [ 0.6290,  0.1041],\n",
            "        [-0.0626,  0.6158],\n",
            "        [-1.1800,  1.6448],\n",
            "        [-0.9745,  1.4444],\n",
            "        [ 0.1676,  0.5721],\n",
            "        [-1.2707,  1.8674],\n",
            "        [-1.3389,  1.9583]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.7154,  1.4293],\n",
            "        [-0.9069,  1.6986],\n",
            "        [-1.1939,  1.7352],\n",
            "        [-1.5480,  2.0717],\n",
            "        [-0.0851,  0.6157],\n",
            "        [-0.4331,  1.1036],\n",
            "        [-0.4728,  1.0437],\n",
            "        [-0.7095,  1.3884],\n",
            "        [-0.6398,  1.1550],\n",
            "        [-0.2863,  0.9277],\n",
            "        [-0.5042,  1.0977],\n",
            "        [-1.0043,  1.4754],\n",
            "        [-0.5493,  0.9810],\n",
            "        [-0.5695,  0.9674],\n",
            "        [-1.5331,  1.8089],\n",
            "        [-0.8866,  1.4578],\n",
            "        [-0.7161,  1.2758],\n",
            "        [ 0.8147, -0.0454],\n",
            "        [ 0.8696, -0.3481],\n",
            "        [-0.3111,  0.8772],\n",
            "        [-1.2302,  1.8095],\n",
            "        [-0.5659,  1.1616],\n",
            "        [-0.8541,  1.4758],\n",
            "        [ 0.7731,  0.0575],\n",
            "        [-0.4913,  1.1658],\n",
            "        [-1.3782,  1.5737],\n",
            "        [-0.4957,  0.8875],\n",
            "        [-1.5666,  1.9783],\n",
            "        [-0.9335,  1.6307],\n",
            "        [ 0.2376,  0.2573],\n",
            "        [ 0.5871, -0.0493],\n",
            "        [-1.2016,  1.8426]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1914,  0.4771],\n",
            "        [-1.0108,  1.4797],\n",
            "        [-1.1274,  1.8482],\n",
            "        [-0.8097,  1.2985],\n",
            "        [-0.7586,  1.3742],\n",
            "        [-1.0826,  1.9734],\n",
            "        [-0.5198,  1.1016],\n",
            "        [ 0.5532,  0.2110],\n",
            "        [-0.3543,  0.7821],\n",
            "        [ 0.3167,  0.2201],\n",
            "        [-1.2366,  1.8017],\n",
            "        [-1.6891,  2.0466],\n",
            "        [-1.4149,  2.0284],\n",
            "        [-1.3706,  1.9673],\n",
            "        [-0.1519,  0.7908],\n",
            "        [-0.8448,  1.2076],\n",
            "        [ 1.1081, -0.4816],\n",
            "        [ 0.6311, -0.0257],\n",
            "        [-0.1496,  0.7168],\n",
            "        [-0.5930,  1.2044],\n",
            "        [-1.1011,  1.6091],\n",
            "        [ 0.1627,  0.4146],\n",
            "        [-0.5375,  1.0378],\n",
            "        [-0.1321,  0.6722],\n",
            "        [ 0.9983, -0.3771],\n",
            "        [-1.1063,  1.6219],\n",
            "        [-1.0787,  1.6569],\n",
            "        [-1.4051,  2.0231],\n",
            "        [-0.1197,  0.6916],\n",
            "        [-1.6814,  1.9854],\n",
            "        [ 0.5725, -0.0557],\n",
            "        [-0.3033,  0.8207]], device='cuda:0'), hidden_states=None, attentions=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  25%|██▌       | 1/4 [02:29<07:27, 149.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9449, -0.2707],\n",
            "        [-0.7315,  1.4276],\n",
            "        [-1.2606,  1.7806],\n",
            "        [ 0.2448,  0.3675],\n",
            "        [ 1.0933, -0.5442],\n",
            "        [-0.0982,  0.6318],\n",
            "        [-1.5515,  1.9130],\n",
            "        [-0.2275,  0.6708],\n",
            "        [-1.6599,  2.0413],\n",
            "        [-0.8404,  1.4252],\n",
            "        [ 0.6114,  0.1114],\n",
            "        [-0.2309,  0.7668],\n",
            "        [ 0.0094,  0.4508],\n",
            "        [-0.2512,  0.8391],\n",
            "        [ 0.8609, -0.3326],\n",
            "        [-0.9353,  1.4885],\n",
            "        [-0.1827,  0.6094],\n",
            "        [-1.2283,  1.8359],\n",
            "        [-1.4998,  1.9440],\n",
            "        [ 0.2128,  0.5907],\n",
            "        [-0.9085,  1.3898],\n",
            "        [-1.3430,  1.8307],\n",
            "        [-1.2329,  1.6324],\n",
            "        [-0.8196,  1.2860],\n",
            "        [ 0.4449,  0.2848],\n",
            "        [ 0.0886,  0.4959],\n",
            "        [-1.3734,  1.9819],\n",
            "        [ 0.0293,  0.4999],\n",
            "        [ 0.3195,  0.3730],\n",
            "        [ 0.7490,  0.1372],\n",
            "        [-1.0723,  1.8940],\n",
            "        [-0.3017,  0.9205]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.6659,  1.9542],\n",
            "        [ 0.6881, -0.1612],\n",
            "        [-0.8950,  1.3932],\n",
            "        [ 0.3297,  0.3446],\n",
            "        [-1.3025,  1.8781],\n",
            "        [-0.9518,  1.5898],\n",
            "        [ 0.2399,  0.5979],\n",
            "        [-0.9902,  1.6178],\n",
            "        [ 0.6686, -0.0180],\n",
            "        [-0.6778,  1.2783],\n",
            "        [-1.1613,  1.8135],\n",
            "        [-0.3399,  0.6114],\n",
            "        [-1.0276,  1.6340],\n",
            "        [-0.8304,  1.4632],\n",
            "        [ 0.1522,  0.3825],\n",
            "        [-0.7196,  1.2849],\n",
            "        [ 0.1595,  0.3852],\n",
            "        [ 0.8784, -0.2972],\n",
            "        [ 0.2453,  0.3129],\n",
            "        [-1.5840,  2.0014],\n",
            "        [-1.0038,  1.6668],\n",
            "        [-0.1602,  0.4668],\n",
            "        [-1.0300,  1.6734],\n",
            "        [-1.5200,  1.7413]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Accuracy: 0.8094135802469137\n",
            "Train loss: 0.30970812595113184\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5444, -1.5390],\n",
            "        [-0.7273,  1.2313],\n",
            "        [-0.2219,  0.7523],\n",
            "        [-1.5725,  1.8612],\n",
            "        [-0.4079,  0.6765],\n",
            "        [ 0.6730, -0.2322],\n",
            "        [-1.8741,  2.3196],\n",
            "        [ 1.0377, -0.9160],\n",
            "        [ 1.6497, -2.0089],\n",
            "        [-0.6055,  0.7984],\n",
            "        [-1.0572,  1.5547],\n",
            "        [-1.3136,  1.6516],\n",
            "        [-1.8412,  2.2301],\n",
            "        [-0.3625,  0.9048],\n",
            "        [-1.7485,  2.2634],\n",
            "        [-0.3635,  0.6959],\n",
            "        [ 0.6197, -0.2108],\n",
            "        [-1.2389,  1.8351],\n",
            "        [ 0.9130, -0.3212],\n",
            "        [-0.8937,  1.3581],\n",
            "        [-1.8785,  2.2225],\n",
            "        [-0.9836,  1.5216],\n",
            "        [-1.6915,  2.2893],\n",
            "        [-1.6785,  2.2639],\n",
            "        [ 1.5868, -2.0547],\n",
            "        [-0.7497,  1.4425],\n",
            "        [-1.4279,  1.8687],\n",
            "        [-1.7527,  2.3374],\n",
            "        [-0.9200,  1.3375],\n",
            "        [-1.9474,  2.3664],\n",
            "        [-0.9934,  1.5662],\n",
            "        [-1.9554,  2.4521]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.5899e+00,  1.9539e+00],\n",
            "        [-1.0030e+00,  1.4833e+00],\n",
            "        [-1.9175e+00,  2.2252e+00],\n",
            "        [-1.5675e+00,  2.0857e+00],\n",
            "        [-1.6913e+00,  2.0755e+00],\n",
            "        [-4.7407e-01,  8.8197e-01],\n",
            "        [-1.5959e+00,  1.9045e+00],\n",
            "        [-1.2768e+00,  1.8341e+00],\n",
            "        [ 7.6344e-01, -1.3672e-02],\n",
            "        [-1.9770e+00,  2.3563e+00],\n",
            "        [-1.7251e+00,  2.1205e+00],\n",
            "        [-1.2176e+00,  1.6183e+00],\n",
            "        [ 5.3451e-01, -2.3286e-01],\n",
            "        [-2.0420e+00,  2.3781e+00],\n",
            "        [-4.3850e-01,  9.4604e-01],\n",
            "        [-1.5727e+00,  2.1250e+00],\n",
            "        [-1.4581e+00,  1.9671e+00],\n",
            "        [-1.7269e+00,  2.1706e+00],\n",
            "        [-9.1496e-01,  1.4502e+00],\n",
            "        [-1.3574e+00,  1.8032e+00],\n",
            "        [-6.3436e-01,  1.2030e+00],\n",
            "        [ 2.6083e-01,  1.0848e-01],\n",
            "        [ 4.6536e-01, -2.7561e-02],\n",
            "        [-1.3714e-03,  4.4022e-01],\n",
            "        [-1.9275e+00,  2.4371e+00],\n",
            "        [-6.3105e-01,  1.2574e+00],\n",
            "        [-2.0301e+00,  2.4877e+00],\n",
            "        [-1.9582e+00,  2.5123e+00],\n",
            "        [-9.2483e-01,  1.2218e+00],\n",
            "        [-1.3448e+00,  1.7211e+00],\n",
            "        [-1.7347e+00,  1.9356e+00],\n",
            "        [ 2.0567e-01,  1.9964e-01]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.9779,  2.2995],\n",
            "        [-1.2670,  1.7892],\n",
            "        [ 1.4411, -1.5939],\n",
            "        [-1.9459,  2.3931],\n",
            "        [-0.4996,  1.1024],\n",
            "        [-1.2671,  1.7898],\n",
            "        [-1.5770,  1.8887],\n",
            "        [ 0.2139,  0.3958],\n",
            "        [-1.4929,  1.8285],\n",
            "        [-1.5490,  2.1482],\n",
            "        [-1.6278,  2.0412],\n",
            "        [-1.8982,  2.1287],\n",
            "        [-0.7555,  1.2170],\n",
            "        [ 0.1212,  0.1876],\n",
            "        [-0.8187,  1.2627],\n",
            "        [-1.8591,  2.3760],\n",
            "        [-1.6381,  2.0815],\n",
            "        [-1.9966,  2.2466],\n",
            "        [-1.4053,  1.9697],\n",
            "        [-1.7132,  2.1435],\n",
            "        [-1.5403,  2.2039],\n",
            "        [ 1.1151, -0.7456],\n",
            "        [-0.9829,  1.4939],\n",
            "        [-1.0297,  1.5553],\n",
            "        [-0.7944,  1.2176],\n",
            "        [-0.9671,  1.5722],\n",
            "        [ 1.4865, -1.8575],\n",
            "        [ 1.1201, -0.8610],\n",
            "        [ 1.0109, -0.6096],\n",
            "        [ 0.8913, -0.6820],\n",
            "        [ 1.0068, -1.4962],\n",
            "        [ 1.0080, -0.6954]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.4143,  2.0459],\n",
            "        [ 1.1957, -1.2601],\n",
            "        [ 0.8073, -0.5728],\n",
            "        [-1.3475,  1.6321],\n",
            "        [ 0.3064,  0.0394],\n",
            "        [ 1.0181, -0.7321],\n",
            "        [-0.9404,  1.5391],\n",
            "        [-0.9433,  1.2244],\n",
            "        [-0.6431,  1.3095],\n",
            "        [-0.3207,  0.8092],\n",
            "        [ 0.5268, -0.1534],\n",
            "        [ 0.6094, -0.2783],\n",
            "        [ 1.1880, -0.9076],\n",
            "        [-1.9874,  2.4741],\n",
            "        [-0.5362,  0.9086],\n",
            "        [-1.4221,  1.9341],\n",
            "        [-1.8071,  2.2460],\n",
            "        [-1.4903,  1.9781],\n",
            "        [ 1.0265, -1.0029],\n",
            "        [-1.7134,  2.3267],\n",
            "        [ 1.5639, -1.8477],\n",
            "        [-0.4872,  0.8932],\n",
            "        [-0.7626,  1.3399],\n",
            "        [-1.7457,  2.1041],\n",
            "        [-1.7859,  2.2878],\n",
            "        [-1.5189,  1.7536],\n",
            "        [-1.0028,  1.5865],\n",
            "        [-1.6597,  1.9969],\n",
            "        [ 0.3385, -0.3642],\n",
            "        [ 1.1401, -0.8413],\n",
            "        [-1.8396,  2.2374],\n",
            "        [-1.6911,  2.2326]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.1449,  1.7078],\n",
            "        [-1.3871,  1.9907],\n",
            "        [-0.0239,  0.4125],\n",
            "        [-1.8425,  2.3144],\n",
            "        [-1.8970,  2.3542],\n",
            "        [ 0.5981, -0.2317],\n",
            "        [-1.3923,  1.8584],\n",
            "        [-1.8980,  2.4096],\n",
            "        [-1.9614,  2.4058],\n",
            "        [-1.4953,  2.1365],\n",
            "        [-1.6542,  1.9785],\n",
            "        [-0.9896,  1.4156],\n",
            "        [ 1.1763, -0.9953],\n",
            "        [-1.5441,  1.8083],\n",
            "        [-1.7813,  2.1764],\n",
            "        [-1.8930,  2.2681],\n",
            "        [-1.2826,  1.6715],\n",
            "        [ 1.1645, -0.8407],\n",
            "        [ 1.1681, -1.0811],\n",
            "        [-0.4510,  0.9644],\n",
            "        [-1.3241,  1.8756],\n",
            "        [-0.4607,  0.9725],\n",
            "        [ 1.0000, -0.5160],\n",
            "        [ 1.2467, -1.1474],\n",
            "        [-1.7607,  2.2794],\n",
            "        [ 0.6684, -0.3465],\n",
            "        [ 0.8277, -0.1886],\n",
            "        [-1.6730,  2.0018],\n",
            "        [-1.0875,  1.4884],\n",
            "        [-2.0187,  2.3707],\n",
            "        [-1.6339,  1.9664],\n",
            "        [ 0.3772, -0.0675]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 0.5222, -0.0038],\n",
            "        [ 1.5286, -1.7776],\n",
            "        [-0.8558,  1.3719],\n",
            "        [-1.2197,  1.5517],\n",
            "        [-0.4363,  0.8491],\n",
            "        [-2.0660,  2.3513],\n",
            "        [-0.9384,  1.3138],\n",
            "        [-1.4983,  1.9784],\n",
            "        [-0.8969,  1.3403],\n",
            "        [ 1.0716, -0.8060],\n",
            "        [-1.6063,  2.2407],\n",
            "        [-1.6696,  2.2403],\n",
            "        [-1.7441,  2.2432],\n",
            "        [-1.4052,  1.8322],\n",
            "        [-1.6170,  2.2767],\n",
            "        [-1.6956,  2.0885],\n",
            "        [-1.0033,  1.5243],\n",
            "        [-1.5008,  1.9179],\n",
            "        [ 0.0467,  0.3884],\n",
            "        [ 1.5504, -1.8037],\n",
            "        [-0.0944,  0.5728],\n",
            "        [-1.8876,  2.0298],\n",
            "        [ 0.1856,  0.3199],\n",
            "        [ 0.0684,  0.4723],\n",
            "        [-1.7701,  2.1467],\n",
            "        [-0.8747,  1.3642],\n",
            "        [-1.6294,  2.1553],\n",
            "        [-1.0485,  1.6560],\n",
            "        [-0.7001,  1.1097],\n",
            "        [ 0.2007,  0.1335],\n",
            "        [-0.0631,  0.3556],\n",
            "        [ 0.1024,  0.2169]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.3292,  2.1241],\n",
            "        [-1.2915,  1.8761],\n",
            "        [-1.5992,  2.0212],\n",
            "        [-1.3221,  2.1151],\n",
            "        [ 1.2097, -1.4250],\n",
            "        [-1.9335,  2.4513],\n",
            "        [ 1.5427, -1.6468],\n",
            "        [-0.2530,  0.3871],\n",
            "        [ 0.5565, -0.1120],\n",
            "        [-2.0514,  2.3342],\n",
            "        [ 0.7082, -0.4055],\n",
            "        [-1.2593,  1.7722],\n",
            "        [-1.9414,  2.3865],\n",
            "        [ 0.0351,  0.4084],\n",
            "        [-1.3812,  2.1127],\n",
            "        [ 1.3181, -1.2008],\n",
            "        [-1.5390,  2.1211],\n",
            "        [ 1.3935, -1.3845],\n",
            "        [-1.3415,  1.8306],\n",
            "        [-1.3476,  1.8725],\n",
            "        [-1.4150,  2.0258],\n",
            "        [-1.3727,  1.9591],\n",
            "        [-0.3163,  0.6311],\n",
            "        [ 0.6563, -0.2008],\n",
            "        [-1.0124,  1.6935],\n",
            "        [-1.1641,  1.6557],\n",
            "        [-2.0802,  2.4440],\n",
            "        [-1.6891,  2.0282],\n",
            "        [-1.4932,  2.1366],\n",
            "        [-2.0211,  2.4881],\n",
            "        [-1.9218,  2.3384],\n",
            "        [-1.6890,  2.2567]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.4310,  1.8530],\n",
            "        [-0.9720,  1.6041],\n",
            "        [-0.2876,  0.7003],\n",
            "        [-1.1109,  1.4888],\n",
            "        [ 1.5151, -1.6345],\n",
            "        [ 1.4875, -1.7652],\n",
            "        [ 0.6764, -0.1815],\n",
            "        [ 0.3400,  0.1560],\n",
            "        [-0.8386,  1.4786],\n",
            "        [ 1.0514, -0.7721],\n",
            "        [-0.1226,  0.4423],\n",
            "        [-2.0579,  2.4012],\n",
            "        [ 1.5125, -1.6499],\n",
            "        [-1.6446,  2.1428],\n",
            "        [-1.0023,  1.4708],\n",
            "        [ 0.4206, -0.1523],\n",
            "        [ 1.0363, -1.4700],\n",
            "        [-0.2773,  0.8309],\n",
            "        [-1.9856,  2.5101],\n",
            "        [ 0.4344, -0.1740],\n",
            "        [-1.4729,  1.8925],\n",
            "        [-1.3025,  1.8646],\n",
            "        [-0.1208,  0.8688],\n",
            "        [ 0.5962, -0.0830],\n",
            "        [-0.6414,  1.0156],\n",
            "        [ 1.3436, -1.1156],\n",
            "        [-1.7424,  2.3091],\n",
            "        [-0.8733,  1.7211],\n",
            "        [ 0.0153,  0.4235],\n",
            "        [-1.3461,  1.7562],\n",
            "        [-0.2383,  0.6297],\n",
            "        [-1.7494,  2.1396]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.7288,  2.1928],\n",
            "        [-1.3268,  1.9656],\n",
            "        [-1.7192,  2.2512],\n",
            "        [-1.7658,  2.3160],\n",
            "        [-1.4248,  2.1101],\n",
            "        [-1.9577,  2.3899],\n",
            "        [ 1.3452, -1.5490],\n",
            "        [-1.6530,  2.2825],\n",
            "        [-0.1741,  0.7357],\n",
            "        [-0.4770,  0.9648],\n",
            "        [-1.6637,  2.0197],\n",
            "        [-1.7165,  2.3101],\n",
            "        [ 1.4688, -1.6806],\n",
            "        [-1.9999,  2.2694],\n",
            "        [-0.8909,  1.4596],\n",
            "        [-1.3181,  1.8168],\n",
            "        [-1.9302,  2.3642],\n",
            "        [-2.0295,  2.4728],\n",
            "        [-1.1391,  1.4939],\n",
            "        [ 0.9220, -0.5159],\n",
            "        [-1.0012,  1.2659],\n",
            "        [ 0.7851, -0.4721],\n",
            "        [-1.2563,  1.7330],\n",
            "        [ 1.5057, -1.5357],\n",
            "        [-0.1273,  0.5271],\n",
            "        [-0.0620,  0.5363],\n",
            "        [-1.2588,  1.5960],\n",
            "        [-1.9699,  2.3854],\n",
            "        [-1.1062,  1.7203],\n",
            "        [-0.6102,  0.9259],\n",
            "        [-1.7429,  2.2881],\n",
            "        [-0.5813,  1.0849]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.5183,  0.9395],\n",
            "        [-1.6215,  2.2542],\n",
            "        [ 0.5942, -0.0966],\n",
            "        [ 1.4257, -1.6668],\n",
            "        [ 0.9519, -0.7211],\n",
            "        [-1.0093,  1.3990],\n",
            "        [-1.5878,  2.1499],\n",
            "        [-1.7882,  2.3413],\n",
            "        [-1.9747,  2.3786],\n",
            "        [ 1.0456, -0.8814],\n",
            "        [ 0.6830, -0.3761],\n",
            "        [-0.3840,  0.9613],\n",
            "        [-0.5446,  1.1652],\n",
            "        [-0.9796,  1.3343],\n",
            "        [-1.1469,  1.5050],\n",
            "        [-2.0759,  2.4670],\n",
            "        [ 0.2251,  0.0702],\n",
            "        [-2.0512,  2.3304],\n",
            "        [-1.7437,  2.3198],\n",
            "        [-0.5531,  1.0220],\n",
            "        [-0.1093,  0.2297],\n",
            "        [-1.7418,  2.2929],\n",
            "        [ 0.1697,  0.1639],\n",
            "        [-1.4521,  1.7367],\n",
            "        [-0.5542,  0.9207],\n",
            "        [ 0.0786,  0.3659],\n",
            "        [-0.8758,  1.4638],\n",
            "        [ 1.5232, -2.1181],\n",
            "        [-1.0228,  1.4251],\n",
            "        [ 0.6491, -0.2860],\n",
            "        [-0.5803,  1.0857],\n",
            "        [-1.5541,  1.9804]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.8911,  1.4096],\n",
            "        [ 0.2772,  0.2766],\n",
            "        [-0.5166,  0.7074],\n",
            "        [-2.0001,  2.4222],\n",
            "        [-1.9998,  2.2762],\n",
            "        [-0.4462,  0.7118],\n",
            "        [-1.4675,  2.1443],\n",
            "        [ 0.3306,  0.3138],\n",
            "        [-2.0020,  2.3629],\n",
            "        [-1.2295,  1.6427],\n",
            "        [-2.0644,  2.3291],\n",
            "        [-1.3583,  1.8632],\n",
            "        [ 0.4528,  0.0424],\n",
            "        [-1.6575,  2.2977],\n",
            "        [-0.8244,  1.2545],\n",
            "        [-1.9542,  2.4823],\n",
            "        [-1.7043,  2.1904],\n",
            "        [ 0.8557, -0.7251],\n",
            "        [-1.1461,  1.6657],\n",
            "        [ 1.0353, -0.6580],\n",
            "        [-2.0494,  2.3663],\n",
            "        [-0.5223,  0.8094],\n",
            "        [ 0.7576, -0.4444],\n",
            "        [-1.7423,  2.2919],\n",
            "        [ 1.2528, -1.0281],\n",
            "        [-1.9991,  2.4289],\n",
            "        [-1.7771,  2.2898],\n",
            "        [-0.9242,  1.5271],\n",
            "        [-1.1954,  1.6451],\n",
            "        [-0.9764,  1.6134],\n",
            "        [ 0.1930,  0.4171],\n",
            "        [-0.1107,  0.5879]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.7589,  1.2528],\n",
            "        [-0.4122,  0.7170],\n",
            "        [-1.6078,  2.1079],\n",
            "        [-1.9237,  2.3138],\n",
            "        [-1.7367,  2.0893],\n",
            "        [ 0.8235, -0.3462],\n",
            "        [-1.6006,  1.9673],\n",
            "        [-1.8907,  2.2909],\n",
            "        [-1.3782,  1.8512],\n",
            "        [ 0.2475, -0.0781],\n",
            "        [-1.7043,  2.1860],\n",
            "        [-0.0097,  0.3951],\n",
            "        [-1.0458,  1.5771],\n",
            "        [-0.1908,  0.6113],\n",
            "        [-2.1185,  2.4306],\n",
            "        [-1.2489,  1.7042],\n",
            "        [-2.1412,  2.3777],\n",
            "        [-1.3499,  1.7880],\n",
            "        [-1.1020,  1.4292],\n",
            "        [ 0.2475,  0.0464],\n",
            "        [ 0.1924,  0.1758],\n",
            "        [-1.7854,  2.2439],\n",
            "        [-1.3436,  2.0212],\n",
            "        [-1.9220,  2.4364],\n",
            "        [ 0.2711,  0.3495],\n",
            "        [-1.3099,  1.8730],\n",
            "        [-1.8933,  2.3013],\n",
            "        [ 0.2660,  0.2504],\n",
            "        [-0.5204,  0.8924],\n",
            "        [-1.5684,  2.1460],\n",
            "        [-1.5439,  1.8867],\n",
            "        [-0.3459,  0.7624]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.7052,  2.2179],\n",
            "        [ 0.5102,  0.1470],\n",
            "        [-0.8609,  1.3987],\n",
            "        [-1.8018,  2.2003],\n",
            "        [-1.4903,  1.6962],\n",
            "        [ 0.7738, -0.3030],\n",
            "        [ 0.6053, -0.3984],\n",
            "        [-1.8304,  2.2976],\n",
            "        [-0.6101,  1.1470],\n",
            "        [-0.9161,  1.3663],\n",
            "        [ 0.6448, -0.3450],\n",
            "        [-1.1319,  1.6971],\n",
            "        [-1.8870,  2.3429],\n",
            "        [ 0.6711, -0.4369],\n",
            "        [ 0.9189, -0.6381],\n",
            "        [-1.7310,  2.2827],\n",
            "        [-1.7597,  2.1523],\n",
            "        [-1.1920,  1.7629],\n",
            "        [ 1.2028, -0.8967],\n",
            "        [-1.7517,  2.2935],\n",
            "        [-0.8400,  1.5879],\n",
            "        [-1.3566,  2.0800],\n",
            "        [ 0.9716, -0.6249],\n",
            "        [-1.0480,  1.5002],\n",
            "        [ 1.4493, -1.6973],\n",
            "        [-1.8536,  2.2791],\n",
            "        [ 0.0433,  0.4457],\n",
            "        [-0.3727,  0.8192],\n",
            "        [ 0.5286, -0.3407],\n",
            "        [-1.2779,  1.4993],\n",
            "        [-0.7727,  1.1551],\n",
            "        [ 1.6388, -2.0531]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.5898,  2.2217],\n",
            "        [-1.5275,  2.0951],\n",
            "        [-1.6091,  2.3143],\n",
            "        [ 1.4882, -1.8402],\n",
            "        [-1.2038,  1.5429],\n",
            "        [-0.5846,  0.9420],\n",
            "        [ 1.5318, -1.8343],\n",
            "        [-1.5269,  1.9855],\n",
            "        [-1.6372,  2.0548],\n",
            "        [-1.8589,  2.3957],\n",
            "        [-1.8858,  2.4160],\n",
            "        [-0.5808,  1.2368],\n",
            "        [-1.9600,  2.2423],\n",
            "        [-0.4071,  1.0025],\n",
            "        [ 0.8742, -0.4903],\n",
            "        [ 1.6111, -1.8426],\n",
            "        [-1.4753,  2.0700],\n",
            "        [ 1.3802, -1.3267],\n",
            "        [-1.2636,  1.4599],\n",
            "        [-1.1664,  1.9668],\n",
            "        [-0.9877,  1.5319],\n",
            "        [-1.6354,  1.9173],\n",
            "        [-1.7795,  2.1944],\n",
            "        [-2.1293,  2.4031],\n",
            "        [-0.8631,  1.2366],\n",
            "        [-1.0460,  1.3340],\n",
            "        [-1.5509,  1.8762],\n",
            "        [-0.6499,  1.3630],\n",
            "        [ 1.3068, -1.1039],\n",
            "        [-2.0144,  2.3136],\n",
            "        [ 1.5509, -1.7121],\n",
            "        [ 0.5264, -0.1170]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.9698,  1.2750],\n",
            "        [ 1.4806, -1.5328],\n",
            "        [ 1.0909, -0.6898],\n",
            "        [ 1.5222, -1.7423],\n",
            "        [ 0.9088, -0.5429],\n",
            "        [-0.8050,  1.1549],\n",
            "        [ 1.2058, -1.1042],\n",
            "        [-1.9131,  2.3558],\n",
            "        [ 1.0608, -0.8533],\n",
            "        [-1.4401,  1.9418],\n",
            "        [-0.7632,  1.1643],\n",
            "        [-1.9398,  2.4477],\n",
            "        [-1.7849,  2.1089],\n",
            "        [ 1.3651, -1.1271],\n",
            "        [-2.0391,  2.4234],\n",
            "        [-1.3984,  1.8137],\n",
            "        [-1.2721,  1.8157],\n",
            "        [-1.9580,  2.3633],\n",
            "        [-1.7631,  2.0939],\n",
            "        [-1.2911,  2.1401],\n",
            "        [-1.1973,  1.8646],\n",
            "        [ 0.6739, -0.3773],\n",
            "        [-1.4362,  1.9972],\n",
            "        [ 0.5011, -0.0152],\n",
            "        [ 0.7042, -0.2964],\n",
            "        [-2.0261,  2.2231],\n",
            "        [-0.9236,  1.2551],\n",
            "        [ 1.2797, -1.1927],\n",
            "        [ 0.9822, -0.7322],\n",
            "        [-1.2292,  1.9447],\n",
            "        [-0.5922,  1.2106],\n",
            "        [ 0.1527,  0.4161]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.8689,  2.1868],\n",
            "        [-0.9337,  1.3989],\n",
            "        [ 1.3476, -1.3227],\n",
            "        [-1.3625,  2.0137],\n",
            "        [ 0.7233, -0.3130],\n",
            "        [ 0.7545, -0.2845],\n",
            "        [-1.4985,  1.8521],\n",
            "        [-2.0630,  2.4690],\n",
            "        [ 0.1695,  0.2000],\n",
            "        [-0.5092,  0.7964],\n",
            "        [ 0.4612, -0.0100],\n",
            "        [ 0.2626,  0.4227],\n",
            "        [ 1.1652, -1.2373],\n",
            "        [-0.1253,  0.6335],\n",
            "        [-0.5754,  1.0070],\n",
            "        [-0.4486,  0.8260],\n",
            "        [-0.9006,  1.5692],\n",
            "        [ 0.7441, -0.4029],\n",
            "        [-1.4307,  1.8265],\n",
            "        [ 0.7670, -0.3585],\n",
            "        [ 0.7416, -0.2978],\n",
            "        [ 0.7792, -0.5663],\n",
            "        [ 1.6307, -1.8966],\n",
            "        [ 1.5144, -1.8474],\n",
            "        [-1.9730,  2.3979],\n",
            "        [-2.1527,  2.4875],\n",
            "        [-1.0969,  1.3926],\n",
            "        [-1.4487,  1.9330],\n",
            "        [-1.8639,  2.1912],\n",
            "        [ 0.0795,  0.1655],\n",
            "        [-0.2115,  0.7436],\n",
            "        [-0.0530,  0.4381]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.0591e+00,  1.3311e+00],\n",
            "        [-5.2116e-01,  1.1917e+00],\n",
            "        [-1.4873e+00,  2.0894e+00],\n",
            "        [-5.0255e-01,  1.0119e+00],\n",
            "        [-1.4498e+00,  1.6820e+00],\n",
            "        [-1.3106e+00,  1.9145e+00],\n",
            "        [-1.6640e+00,  2.2140e+00],\n",
            "        [-1.8727e+00,  2.2910e+00],\n",
            "        [-1.9735e+00,  2.2835e+00],\n",
            "        [-1.0372e+00,  1.5439e+00],\n",
            "        [-1.1549e+00,  1.7538e+00],\n",
            "        [-1.8911e+00,  2.3745e+00],\n",
            "        [-1.6457e+00,  2.2618e+00],\n",
            "        [ 1.2006e+00, -9.6787e-01],\n",
            "        [ 9.3777e-01, -8.6846e-01],\n",
            "        [-9.6446e-01,  1.1912e+00],\n",
            "        [-1.5707e+00,  1.9005e+00],\n",
            "        [ 1.1863e+00, -1.0692e+00],\n",
            "        [-1.5546e+00,  1.9814e+00],\n",
            "        [ 8.1403e-01, -3.6172e-01],\n",
            "        [-7.2010e-01,  1.2421e+00],\n",
            "        [-2.2996e-01,  7.1972e-01],\n",
            "        [ 1.2175e+00, -1.3294e+00],\n",
            "        [ 7.8044e-01, -3.1411e-01],\n",
            "        [-2.9108e-01,  7.6066e-01],\n",
            "        [-1.9535e+00,  2.4192e+00],\n",
            "        [-1.8875e+00,  2.4735e+00],\n",
            "        [-9.6547e-01,  1.3576e+00],\n",
            "        [ 8.0137e-01, -4.0700e-01],\n",
            "        [ 3.8144e-01,  2.2688e-04],\n",
            "        [ 1.5649e+00, -2.0230e+00],\n",
            "        [-1.6627e+00,  2.3158e+00]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.1193,  0.4572],\n",
            "        [-1.6339,  2.0383],\n",
            "        [-1.2852,  1.8902],\n",
            "        [-0.7138,  1.3131],\n",
            "        [-1.6982,  2.0858],\n",
            "        [-0.4556,  0.8877],\n",
            "        [-1.9069,  2.3368],\n",
            "        [-1.6951,  1.9947],\n",
            "        [ 1.2368, -0.9377],\n",
            "        [-1.7839,  2.3268],\n",
            "        [ 0.4546,  0.1060],\n",
            "        [-1.5646,  2.0384],\n",
            "        [ 0.6028, -0.2617],\n",
            "        [-1.0102,  1.3198],\n",
            "        [-1.0249,  1.7838],\n",
            "        [-1.8960,  2.4034],\n",
            "        [-0.8661,  1.4304],\n",
            "        [-2.0392,  2.3288],\n",
            "        [ 1.5344, -1.7624],\n",
            "        [ 0.6527, -0.5415],\n",
            "        [-0.2153,  0.7504],\n",
            "        [ 0.8917, -0.7211],\n",
            "        [-0.3548,  1.0387],\n",
            "        [-0.7285,  0.8980],\n",
            "        [ 0.8179, -0.4578],\n",
            "        [ 1.0644, -0.9846],\n",
            "        [-0.6891,  1.1928],\n",
            "        [ 1.1365, -0.6592],\n",
            "        [ 0.1473,  0.3811],\n",
            "        [-0.8182,  0.9627],\n",
            "        [-1.2037,  1.5281],\n",
            "        [-1.3450,  1.8332]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 1.1321, -0.8245],\n",
            "        [-0.6831,  1.1258],\n",
            "        [-1.8970,  2.3871],\n",
            "        [-1.8180,  2.1458],\n",
            "        [ 1.3795, -1.2724],\n",
            "        [ 1.1549, -0.7751],\n",
            "        [-0.4880,  1.0082],\n",
            "        [-0.2162,  0.6425],\n",
            "        [-1.5232,  2.0612],\n",
            "        [-1.5538,  1.8836],\n",
            "        [-0.9890,  1.5508],\n",
            "        [-1.5454,  1.9576],\n",
            "        [-1.6465,  2.0375],\n",
            "        [ 0.5002, -0.0668],\n",
            "        [ 1.1473, -1.0997],\n",
            "        [-0.9710,  1.7282],\n",
            "        [-0.8350,  1.2213],\n",
            "        [ 1.2032, -1.0409],\n",
            "        [-1.9084,  2.2527],\n",
            "        [-1.6497,  2.1471],\n",
            "        [-1.3755,  1.8044],\n",
            "        [-0.3808,  0.8418],\n",
            "        [ 0.3068,  0.3487],\n",
            "        [-1.6102,  2.2093],\n",
            "        [ 1.5675, -2.1113],\n",
            "        [ 0.8613, -0.2897],\n",
            "        [-0.1251,  0.6059],\n",
            "        [-0.6449,  1.3088],\n",
            "        [-0.6374,  1.1436],\n",
            "        [-1.5277,  2.1495],\n",
            "        [-0.9488,  1.4624],\n",
            "        [-1.1293,  1.7805]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.7016,  2.1030],\n",
            "        [-1.2512,  1.8217],\n",
            "        [-0.5782,  1.0885],\n",
            "        [-1.7997,  2.1114],\n",
            "        [-1.8218,  2.1813],\n",
            "        [-1.7772,  2.3008],\n",
            "        [-1.8777,  2.2714],\n",
            "        [-1.5050,  1.8564],\n",
            "        [ 0.6407, -0.1093],\n",
            "        [-1.4597,  2.0017],\n",
            "        [-1.7056,  2.2412],\n",
            "        [-1.1752,  1.6760],\n",
            "        [ 1.6620, -2.1872],\n",
            "        [-1.6686,  2.0906],\n",
            "        [-1.8256,  2.4555],\n",
            "        [-1.9226,  2.2451],\n",
            "        [-1.5729,  1.9931],\n",
            "        [-1.3452,  1.9222],\n",
            "        [ 1.6058, -1.9369],\n",
            "        [-1.8194,  2.1528],\n",
            "        [ 1.5833, -1.7863],\n",
            "        [ 0.5682, -0.4478],\n",
            "        [ 1.1515, -0.7964],\n",
            "        [-0.9123,  1.2890],\n",
            "        [-1.8010,  2.2179],\n",
            "        [ 1.5991, -2.0080],\n",
            "        [ 1.2468, -1.1009],\n",
            "        [ 0.5423, -0.0426],\n",
            "        [ 1.1142, -1.0849],\n",
            "        [-1.1016,  1.5924],\n",
            "        [-0.1636,  0.5380],\n",
            "        [-1.5609,  1.9932]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 1.4860, -1.6951],\n",
            "        [ 1.1017, -0.7878],\n",
            "        [-0.7369,  1.2378],\n",
            "        [-1.3741,  2.0575],\n",
            "        [-1.2978,  2.0310],\n",
            "        [-1.7145,  2.0634],\n",
            "        [-1.8241,  2.2393],\n",
            "        [ 0.1005,  0.4340],\n",
            "        [-1.3331,  2.0509],\n",
            "        [-1.3046,  1.7509],\n",
            "        [-1.6932,  2.1264],\n",
            "        [-1.3661,  1.7286],\n",
            "        [-0.4974,  0.7359],\n",
            "        [-0.6562,  1.1652],\n",
            "        [-1.0667,  1.5939],\n",
            "        [-1.9086,  2.3421],\n",
            "        [ 0.8110, -0.4157],\n",
            "        [ 0.5200, -0.1656],\n",
            "        [-2.0258,  2.4916],\n",
            "        [ 0.6586, -0.3058],\n",
            "        [-1.4878,  1.9589],\n",
            "        [-1.6142,  2.1197],\n",
            "        [ 0.1061,  0.1453],\n",
            "        [ 0.1566,  0.0461],\n",
            "        [-0.4596,  1.0064],\n",
            "        [-2.0268,  2.4673],\n",
            "        [-0.1642,  0.5870],\n",
            "        [-1.2676,  1.6263],\n",
            "        [-1.2369,  2.0439],\n",
            "        [-1.5545,  1.9281],\n",
            "        [-0.9880,  1.5205],\n",
            "        [ 1.5366, -1.8413]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.2354,  1.6587],\n",
            "        [ 1.5193, -1.7494],\n",
            "        [-0.4119,  0.9680],\n",
            "        [-0.1968,  0.5239],\n",
            "        [-1.3454,  1.9115],\n",
            "        [-1.4472,  1.9285],\n",
            "        [-0.7882,  1.3103],\n",
            "        [-1.3271,  1.6247],\n",
            "        [ 0.5992, -0.2429],\n",
            "        [-0.5697,  1.0235],\n",
            "        [-0.0406,  0.4261],\n",
            "        [-0.2400,  0.7098],\n",
            "        [-0.8067,  1.3470],\n",
            "        [-0.7653,  1.2434],\n",
            "        [-1.8493,  2.4432],\n",
            "        [ 0.5275, -0.0123],\n",
            "        [-1.9208,  2.3775],\n",
            "        [-1.7926,  2.0933],\n",
            "        [-1.6175,  2.0619],\n",
            "        [-0.8710,  1.3904],\n",
            "        [-1.9768,  2.5094],\n",
            "        [-0.8321,  1.3814],\n",
            "        [-1.8872,  2.1928],\n",
            "        [-2.0065,  2.3793],\n",
            "        [-1.0680,  1.5030],\n",
            "        [-1.7879,  2.0576],\n",
            "        [-2.0199,  2.3881],\n",
            "        [-0.6031,  1.2622],\n",
            "        [ 0.8865, -0.3198],\n",
            "        [ 0.2833,  0.1888],\n",
            "        [ 1.4375, -1.2739],\n",
            "        [-1.5136,  1.9110]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.1230,  1.6548],\n",
            "        [-1.2279,  1.6770],\n",
            "        [ 1.0942, -0.6797],\n",
            "        [ 1.3947, -1.3818],\n",
            "        [-1.1852,  1.6916],\n",
            "        [ 0.4709,  0.0031],\n",
            "        [-1.0614,  1.4992],\n",
            "        [-0.9550,  1.4424],\n",
            "        [ 0.6455, -0.1655],\n",
            "        [ 0.5124, -0.0749],\n",
            "        [-0.3426,  0.6663],\n",
            "        [ 0.0341,  0.3498],\n",
            "        [-1.1765,  1.5879],\n",
            "        [-1.9439,  2.3233],\n",
            "        [ 1.4343, -1.7677],\n",
            "        [ 1.5925, -1.8735],\n",
            "        [ 1.5926, -1.9143],\n",
            "        [-1.5613,  1.9796],\n",
            "        [-0.0263,  0.6038],\n",
            "        [-0.4362,  0.9293],\n",
            "        [-0.8673,  1.4463],\n",
            "        [-1.0909,  1.7426],\n",
            "        [ 1.6049, -2.0361],\n",
            "        [-1.4418,  2.0033],\n",
            "        [-0.9116,  1.2685],\n",
            "        [ 0.3222,  0.2422],\n",
            "        [ 0.8718, -0.6426],\n",
            "        [-1.4247,  1.8328],\n",
            "        [-0.7643,  1.1302],\n",
            "        [ 0.2308,  0.3651],\n",
            "        [-1.5439,  2.0485],\n",
            "        [-1.7840,  2.3552]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.4098,  1.0216],\n",
            "        [-1.4314,  2.1181],\n",
            "        [-1.4294,  1.9372],\n",
            "        [-1.9502,  2.4199],\n",
            "        [ 0.1211,  0.2460],\n",
            "        [-0.4309,  1.0519],\n",
            "        [-0.5885,  1.0129],\n",
            "        [-1.3325,  1.9209],\n",
            "        [-0.6029,  1.0430],\n",
            "        [-0.5948,  1.0569],\n",
            "        [-0.8078,  1.3202],\n",
            "        [-0.2997,  0.6472],\n",
            "        [ 0.5957, -0.2004],\n",
            "        [-0.1614,  0.4317],\n",
            "        [-1.8112,  2.1868],\n",
            "        [-1.6741,  2.1002],\n",
            "        [-1.1348,  1.7322],\n",
            "        [ 1.1185, -0.6807],\n",
            "        [ 1.3680, -1.6665],\n",
            "        [-0.9037,  1.4809],\n",
            "        [-1.7563,  2.2223],\n",
            "        [-0.1458,  0.5451],\n",
            "        [-0.8769,  1.4428],\n",
            "        [ 0.7777, -0.2496],\n",
            "        [-0.2440,  0.8080],\n",
            "        [-1.7318,  1.9960],\n",
            "        [-0.0506,  0.4014],\n",
            "        [-1.9931,  2.3258],\n",
            "        [-1.4769,  2.0195],\n",
            "        [ 1.1119, -0.7569],\n",
            "        [ 1.1822, -0.9316],\n",
            "        [-1.7348,  2.3178]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 0.6597, -0.1571],\n",
            "        [-1.0565,  1.4816],\n",
            "        [-1.7140,  2.3792],\n",
            "        [-0.8406,  1.2484],\n",
            "        [-1.4794,  1.9371],\n",
            "        [-1.4491,  2.2884],\n",
            "        [-0.7454,  1.2424],\n",
            "        [ 0.6980, -0.2865],\n",
            "        [-0.4989,  0.7883],\n",
            "        [ 0.2783,  0.0893],\n",
            "        [-1.7204,  2.0720],\n",
            "        [-2.1706,  2.4856],\n",
            "        [-1.9102,  2.4728],\n",
            "        [-1.9199,  2.3835],\n",
            "        [-0.2729,  0.8446],\n",
            "        [-0.9103,  1.1905],\n",
            "        [ 1.7296, -2.0200],\n",
            "        [ 1.2004, -0.9150],\n",
            "        [ 0.0241,  0.4846],\n",
            "        [-0.6656,  1.1517],\n",
            "        [-1.1742,  1.6288],\n",
            "        [-0.3657,  0.8606],\n",
            "        [-0.5790,  0.9689],\n",
            "        [-0.0467,  0.5133],\n",
            "        [ 1.4855, -1.5472],\n",
            "        [-1.1938,  1.6715],\n",
            "        [-1.6708,  2.0940],\n",
            "        [-1.8333,  2.3624],\n",
            "        [ 0.3450,  0.1594],\n",
            "        [-2.1068,  2.4366],\n",
            "        [ 1.1065, -1.3016],\n",
            "        [ 0.7961, -0.3287]], device='cuda:0'), hidden_states=None, attentions=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  50%|█████     | 2/4 [05:07<05:08, 154.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 1.4409, -1.3705],\n",
            "        [-0.9536,  1.5571],\n",
            "        [-1.7485,  2.1416],\n",
            "        [ 0.6941, -0.2531],\n",
            "        [ 1.6136, -1.8456],\n",
            "        [ 0.3944,  0.0801],\n",
            "        [-1.8078,  2.2860],\n",
            "        [-0.8091,  1.1802],\n",
            "        [-1.9600,  2.3765],\n",
            "        [-1.1891,  1.7148],\n",
            "        [ 1.1359, -0.7718],\n",
            "        [-0.0906,  0.4755],\n",
            "        [ 0.5353, -0.3116],\n",
            "        [-0.4803,  0.8572],\n",
            "        [ 1.0858, -0.9816],\n",
            "        [-0.0922,  0.4539],\n",
            "        [-0.6964,  0.8994],\n",
            "        [-1.5190,  2.0106],\n",
            "        [-1.5982,  2.1146],\n",
            "        [ 0.0954,  0.5935],\n",
            "        [-1.1666,  1.5448],\n",
            "        [-1.8083,  2.1948],\n",
            "        [-1.4846,  1.9041],\n",
            "        [-0.8822,  1.2694],\n",
            "        [ 0.7637, -0.2794],\n",
            "        [-0.4529,  0.8076],\n",
            "        [-1.9401,  2.4189],\n",
            "        [ 1.0478, -0.9718],\n",
            "        [ 0.3527,  0.2073],\n",
            "        [ 0.8536, -0.1278],\n",
            "        [-1.5640,  2.3004],\n",
            "        [-1.6361,  2.2321]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.0685,  2.3178],\n",
            "        [ 1.0188, -0.8669],\n",
            "        [-1.0692,  1.5304],\n",
            "        [ 0.7708, -0.2642],\n",
            "        [-1.9078,  2.3543],\n",
            "        [-1.7350,  2.2646],\n",
            "        [ 0.4010,  0.2820],\n",
            "        [-1.5188,  2.0481],\n",
            "        [ 0.7132, -0.3214],\n",
            "        [-1.3884,  1.9163],\n",
            "        [-1.7010,  2.2398],\n",
            "        [ 0.2721, -0.2002],\n",
            "        [-1.4799,  1.9763],\n",
            "        [-1.0130,  1.5765],\n",
            "        [-0.2667,  0.5703],\n",
            "        [-1.2074,  1.6994],\n",
            "        [ 0.7106, -0.2633],\n",
            "        [ 1.2014, -0.9237],\n",
            "        [-0.5972,  0.9596],\n",
            "        [-1.9949,  2.4525],\n",
            "        [-0.9216,  1.5060],\n",
            "        [-0.7571,  0.8863],\n",
            "        [-1.1957,  1.7436],\n",
            "        [-1.8553,  1.9944]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Accuracy: 0.8225308641975309\n",
            "Train loss: 0.18713086798663456\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1292, -2.4592],\n",
            "        [-1.2928,  1.8176],\n",
            "        [-1.5400,  2.1049],\n",
            "        [-2.1797,  2.5001],\n",
            "        [-1.2460,  1.4399],\n",
            "        [ 0.6790, -0.4609],\n",
            "        [-2.4094,  2.7950],\n",
            "        [ 1.2968, -1.6369],\n",
            "        [ 1.9156, -2.6508],\n",
            "        [-1.1660,  1.2799],\n",
            "        [-1.4153,  1.8987],\n",
            "        [-2.1145,  2.3920],\n",
            "        [-2.3968,  2.7792],\n",
            "        [-0.5974,  1.0861],\n",
            "        [-2.3526,  2.8144],\n",
            "        [-0.4008,  0.7625],\n",
            "        [ 0.5641, -0.2366],\n",
            "        [-1.6699,  2.3178],\n",
            "        [ 1.1401, -0.6541],\n",
            "        [-1.5044,  1.9815],\n",
            "        [-2.4567,  2.7793],\n",
            "        [-1.8273,  2.4033],\n",
            "        [-2.2834,  2.8134],\n",
            "        [-2.0551,  2.7316],\n",
            "        [ 1.8070, -2.5856],\n",
            "        [-1.1339,  1.8781],\n",
            "        [-1.9450,  2.4176],\n",
            "        [-2.3014,  2.8272],\n",
            "        [-0.7311,  1.1442],\n",
            "        [-2.4581,  2.8746],\n",
            "        [-1.6371,  2.1885],\n",
            "        [-2.3799,  2.7831]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.3052,  2.6769],\n",
            "        [-1.6769,  2.2486],\n",
            "        [-2.4236,  2.6777],\n",
            "        [-2.3071,  2.8359],\n",
            "        [-2.3454,  2.7539],\n",
            "        [ 0.0445,  0.1985],\n",
            "        [-2.3217,  2.5334],\n",
            "        [-1.6937,  2.2469],\n",
            "        [ 1.4004, -0.8770],\n",
            "        [-2.4680,  2.8122],\n",
            "        [-2.3319,  2.6557],\n",
            "        [-1.9328,  2.2818],\n",
            "        [-0.4968,  0.7349],\n",
            "        [-2.4119,  2.7826],\n",
            "        [-1.3191,  1.9646],\n",
            "        [-2.1260,  2.6241],\n",
            "        [-2.1410,  2.6481],\n",
            "        [-2.3255,  2.7037],\n",
            "        [-1.3853,  1.8590],\n",
            "        [-2.0893,  2.4991],\n",
            "        [-1.5486,  2.1405],\n",
            "        [-0.5504,  0.8518],\n",
            "        [ 0.0954,  0.3549],\n",
            "        [-1.9469,  2.4249],\n",
            "        [-2.4029,  2.8818],\n",
            "        [-0.9787,  1.6153],\n",
            "        [-2.4261,  2.8895],\n",
            "        [-2.3824,  2.9592],\n",
            "        [-1.4439,  1.7189],\n",
            "        [-2.1011,  2.4491],\n",
            "        [-2.4879,  2.7832],\n",
            "        [ 0.1884,  0.1126]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.4411,  2.7480],\n",
            "        [-1.9978,  2.4558],\n",
            "        [ 1.9613, -2.3510],\n",
            "        [-2.4195,  2.9028],\n",
            "        [-1.1925,  1.9226],\n",
            "        [-1.8835,  2.5303],\n",
            "        [-2.2944,  2.6337],\n",
            "        [ 0.2169,  0.4189],\n",
            "        [-2.1430,  2.4905],\n",
            "        [-2.0133,  2.6249],\n",
            "        [-2.1668,  2.6498],\n",
            "        [-2.3874,  2.6553],\n",
            "        [-1.3998,  1.9388],\n",
            "        [-1.1833,  1.6149],\n",
            "        [-1.6264,  2.0935],\n",
            "        [-2.4510,  2.9009],\n",
            "        [-2.1806,  2.5625],\n",
            "        [-2.5082,  2.7489],\n",
            "        [-2.0309,  2.5400],\n",
            "        [-2.2897,  2.7161],\n",
            "        [-2.1429,  2.7705],\n",
            "        [ 1.5457, -1.5213],\n",
            "        [-1.8972,  2.3780],\n",
            "        [-1.7727,  2.3034],\n",
            "        [-1.1949,  1.5984],\n",
            "        [-1.5415,  2.1984],\n",
            "        [ 1.8169, -2.5896],\n",
            "        [ 1.6543, -1.7278],\n",
            "        [ 1.7083, -1.7424],\n",
            "        [ 1.3404, -1.4978],\n",
            "        [ 1.4504, -2.2558],\n",
            "        [ 1.4994, -1.3386]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.2214,  2.8039],\n",
            "        [ 1.7779, -2.2142],\n",
            "        [ 1.5257, -1.8000],\n",
            "        [-2.0653,  2.3092],\n",
            "        [-0.0496,  0.1922],\n",
            "        [ 1.5287, -1.4845],\n",
            "        [-1.7546,  2.3640],\n",
            "        [-1.4572,  1.6608],\n",
            "        [-0.9692,  1.7070],\n",
            "        [-0.6753,  1.1594],\n",
            "        [ 0.6966, -0.5128],\n",
            "        [-0.2371,  0.3602],\n",
            "        [ 1.7710, -1.8279],\n",
            "        [-2.3936,  2.8742],\n",
            "        [-1.3722,  1.7416],\n",
            "        [-2.1685,  2.6679],\n",
            "        [-2.3271,  2.7360],\n",
            "        [-2.2202,  2.6714],\n",
            "        [ 1.6064, -2.0278],\n",
            "        [-2.1134,  2.6947],\n",
            "        [ 1.9181, -2.4478],\n",
            "        [-1.2383,  1.5681],\n",
            "        [-1.5337,  2.1978],\n",
            "        [-2.4261,  2.7759],\n",
            "        [-2.3088,  2.7858],\n",
            "        [-1.8695,  2.0713],\n",
            "        [-1.6650,  2.2644],\n",
            "        [-2.2495,  2.5998],\n",
            "        [-0.2331,  0.0734],\n",
            "        [ 1.3501, -1.2812],\n",
            "        [-2.4591,  2.8176],\n",
            "        [-2.1362,  2.7200]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.7524,  2.3391],\n",
            "        [-1.9123,  2.5342],\n",
            "        [-0.7236,  1.1287],\n",
            "        [-2.4095,  2.8345],\n",
            "        [-2.2826,  2.6797],\n",
            "        [ 0.8865, -0.7263],\n",
            "        [-2.2429,  2.7224],\n",
            "        [-2.3999,  2.8800],\n",
            "        [-2.4107,  2.8477],\n",
            "        [-2.0221,  2.6704],\n",
            "        [-2.3373,  2.6153],\n",
            "        [-1.6773,  2.0318],\n",
            "        [ 1.5702, -1.5995],\n",
            "        [-2.2479,  2.5000],\n",
            "        [-2.3133,  2.6681],\n",
            "        [-2.4026,  2.7876],\n",
            "        [-2.1974,  2.6593],\n",
            "        [ 1.9327, -2.0827],\n",
            "        [ 1.7119, -2.2110],\n",
            "        [-1.0812,  1.6113],\n",
            "        [-2.1551,  2.7128],\n",
            "        [-0.7122,  1.2882],\n",
            "        [ 1.5731, -1.3368],\n",
            "        [ 1.6811, -1.8738],\n",
            "        [-2.3049,  2.7866],\n",
            "        [ 0.1844,  0.0265],\n",
            "        [ 1.2318, -0.9058],\n",
            "        [-2.2965,  2.5749],\n",
            "        [-1.8118,  2.2193],\n",
            "        [-2.4811,  2.8373],\n",
            "        [-2.3711,  2.6991],\n",
            "        [ 0.0407,  0.1622]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 0.8996, -0.6266],\n",
            "        [ 2.0013, -2.5211],\n",
            "        [-0.9374,  1.3944],\n",
            "        [-1.9492,  2.2792],\n",
            "        [-0.6589,  0.9842],\n",
            "        [-2.3482,  2.6410],\n",
            "        [-1.6424,  1.9746],\n",
            "        [-2.0643,  2.5563],\n",
            "        [-2.1855,  2.6312],\n",
            "        [ 1.6201, -1.7190],\n",
            "        [-2.2159,  2.8072],\n",
            "        [-2.2535,  2.8396],\n",
            "        [-2.3752,  2.8061],\n",
            "        [-2.2240,  2.5743],\n",
            "        [-2.2068,  2.8097],\n",
            "        [-2.1873,  2.6235],\n",
            "        [-1.5176,  2.0509],\n",
            "        [-2.2578,  2.6036],\n",
            "        [ 0.2553,  0.1640],\n",
            "        [ 1.9957, -2.5628],\n",
            "        [-0.2247,  0.7078],\n",
            "        [-2.1506,  2.3915],\n",
            "        [ 0.5454, -0.0657],\n",
            "        [-0.0623,  0.5984],\n",
            "        [-2.2569,  2.5642],\n",
            "        [-1.4152,  2.0103],\n",
            "        [-1.9653,  2.4912],\n",
            "        [-1.8354,  2.4025],\n",
            "        [-1.3423,  1.7577],\n",
            "        [ 0.1813,  0.1014],\n",
            "        [-0.3271,  0.5805],\n",
            "        [-0.0039,  0.0668]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.7223,  2.5543],\n",
            "        [-1.8019,  2.4335],\n",
            "        [-2.1382,  2.5541],\n",
            "        [-1.8460,  2.7076],\n",
            "        [ 1.5349, -2.0089],\n",
            "        [-2.3854,  2.8683],\n",
            "        [ 1.9641, -2.2782],\n",
            "        [-1.1731,  1.4165],\n",
            "        [ 1.1635, -1.1417],\n",
            "        [-2.4889,  2.8020],\n",
            "        [ 0.9580, -0.7791],\n",
            "        [-2.3000,  2.8350],\n",
            "        [-2.4052,  2.8596],\n",
            "        [ 0.4601, -0.0331],\n",
            "        [-2.1664,  2.7477],\n",
            "        [ 1.8795, -2.1328],\n",
            "        [-2.0467,  2.5520],\n",
            "        [ 2.0810, -2.5817],\n",
            "        [-1.9160,  2.3004],\n",
            "        [-2.0068,  2.5670],\n",
            "        [-2.1302,  2.7062],\n",
            "        [-1.9556,  2.5187],\n",
            "        [-0.8799,  1.1163],\n",
            "        [ 0.7371, -0.2643],\n",
            "        [-1.4671,  2.1062],\n",
            "        [-2.0441,  2.5490],\n",
            "        [-2.4700,  2.8512],\n",
            "        [-2.3988,  2.7688],\n",
            "        [-2.0343,  2.6782],\n",
            "        [-2.4432,  2.8845],\n",
            "        [-2.4022,  2.7636],\n",
            "        [-2.1211,  2.6923]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.2833,  2.7212],\n",
            "        [-1.1112,  1.7725],\n",
            "        [-0.9977,  1.4633],\n",
            "        [-1.6041,  1.9757],\n",
            "        [ 1.9593, -2.6371],\n",
            "        [ 1.8567, -2.3220],\n",
            "        [ 1.1440, -0.9062],\n",
            "        [ 0.2465,  0.2352],\n",
            "        [-1.5163,  2.1769],\n",
            "        [ 1.1338, -1.4039],\n",
            "        [-0.5243,  0.6355],\n",
            "        [-2.4455,  2.7993],\n",
            "        [ 1.8065, -2.4873],\n",
            "        [-1.9989,  2.5811],\n",
            "        [-1.6022,  2.1092],\n",
            "        [-0.2312,  0.4110],\n",
            "        [ 1.3462, -2.0679],\n",
            "        [-0.5891,  1.1868],\n",
            "        [-2.3732,  2.8643],\n",
            "        [ 0.5587, -0.5017],\n",
            "        [-2.1148,  2.5136],\n",
            "        [-0.9439,  1.4911],\n",
            "        [ 0.4137,  0.2695],\n",
            "        [ 0.8566, -0.4176],\n",
            "        [-1.5680,  1.9554],\n",
            "        [ 1.9709, -2.2630],\n",
            "        [-2.2858,  2.7929],\n",
            "        [-1.5087,  2.3879],\n",
            "        [-0.3904,  0.8202],\n",
            "        [-1.8070,  2.1771],\n",
            "        [-0.6819,  0.9968],\n",
            "        [-2.3164,  2.7204]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.3568,  2.7146],\n",
            "        [-1.8568,  2.5270],\n",
            "        [-2.2727,  2.7429],\n",
            "        [-2.3499,  2.8699],\n",
            "        [-2.1401,  2.7646],\n",
            "        [-2.4088,  2.8064],\n",
            "        [ 1.8193, -2.5412],\n",
            "        [-2.1398,  2.7441],\n",
            "        [-0.3833,  0.9709],\n",
            "        [-1.0561,  1.5748],\n",
            "        [-2.3583,  2.7342],\n",
            "        [-2.1600,  2.7348],\n",
            "        [ 1.6981, -1.9504],\n",
            "        [-2.3631,  2.6493],\n",
            "        [-1.2565,  1.8842],\n",
            "        [-1.9173,  2.5100],\n",
            "        [-2.4723,  2.8407],\n",
            "        [-2.4705,  2.9245],\n",
            "        [-1.2506,  1.6119],\n",
            "        [ 1.4240, -1.5013],\n",
            "        [-1.9291,  2.2100],\n",
            "        [ 1.0525, -1.0088],\n",
            "        [-1.8011,  2.3666],\n",
            "        [ 1.8748, -2.0210],\n",
            "        [-0.4275,  0.6874],\n",
            "        [ 0.4208, -0.1540],\n",
            "        [-1.9899,  2.2994],\n",
            "        [-2.4146,  2.8051],\n",
            "        [-1.8333,  2.3836],\n",
            "        [-1.0923,  1.3779],\n",
            "        [-2.3838,  2.9094],\n",
            "        [-1.0713,  1.5945]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.2188,  1.6418],\n",
            "        [-2.1009,  2.7321],\n",
            "        [ 0.7828, -0.3064],\n",
            "        [ 1.5886, -1.8107],\n",
            "        [ 1.3875, -1.5393],\n",
            "        [-1.6133,  1.8485],\n",
            "        [-2.2639,  2.8375],\n",
            "        [-2.2971,  2.8197],\n",
            "        [-2.4080,  2.8038],\n",
            "        [ 1.4651, -1.6830],\n",
            "        [ 1.0009, -0.8723],\n",
            "        [-1.1036,  1.7360],\n",
            "        [-0.4088,  1.0686],\n",
            "        [-2.0175,  2.3677],\n",
            "        [-1.6270,  1.9469],\n",
            "        [-2.4918,  2.9432],\n",
            "        [-0.0959,  0.2315],\n",
            "        [-2.4672,  2.7674],\n",
            "        [-2.2558,  2.7738],\n",
            "        [-0.9569,  1.4374],\n",
            "        [-0.9654,  1.0609],\n",
            "        [-2.3432,  2.8226],\n",
            "        [-0.5230,  0.7128],\n",
            "        [-2.0454,  2.3696],\n",
            "        [-1.3297,  1.6544],\n",
            "        [ 0.0331,  0.3759],\n",
            "        [-1.7545,  2.3751],\n",
            "        [ 1.7779, -2.5905],\n",
            "        [-1.4684,  1.8762],\n",
            "        [-1.8773,  2.2937],\n",
            "        [-1.2972,  1.8302],\n",
            "        [-2.1827,  2.6509]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.8750,  2.4023],\n",
            "        [-0.6477,  1.1317],\n",
            "        [-1.1249,  1.2391],\n",
            "        [-2.4112,  2.8101],\n",
            "        [-2.3639,  2.6020],\n",
            "        [-0.6511,  0.8741],\n",
            "        [-2.0838,  2.8332],\n",
            "        [ 0.4866,  0.1597],\n",
            "        [-2.5014,  2.8688],\n",
            "        [-1.7318,  2.1306],\n",
            "        [-2.3980,  2.7003],\n",
            "        [-2.0817,  2.5841],\n",
            "        [ 0.7416, -0.2645],\n",
            "        [-2.2479,  2.7844],\n",
            "        [-1.2910,  1.6740],\n",
            "        [-2.3816,  2.8518],\n",
            "        [-2.2252,  2.6440],\n",
            "        [ 0.7711, -1.0176],\n",
            "        [-1.7697,  2.3596],\n",
            "        [ 1.5623, -1.4850],\n",
            "        [-2.4001,  2.7468],\n",
            "        [-0.4952,  0.3814],\n",
            "        [ 1.0934, -0.9434],\n",
            "        [-2.3007,  2.8409],\n",
            "        [ 1.4392, -1.3222],\n",
            "        [-2.4657,  2.8696],\n",
            "        [-2.2998,  2.7939],\n",
            "        [-1.7762,  2.4714],\n",
            "        [-1.9312,  2.3251],\n",
            "        [-1.4331,  2.0918],\n",
            "        [ 0.9613, -0.5493],\n",
            "        [-0.3617,  0.8192]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.3572,  1.8875],\n",
            "        [-1.1399,  1.3598],\n",
            "        [-2.1047,  2.5247],\n",
            "        [-2.4298,  2.8350],\n",
            "        [-2.2979,  2.6352],\n",
            "        [ 1.4675, -1.2622],\n",
            "        [-2.1016,  2.5445],\n",
            "        [-2.3220,  2.6557],\n",
            "        [-2.1101,  2.5420],\n",
            "        [ 0.3538, -0.6171],\n",
            "        [-2.3914,  2.8251],\n",
            "        [-0.2471,  0.5822],\n",
            "        [-1.5236,  2.0667],\n",
            "        [-0.7190,  1.0355],\n",
            "        [-2.4012,  2.7234],\n",
            "        [-1.7413,  2.1775],\n",
            "        [-2.5531,  2.7784],\n",
            "        [-2.0184,  2.4815],\n",
            "        [-1.8648,  2.2087],\n",
            "        [-0.4761,  0.7614],\n",
            "        [ 0.7676, -0.4942],\n",
            "        [-2.3326,  2.7700],\n",
            "        [-1.9310,  2.5307],\n",
            "        [-2.3728,  2.8858],\n",
            "        [ 0.5906, -0.0118],\n",
            "        [-1.9878,  2.5424],\n",
            "        [-2.4421,  2.8176],\n",
            "        [ 1.6003, -1.8697],\n",
            "        [-0.8460,  1.1686],\n",
            "        [-2.2242,  2.6967],\n",
            "        [-2.2686,  2.6281],\n",
            "        [-1.7026,  2.2015]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.2226,  2.6645],\n",
            "        [ 0.8401, -0.2418],\n",
            "        [-0.8827,  1.4695],\n",
            "        [-2.3755,  2.7625],\n",
            "        [-2.0098,  2.2102],\n",
            "        [ 0.8508, -0.4357],\n",
            "        [ 0.9896, -1.0714],\n",
            "        [-2.3845,  2.7607],\n",
            "        [-1.1214,  1.5484],\n",
            "        [-1.7554,  2.2126],\n",
            "        [ 0.8930, -0.7830],\n",
            "        [-1.8817,  2.4561],\n",
            "        [-2.3941,  2.8635],\n",
            "        [ 0.3727, -0.7344],\n",
            "        [ 1.2726, -1.1235],\n",
            "        [-2.4503,  2.8848],\n",
            "        [-2.3976,  2.7067],\n",
            "        [-1.9133,  2.4156],\n",
            "        [ 1.8915, -2.2765],\n",
            "        [-2.3425,  2.8557],\n",
            "        [-1.2323,  2.0082],\n",
            "        [-2.0433,  2.7579],\n",
            "        [ 1.0148, -0.7791],\n",
            "        [-1.6235,  2.0535],\n",
            "        [ 1.7053, -2.4465],\n",
            "        [-2.3373,  2.7367],\n",
            "        [-1.6940,  2.2374],\n",
            "        [-0.9034,  1.2338],\n",
            "        [ 1.1215, -1.3686],\n",
            "        [-1.4392,  1.6424],\n",
            "        [-1.1585,  1.5184],\n",
            "        [ 1.8804, -2.6321]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.2460,  2.8573],\n",
            "        [-2.1503,  2.6910],\n",
            "        [-2.1459,  2.8438],\n",
            "        [ 1.9567, -2.6542],\n",
            "        [-1.7125,  2.0584],\n",
            "        [-1.2672,  1.5636],\n",
            "        [ 1.9654, -2.6519],\n",
            "        [-1.6422,  2.1510],\n",
            "        [-2.3032,  2.6363],\n",
            "        [-2.3436,  2.8467],\n",
            "        [-2.3781,  2.8714],\n",
            "        [-1.5367,  2.2826],\n",
            "        [-2.4927,  2.7492],\n",
            "        [-0.1663,  0.6845],\n",
            "        [ 1.0929, -0.8503],\n",
            "        [ 1.9309, -2.6291],\n",
            "        [-2.2090,  2.7720],\n",
            "        [ 1.5770, -1.6069],\n",
            "        [-1.8804,  2.1332],\n",
            "        [-1.8274,  2.6421],\n",
            "        [-1.7412,  2.2505],\n",
            "        [-2.0465,  2.2994],\n",
            "        [-2.3146,  2.7105],\n",
            "        [-2.4329,  2.7314],\n",
            "        [-2.4161,  2.7283],\n",
            "        [-1.8965,  2.2069],\n",
            "        [-1.8226,  2.1174],\n",
            "        [-1.0057,  1.6987],\n",
            "        [ 1.7167, -1.8982],\n",
            "        [-2.4442,  2.7337],\n",
            "        [ 2.0563, -2.6148],\n",
            "        [ 0.5121, -0.0953]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.4371,  1.7354],\n",
            "        [ 2.0235, -2.4092],\n",
            "        [ 1.0562, -0.6998],\n",
            "        [ 1.9473, -2.4625],\n",
            "        [ 1.0446, -0.8376],\n",
            "        [ 0.9367, -1.0152],\n",
            "        [ 1.6178, -1.8319],\n",
            "        [-2.3541,  2.8387],\n",
            "        [ 1.8033, -2.0428],\n",
            "        [-2.1836,  2.6442],\n",
            "        [-1.4714,  1.8412],\n",
            "        [-2.3913,  2.9261],\n",
            "        [-2.2578,  2.6310],\n",
            "        [ 1.9454, -2.1006],\n",
            "        [-2.4305,  2.8242],\n",
            "        [-2.0438,  2.3959],\n",
            "        [-1.9168,  2.5241],\n",
            "        [-2.4225,  2.8015],\n",
            "        [-2.3493,  2.6683],\n",
            "        [-1.7602,  2.6304],\n",
            "        [-1.9462,  2.6987],\n",
            "        [ 0.6369, -0.4574],\n",
            "        [-1.9828,  2.5624],\n",
            "        [ 0.2810,  0.1673],\n",
            "        [ 0.6053, -0.3470],\n",
            "        [-2.4725,  2.6805],\n",
            "        [-1.5764,  1.8584],\n",
            "        [ 1.6447, -1.6694],\n",
            "        [ 1.5485, -1.8669],\n",
            "        [-2.0131,  2.6900],\n",
            "        [-0.6138,  1.2586],\n",
            "        [-0.1277,  0.6742]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.4246,  2.6780],\n",
            "        [-1.6013,  2.0816],\n",
            "        [ 1.7202, -1.9879],\n",
            "        [-1.9303,  2.5772],\n",
            "        [ 1.1275, -0.8380],\n",
            "        [ 1.2232, -0.8937],\n",
            "        [-2.2347,  2.5571],\n",
            "        [-2.4479,  2.8476],\n",
            "        [-1.4611,  1.7734],\n",
            "        [-1.4584,  1.7249],\n",
            "        [ 0.1313,  0.3974],\n",
            "        [ 0.5529,  0.1518],\n",
            "        [ 1.6789, -2.2325],\n",
            "        [-1.7837,  2.3423],\n",
            "        [-1.3571,  1.7242],\n",
            "        [-1.1896,  1.6037],\n",
            "        [-1.4732,  2.1876],\n",
            "        [ 1.2475, -1.0038],\n",
            "        [-1.8676,  2.3241],\n",
            "        [ 0.6069, -0.3060],\n",
            "        [ 1.1844, -0.9319],\n",
            "        [ 0.8746, -0.9960],\n",
            "        [ 1.9758, -2.5581],\n",
            "        [ 1.8906, -2.3607],\n",
            "        [-2.3857,  2.8155],\n",
            "        [-2.5328,  2.9013],\n",
            "        [-2.0319,  2.3759],\n",
            "        [-2.0963,  2.5249],\n",
            "        [-2.3631,  2.7156],\n",
            "        [-0.5331,  0.6305],\n",
            "        [-0.4484,  0.8952],\n",
            "        [ 0.2413,  0.1360]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.1708,  2.4347],\n",
            "        [-1.2982,  2.0286],\n",
            "        [-2.1751,  2.7675],\n",
            "        [-0.6981,  1.1591],\n",
            "        [-1.5732,  1.7666],\n",
            "        [-1.8226,  2.5221],\n",
            "        [-2.2498,  2.7280],\n",
            "        [-2.4266,  2.8407],\n",
            "        [-2.4620,  2.7878],\n",
            "        [-1.7378,  2.2655],\n",
            "        [-1.8666,  2.4985],\n",
            "        [-2.3173,  2.7638],\n",
            "        [-2.2850,  2.7801],\n",
            "        [ 1.7227, -1.7778],\n",
            "        [ 1.0578, -1.3301],\n",
            "        [-1.8474,  2.0928],\n",
            "        [-2.2101,  2.4917],\n",
            "        [ 1.7267, -2.0241],\n",
            "        [-2.2149,  2.6014],\n",
            "        [ 0.6974, -0.1382],\n",
            "        [-1.3022,  1.8305],\n",
            "        [-0.9042,  1.4814],\n",
            "        [ 0.6338, -0.7595],\n",
            "        [ 0.7068, -0.3872],\n",
            "        [-1.8101,  2.4201],\n",
            "        [-2.3597,  2.7736],\n",
            "        [-2.2659,  2.8486],\n",
            "        [-0.5752,  1.0059],\n",
            "        [ 0.3971, -0.0464],\n",
            "        [-0.6986,  0.8737],\n",
            "        [ 1.9373, -2.6410],\n",
            "        [-2.1647,  2.7710]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.6598,  0.9603],\n",
            "        [-2.3121,  2.6996],\n",
            "        [-2.1068,  2.6757],\n",
            "        [-1.7222,  2.3638],\n",
            "        [-2.4334,  2.7370],\n",
            "        [-0.1406,  0.5893],\n",
            "        [-2.4416,  2.8396],\n",
            "        [-2.2547,  2.5235],\n",
            "        [ 1.9209, -2.2032],\n",
            "        [-2.3114,  2.8272],\n",
            "        [ 0.0298,  0.5072],\n",
            "        [-2.1138,  2.5861],\n",
            "        [ 0.3372, -0.0897],\n",
            "        [-2.0334,  2.3821],\n",
            "        [-1.8407,  2.5621],\n",
            "        [-2.3636,  2.8493],\n",
            "        [-1.6965,  2.3155],\n",
            "        [-2.4467,  2.7329],\n",
            "        [ 1.9090, -2.2741],\n",
            "        [ 1.2048, -1.4923],\n",
            "        [-1.1713,  1.6547],\n",
            "        [ 0.9543, -0.8821],\n",
            "        [-0.3361,  1.0555],\n",
            "        [-1.5313,  1.6250],\n",
            "        [ 0.8519, -0.6111],\n",
            "        [ 1.6156, -1.9689],\n",
            "        [-1.5461,  2.0657],\n",
            "        [ 1.2474, -0.8687],\n",
            "        [ 0.1492,  0.3437],\n",
            "        [-1.2869,  1.3486],\n",
            "        [-2.0655,  2.4223],\n",
            "        [-2.0253,  2.5070]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 1.3964, -1.2264],\n",
            "        [-1.5481,  2.0318],\n",
            "        [-2.3945,  2.8683],\n",
            "        [-2.3429,  2.6491],\n",
            "        [ 1.7674, -2.0610],\n",
            "        [ 1.7538, -1.9085],\n",
            "        [-0.7929,  1.3055],\n",
            "        [-0.3149,  0.6384],\n",
            "        [-2.2118,  2.6671],\n",
            "        [-2.1603,  2.5307],\n",
            "        [-1.6092,  2.1938],\n",
            "        [-0.9244,  1.2947],\n",
            "        [-2.3205,  2.6947],\n",
            "        [ 0.6497, -0.6087],\n",
            "        [ 1.7400, -2.0521],\n",
            "        [-1.5963,  2.3210],\n",
            "        [-1.9388,  2.3879],\n",
            "        [ 1.7981, -2.0453],\n",
            "        [-2.4418,  2.7453],\n",
            "        [-2.2289,  2.6409],\n",
            "        [-2.2948,  2.6769],\n",
            "        [-1.0607,  1.4749],\n",
            "        [ 0.2724,  0.3162],\n",
            "        [-2.0613,  2.6678],\n",
            "        [ 1.7093, -2.6486],\n",
            "        [ 1.0231, -0.5687],\n",
            "        [-0.7968,  1.3609],\n",
            "        [-1.5043,  2.2988],\n",
            "        [-1.3359,  1.9249],\n",
            "        [-2.1158,  2.6924],\n",
            "        [-1.7871,  2.3168],\n",
            "        [-1.8253,  2.4319]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.3397,  2.7444],\n",
            "        [-1.7180,  2.2990],\n",
            "        [-1.2202,  1.8419],\n",
            "        [-2.3609,  2.6997],\n",
            "        [-2.3427,  2.6502],\n",
            "        [-2.3310,  2.7760],\n",
            "        [-2.3720,  2.7075],\n",
            "        [-2.2731,  2.6704],\n",
            "        [ 0.6815, -0.1640],\n",
            "        [-1.9177,  2.4033],\n",
            "        [-2.2379,  2.7040],\n",
            "        [-2.0720,  2.5939],\n",
            "        [ 1.6771, -2.4812],\n",
            "        [-2.2223,  2.6148],\n",
            "        [-2.2949,  2.8842],\n",
            "        [-2.3585,  2.6747],\n",
            "        [-2.2853,  2.6424],\n",
            "        [-1.9738,  2.5179],\n",
            "        [ 1.9835, -2.6569],\n",
            "        [-2.2355,  2.5531],\n",
            "        [ 2.0564, -2.5905],\n",
            "        [ 1.3880, -1.8711],\n",
            "        [ 1.4569, -1.1010],\n",
            "        [-1.5920,  2.0108],\n",
            "        [-2.3028,  2.7744],\n",
            "        [ 1.8024, -2.6321],\n",
            "        [ 1.7002, -2.2396],\n",
            "        [ 0.1727,  0.2218],\n",
            "        [ 1.1293, -1.1648],\n",
            "        [-2.1390,  2.6045],\n",
            "        [-0.1372,  0.4065],\n",
            "        [-2.1544,  2.5315]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 1.8668, -2.3384],\n",
            "        [ 1.7656, -1.9747],\n",
            "        [-0.7517,  1.2777],\n",
            "        [-1.8149,  2.5210],\n",
            "        [-2.0757,  2.8021],\n",
            "        [-2.3254,  2.6420],\n",
            "        [-2.3556,  2.7003],\n",
            "        [-0.0056,  0.4880],\n",
            "        [-2.0287,  2.7195],\n",
            "        [-1.8561,  2.3522],\n",
            "        [-2.1024,  2.5423],\n",
            "        [-1.8666,  2.2137],\n",
            "        [-1.2221,  1.4376],\n",
            "        [-1.3688,  1.9172],\n",
            "        [-1.6455,  2.2326],\n",
            "        [-2.4136,  2.8554],\n",
            "        [ 1.7201, -1.7730],\n",
            "        [-0.1426,  0.4831],\n",
            "        [-2.4774,  2.8776],\n",
            "        [ 0.1728, -0.0262],\n",
            "        [-1.9565,  2.4685],\n",
            "        [-2.2403,  2.6931],\n",
            "        [-0.6146,  0.7026],\n",
            "        [ 1.2212, -1.5370],\n",
            "        [-0.8447,  1.4588],\n",
            "        [-2.3685,  2.8293],\n",
            "        [-0.5291,  0.9149],\n",
            "        [-1.9913,  2.4071],\n",
            "        [-1.6636,  2.5409],\n",
            "        [-2.2246,  2.5706],\n",
            "        [-1.6034,  2.2114],\n",
            "        [ 1.8620, -2.5956]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.9262,  2.3911],\n",
            "        [ 1.9628, -2.5465],\n",
            "        [-0.7240,  1.3132],\n",
            "        [-1.2424,  1.7352],\n",
            "        [-1.4961,  2.0654],\n",
            "        [-2.0654,  2.5257],\n",
            "        [-1.3213,  1.9204],\n",
            "        [-1.8825,  2.1051],\n",
            "        [ 0.3635, -0.0349],\n",
            "        [-1.6199,  2.1369],\n",
            "        [-0.4411,  0.7725],\n",
            "        [-0.6892,  1.1649],\n",
            "        [-1.3234,  1.9259],\n",
            "        [-1.1357,  1.6084],\n",
            "        [-2.3645,  2.8712],\n",
            "        [ 0.9893, -0.5060],\n",
            "        [-2.3096,  2.7290],\n",
            "        [-2.3567,  2.7018],\n",
            "        [-2.2070,  2.6095],\n",
            "        [-1.4204,  2.0318],\n",
            "        [-2.4648,  2.9986],\n",
            "        [-1.8410,  2.3791],\n",
            "        [-2.4610,  2.7864],\n",
            "        [-2.4631,  2.8440],\n",
            "        [-1.8735,  2.3385],\n",
            "        [-2.2973,  2.5429],\n",
            "        [-2.4446,  2.7999],\n",
            "        [-0.3640,  0.9917],\n",
            "        [ 1.4087, -1.0499],\n",
            "        [-0.6986,  1.2199],\n",
            "        [ 2.0968, -2.3974],\n",
            "        [-2.0960,  2.5005]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.7386,  2.3088],\n",
            "        [-1.9081,  2.3913],\n",
            "        [ 1.1540, -0.8144],\n",
            "        [ 1.9025, -2.3063],\n",
            "        [-2.0553,  2.6163],\n",
            "        [ 0.0812,  0.4333],\n",
            "        [-1.7704,  2.2126],\n",
            "        [-1.4368,  1.9830],\n",
            "        [ 0.7584, -0.3299],\n",
            "        [ 0.5460, -0.1289],\n",
            "        [-0.9675,  1.2344],\n",
            "        [ 0.2291,  0.0660],\n",
            "        [-1.8634,  2.2770],\n",
            "        [-2.3069,  2.6685],\n",
            "        [ 1.6678, -2.4648],\n",
            "        [ 1.8246, -2.4560],\n",
            "        [ 1.8695, -2.6512],\n",
            "        [-2.1445,  2.4799],\n",
            "        [ 0.0981,  0.3895],\n",
            "        [-0.5644,  0.9915],\n",
            "        [-1.2359,  1.8834],\n",
            "        [-1.6875,  2.4205],\n",
            "        [ 1.8326, -2.5974],\n",
            "        [-2.2872,  2.8165],\n",
            "        [-1.0773,  1.2954],\n",
            "        [-0.1592,  0.6647],\n",
            "        [ 1.0705, -1.0765],\n",
            "        [-1.9632,  2.3726],\n",
            "        [-0.6247,  0.8081],\n",
            "        [ 0.2563,  0.3420],\n",
            "        [-2.2110,  2.7371],\n",
            "        [-2.2798,  2.8112]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.3892,  1.0713],\n",
            "        [-1.8115,  2.5376],\n",
            "        [-2.3731,  2.7940],\n",
            "        [-2.4231,  2.8896],\n",
            "        [-0.1069,  0.4047],\n",
            "        [-0.6591,  1.3451],\n",
            "        [-1.3694,  1.7686],\n",
            "        [-2.2271,  2.7721],\n",
            "        [-1.1784,  1.6459],\n",
            "        [-1.0393,  1.4893],\n",
            "        [-1.4581,  1.9587],\n",
            "        [-0.9017,  1.1330],\n",
            "        [ 1.6802, -1.9207],\n",
            "        [-0.6990,  0.8990],\n",
            "        [-2.2659,  2.6476],\n",
            "        [-2.2543,  2.6431],\n",
            "        [-1.4740,  2.1015],\n",
            "        [ 1.5530, -1.2053],\n",
            "        [ 1.6992, -2.5120],\n",
            "        [-1.4861,  2.1115],\n",
            "        [-2.2709,  2.6443],\n",
            "        [-1.3416,  1.7380],\n",
            "        [-1.6780,  2.2940],\n",
            "        [ 1.5647, -1.4397],\n",
            "        [-1.2424,  1.7695],\n",
            "        [-2.3961,  2.6626],\n",
            "        [-1.1083,  1.3319],\n",
            "        [-2.3830,  2.7187],\n",
            "        [-2.2175,  2.6884],\n",
            "        [ 1.2738, -1.0440],\n",
            "        [ 1.5913, -1.5723],\n",
            "        [-2.2612,  2.7935]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 0.3660,  0.1468],\n",
            "        [-1.8096,  2.2364],\n",
            "        [-2.2883,  2.8642],\n",
            "        [-1.4424,  1.8658],\n",
            "        [-2.0635,  2.5619],\n",
            "        [-2.0404,  2.8687],\n",
            "        [-1.2352,  1.7534],\n",
            "        [-0.1120,  0.6113],\n",
            "        [-1.1963,  1.4576],\n",
            "        [-0.5025,  0.7848],\n",
            "        [-2.2447,  2.6118],\n",
            "        [-2.4341,  2.7812],\n",
            "        [-2.4240,  2.9318],\n",
            "        [-2.4030,  2.8603],\n",
            "        [-0.4607,  1.1248],\n",
            "        [-1.5867,  1.7961],\n",
            "        [ 2.1556, -2.6593],\n",
            "        [ 1.2508, -1.0949],\n",
            "        [-0.0552,  0.5303],\n",
            "        [-1.4950,  2.0481],\n",
            "        [-2.0000,  2.3966],\n",
            "        [-1.1601,  1.7270],\n",
            "        [-1.6551,  2.0350],\n",
            "        [ 0.2914,  0.1881],\n",
            "        [ 1.9814, -2.6636],\n",
            "        [-1.8986,  2.3577],\n",
            "        [-2.1099,  2.5492],\n",
            "        [-2.4103,  2.8982],\n",
            "        [ 0.6513, -0.2508],\n",
            "        [-2.3506,  2.6798],\n",
            "        [ 1.3450, -1.7722],\n",
            "        [ 1.2268, -1.0281]], device='cuda:0'), hidden_states=None, attentions=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  75%|███████▌  | 3/4 [07:47<02:37, 157.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 2.0749, -2.5020],\n",
            "        [-1.2071,  1.8561],\n",
            "        [-2.2936,  2.6424],\n",
            "        [ 0.8320, -0.4246],\n",
            "        [ 2.0597, -2.6008],\n",
            "        [ 0.8665, -0.5440],\n",
            "        [-2.3442,  2.7845],\n",
            "        [-1.6040,  1.8713],\n",
            "        [-2.4591,  2.8786],\n",
            "        [-1.8858,  2.4056],\n",
            "        [ 1.1174, -0.8089],\n",
            "        [-0.7842,  1.1609],\n",
            "        [ 1.6746, -2.1916],\n",
            "        [-0.8773,  1.2384],\n",
            "        [ 1.8707, -2.3872],\n",
            "        [-1.2641,  1.6459],\n",
            "        [-1.4623,  1.6135],\n",
            "        [-2.2063,  2.7467],\n",
            "        [-2.2347,  2.7102],\n",
            "        [-0.6077,  1.3652],\n",
            "        [-1.8841,  2.2539],\n",
            "        [-2.3604,  2.7006],\n",
            "        [-2.1667,  2.5830],\n",
            "        [-1.3456,  1.7841],\n",
            "        [ 0.8338, -0.3556],\n",
            "        [-0.6701,  0.9706],\n",
            "        [-2.4435,  2.8429],\n",
            "        [ 1.4928, -1.8312],\n",
            "        [ 0.2147,  0.3597],\n",
            "        [ 0.8037, -0.0860],\n",
            "        [-2.0614,  2.8094],\n",
            "        [-2.1037,  2.6887]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.4071,  2.7166],\n",
            "        [ 1.6443, -1.9530],\n",
            "        [-1.9448,  2.3798],\n",
            "        [ 0.7540, -0.2727],\n",
            "        [-2.3959,  2.8222],\n",
            "        [-2.2328,  2.7338],\n",
            "        [-0.1262,  0.8425],\n",
            "        [-2.2290,  2.7390],\n",
            "        [ 0.8039, -0.5094],\n",
            "        [-2.1780,  2.5745],\n",
            "        [-2.1907,  2.6784],\n",
            "        [-0.2541,  0.1315],\n",
            "        [-2.1351,  2.5920],\n",
            "        [-1.8388,  2.3999],\n",
            "        [-0.8701,  1.0989],\n",
            "        [-1.9842,  2.4670],\n",
            "        [ 0.8297, -0.4384],\n",
            "        [ 1.7233, -1.8981],\n",
            "        [-1.4721,  1.9123],\n",
            "        [-2.3667,  2.8084],\n",
            "        [-1.8881,  2.5198],\n",
            "        [-0.9353,  0.9823],\n",
            "        [-1.6617,  2.2423],\n",
            "        [-2.3250,  2.4938]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Accuracy: 0.8225308641975309\n",
            "Train loss: 0.11204191495536893\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3727, -2.7668],\n",
            "        [-1.9279,  2.4509],\n",
            "        [-2.2431,  2.6909],\n",
            "        [-2.4704,  2.7519],\n",
            "        [-1.9826,  2.1847],\n",
            "        [ 0.8803, -0.7645],\n",
            "        [-2.5740,  2.9111],\n",
            "        [ 1.5293, -2.1058],\n",
            "        [ 2.1139, -2.8856],\n",
            "        [-1.8613,  2.0064],\n",
            "        [-1.8446,  2.2136],\n",
            "        [-2.3947,  2.6913],\n",
            "        [-2.5232,  2.8731],\n",
            "        [-1.1091,  1.6050],\n",
            "        [-2.5753,  3.0011],\n",
            "        [-1.5852,  1.8509],\n",
            "        [ 0.1523,  0.0727],\n",
            "        [-2.2424,  2.8523],\n",
            "        [ 1.2622, -0.8061],\n",
            "        [-2.1585,  2.6322],\n",
            "        [-2.5754,  2.8874],\n",
            "        [-2.4261,  2.9548],\n",
            "        [-2.5041,  2.9804],\n",
            "        [-2.4892,  2.9697],\n",
            "        [ 2.0088, -2.8093],\n",
            "        [-1.7369,  2.5135],\n",
            "        [-2.3944,  2.7883],\n",
            "        [-2.4548,  2.8887],\n",
            "        [-1.1674,  1.5817],\n",
            "        [-2.5520,  2.9332],\n",
            "        [-2.2463,  2.7056],\n",
            "        [-2.4290,  2.7790]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.4997,  2.8617],\n",
            "        [-2.2242,  2.7911],\n",
            "        [-2.5646,  2.8287],\n",
            "        [-2.5250,  3.0240],\n",
            "        [-2.5768,  2.9522],\n",
            "        [ 0.3941, -0.2827],\n",
            "        [-2.4219,  2.6203],\n",
            "        [-2.1186,  2.6374],\n",
            "        [ 1.6846, -1.2895],\n",
            "        [-2.5825,  2.9058],\n",
            "        [-2.5315,  2.8179],\n",
            "        [-2.4684,  2.7837],\n",
            "        [-0.7388,  0.9535],\n",
            "        [-2.4423,  2.7705],\n",
            "        [-2.2438,  2.7964],\n",
            "        [-2.5397,  2.9927],\n",
            "        [-2.5153,  2.9729],\n",
            "        [-2.4689,  2.8017],\n",
            "        [-1.8970,  2.3047],\n",
            "        [-2.4711,  2.8254],\n",
            "        [-2.1705,  2.6975],\n",
            "        [-0.8002,  1.0220],\n",
            "        [ 0.0593,  0.3100],\n",
            "        [-2.2758,  2.6819],\n",
            "        [-2.5388,  2.9520],\n",
            "        [-1.6152,  2.2745],\n",
            "        [-2.4742,  2.9022],\n",
            "        [-2.6066,  3.1164],\n",
            "        [-2.1445,  2.3900],\n",
            "        [-2.5041,  2.8076],\n",
            "        [-2.5908,  2.8874],\n",
            "        [-0.1735,  0.4691]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.4998,  2.7780],\n",
            "        [-2.4470,  2.8420],\n",
            "        [ 2.2213, -2.7028],\n",
            "        [-2.6245,  3.0167],\n",
            "        [-2.0844,  2.7545],\n",
            "        [-2.4560,  3.0200],\n",
            "        [-2.6304,  2.9092],\n",
            "        [-0.0925,  0.6988],\n",
            "        [-2.4419,  2.7902],\n",
            "        [-2.4811,  2.8931],\n",
            "        [-2.5568,  2.9997],\n",
            "        [-2.6591,  2.9143],\n",
            "        [-1.8459,  2.3144],\n",
            "        [-1.8912,  2.2985],\n",
            "        [-2.0905,  2.5237],\n",
            "        [-2.4920,  2.8505],\n",
            "        [-2.5029,  2.8330],\n",
            "        [-2.6143,  2.8760],\n",
            "        [-2.3826,  2.7871],\n",
            "        [-2.5585,  2.9802],\n",
            "        [-2.4109,  2.9440],\n",
            "        [ 1.9403, -2.0475],\n",
            "        [-2.4646,  2.8011],\n",
            "        [-2.2623,  2.7581],\n",
            "        [-1.7932,  2.1583],\n",
            "        [-2.2087,  2.8023],\n",
            "        [ 2.0094, -2.8148],\n",
            "        [ 1.9586, -2.2050],\n",
            "        [ 2.0618, -2.2968],\n",
            "        [ 1.6982, -2.1245],\n",
            "        [ 1.5356, -2.4064],\n",
            "        [ 1.8805, -2.0046]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.5129,  3.0208],\n",
            "        [ 2.0648, -2.5898],\n",
            "        [ 1.8743, -2.3385],\n",
            "        [-2.4728,  2.7287],\n",
            "        [-0.4237,  0.4102],\n",
            "        [ 1.8952, -2.1365],\n",
            "        [-2.2763,  2.7967],\n",
            "        [-2.1088,  2.3058],\n",
            "        [-1.3738,  2.1598],\n",
            "        [-0.9704,  1.4107],\n",
            "        [ 1.2760, -1.4626],\n",
            "        [-0.6050,  0.6080],\n",
            "        [ 2.1024, -2.3269],\n",
            "        [-2.5388,  2.9781],\n",
            "        [-1.7475,  2.1246],\n",
            "        [-2.5652,  2.9404],\n",
            "        [-2.5869,  2.9008],\n",
            "        [-2.5265,  2.8890],\n",
            "        [ 1.8790, -2.5068],\n",
            "        [-2.3608,  2.8277],\n",
            "        [ 2.1241, -2.7641],\n",
            "        [-1.7986,  2.0567],\n",
            "        [-2.1752,  2.7096],\n",
            "        [-2.5399,  2.8758],\n",
            "        [-2.5581,  2.9267],\n",
            "        [-2.2135,  2.4187],\n",
            "        [-2.3024,  2.7670],\n",
            "        [-2.5263,  2.8377],\n",
            "        [-0.8818,  0.7683],\n",
            "        [ 1.6835, -1.8098],\n",
            "        [-2.6531,  2.9607],\n",
            "        [-2.4841,  2.9450]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.2542,  2.8136],\n",
            "        [-2.3585,  2.9181],\n",
            "        [-1.1867,  1.6191],\n",
            "        [-2.5819,  2.9359],\n",
            "        [-2.4279,  2.7607],\n",
            "        [ 0.9831, -0.9796],\n",
            "        [-2.6081,  3.0269],\n",
            "        [-2.5661,  3.0056],\n",
            "        [-2.5667,  2.9660],\n",
            "        [-2.4279,  2.9879],\n",
            "        [-2.5011,  2.7768],\n",
            "        [-2.2847,  2.6432],\n",
            "        [ 1.9973, -2.2510],\n",
            "        [-2.5038,  2.7491],\n",
            "        [-2.4504,  2.7718],\n",
            "        [-2.5382,  2.9006],\n",
            "        [-2.6150,  3.0092],\n",
            "        [ 2.1736, -2.3689],\n",
            "        [ 1.9389, -2.6685],\n",
            "        [-1.9135,  2.4108],\n",
            "        [-2.4650,  2.9422],\n",
            "        [-1.5505,  2.1767],\n",
            "        [ 2.0116, -2.1217],\n",
            "        [ 2.0555, -2.5462],\n",
            "        [-2.5377,  2.9624],\n",
            "        [-0.5169,  0.6719],\n",
            "        [ 1.3539, -1.0424],\n",
            "        [-2.4026,  2.6423],\n",
            "        [-2.3712,  2.7474],\n",
            "        [-2.5866,  2.9119],\n",
            "        [-2.6319,  2.9449],\n",
            "        [-0.4820,  0.5839]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9128, -0.7963],\n",
            "        [ 2.1949, -2.8877],\n",
            "        [-1.3247,  1.7097],\n",
            "        [-2.3594,  2.6879],\n",
            "        [-1.5804,  1.8484],\n",
            "        [-2.3919,  2.6678],\n",
            "        [-2.1455,  2.4222],\n",
            "        [-2.4841,  2.8908],\n",
            "        [-2.6652,  3.0819],\n",
            "        [ 1.8610, -2.1602],\n",
            "        [-2.5487,  3.0378],\n",
            "        [-2.5646,  3.0614],\n",
            "        [-2.5254,  2.9063],\n",
            "        [-2.5301,  2.8658],\n",
            "        [-2.5144,  3.0678],\n",
            "        [-2.5485,  2.9264],\n",
            "        [-2.1176,  2.6179],\n",
            "        [-2.4522,  2.7971],\n",
            "        [-0.5812,  0.9666],\n",
            "        [ 2.2104, -2.8665],\n",
            "        [-1.1707,  1.7092],\n",
            "        [-2.1625,  2.4492],\n",
            "        [-0.1595,  0.6680],\n",
            "        [-0.7033,  1.2205],\n",
            "        [-2.5142,  2.8053],\n",
            "        [-2.1631,  2.6484],\n",
            "        [-2.3200,  2.7069],\n",
            "        [-2.3777,  2.8753],\n",
            "        [-1.8214,  2.2258],\n",
            "        [-0.3869,  0.5856],\n",
            "        [-0.9122,  1.0884],\n",
            "        [-0.0616,  0.0042]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.1091,  2.8460],\n",
            "        [-2.2468,  2.8459],\n",
            "        [-2.5595,  2.9139],\n",
            "        [-2.4241,  3.1232],\n",
            "        [ 1.7110, -2.3744],\n",
            "        [-2.5043,  2.9512],\n",
            "        [ 2.2385, -2.6799],\n",
            "        [-2.3431,  2.5742],\n",
            "        [ 1.6457, -1.9494],\n",
            "        [-2.6347,  2.9357],\n",
            "        [ 1.2461, -1.3713],\n",
            "        [-2.5109,  2.9467],\n",
            "        [-2.5940,  3.0188],\n",
            "        [ 0.3316,  0.0440],\n",
            "        [-2.4257,  2.8963],\n",
            "        [ 2.1331, -2.4807],\n",
            "        [-2.3512,  2.7584],\n",
            "        [ 2.2963, -2.9385],\n",
            "        [-2.3689,  2.7290],\n",
            "        [-2.4673,  2.9143],\n",
            "        [-2.4831,  2.9801],\n",
            "        [-2.4026,  2.8642],\n",
            "        [-1.5806,  1.7756],\n",
            "        [ 0.6224, -0.1412],\n",
            "        [-1.9978,  2.6317],\n",
            "        [-2.5528,  2.9622],\n",
            "        [-2.5344,  2.8904],\n",
            "        [-2.5703,  2.9363],\n",
            "        [-2.4082,  2.9824],\n",
            "        [-2.5205,  2.9083],\n",
            "        [-2.4865,  2.8083],\n",
            "        [-2.3742,  2.8517]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.5436,  2.9506],\n",
            "        [-1.5714,  2.2577],\n",
            "        [-1.7502,  2.1411],\n",
            "        [-2.2704,  2.6406],\n",
            "        [ 2.1392, -2.9082],\n",
            "        [ 2.1216, -2.7141],\n",
            "        [ 1.5079, -1.5137],\n",
            "        [-0.1202,  0.6168],\n",
            "        [-2.1264,  2.7057],\n",
            "        [ 1.1805, -1.6282],\n",
            "        [-1.0769,  1.0412],\n",
            "        [-2.5259,  2.8767],\n",
            "        [ 1.9466, -2.7249],\n",
            "        [-2.4023,  2.8769],\n",
            "        [-2.1006,  2.5291],\n",
            "        [-0.7374,  0.8417],\n",
            "        [ 1.4476, -2.3150],\n",
            "        [-1.2114,  1.8165],\n",
            "        [-2.4406,  2.8689],\n",
            "        [ 0.7351, -0.9011],\n",
            "        [-2.3913,  2.7243],\n",
            "        [-1.9003,  2.3366],\n",
            "        [-0.2009,  1.0161],\n",
            "        [ 0.8625, -0.5578],\n",
            "        [-2.3618,  2.7197],\n",
            "        [ 2.2037, -2.7161],\n",
            "        [-2.5622,  3.0226],\n",
            "        [-2.0434,  2.8408],\n",
            "        [-1.2028,  1.6029],\n",
            "        [-2.2191,  2.5866],\n",
            "        [-1.1699,  1.4786],\n",
            "        [-2.5042,  2.8885]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.5340,  2.8082],\n",
            "        [-2.3465,  2.9035],\n",
            "        [-2.5148,  2.9294],\n",
            "        [-2.5651,  3.0267],\n",
            "        [-2.4868,  2.9863],\n",
            "        [-2.4947,  2.8340],\n",
            "        [ 2.0314, -2.8268],\n",
            "        [-2.4659,  2.9108],\n",
            "        [-1.1935,  1.7737],\n",
            "        [-1.8193,  2.3411],\n",
            "        [-2.4521,  2.8082],\n",
            "        [-2.4361,  2.9259],\n",
            "        [ 2.0542, -2.5717],\n",
            "        [-2.4333,  2.7166],\n",
            "        [-1.6619,  2.2730],\n",
            "        [-2.4470,  2.9873],\n",
            "        [-2.6843,  3.0198],\n",
            "        [-2.6016,  3.0094],\n",
            "        [-1.6234,  2.0177],\n",
            "        [ 1.7578, -2.0233],\n",
            "        [-2.3934,  2.7063],\n",
            "        [ 1.2800, -1.3752],\n",
            "        [-2.2805,  2.7674],\n",
            "        [ 2.1857, -2.5098],\n",
            "        [-0.9625,  1.2024],\n",
            "        [-0.0824,  0.3386],\n",
            "        [-2.4821,  2.7599],\n",
            "        [-2.4854,  2.8285],\n",
            "        [-2.2542,  2.6738],\n",
            "        [-1.8107,  2.0935],\n",
            "        [-2.6453,  3.1291],\n",
            "        [-1.7624,  2.3491]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.7451,  2.1002],\n",
            "        [-2.5004,  2.9648],\n",
            "        [ 0.7358, -0.2853],\n",
            "        [ 2.0150, -2.4856],\n",
            "        [ 1.7952, -2.1308],\n",
            "        [-2.1792,  2.4437],\n",
            "        [-2.5665,  3.0937],\n",
            "        [-2.4905,  2.9319],\n",
            "        [-2.5290,  2.8838],\n",
            "        [ 1.9037, -2.2940],\n",
            "        [ 1.0878, -1.0777],\n",
            "        [-1.8517,  2.4806],\n",
            "        [-0.8865,  1.5972],\n",
            "        [-2.5467,  2.8473],\n",
            "        [-2.2553,  2.5040],\n",
            "        [-2.6583,  3.0646],\n",
            "        [-0.4405,  0.4438],\n",
            "        [-2.5205,  2.8157],\n",
            "        [-2.4910,  2.9021],\n",
            "        [-1.8202,  2.2886],\n",
            "        [-1.6593,  1.6764],\n",
            "        [-2.5211,  2.9594],\n",
            "        [-0.7904,  0.8887],\n",
            "        [-2.5237,  2.7695],\n",
            "        [-1.6119,  1.9049],\n",
            "        [-0.0954,  0.4295],\n",
            "        [-2.2468,  2.7556],\n",
            "        [ 1.9274, -2.7418],\n",
            "        [-1.9513,  2.3743],\n",
            "        [-2.2307,  2.6176],\n",
            "        [-2.1368,  2.5855],\n",
            "        [-2.4876,  2.9398]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.3266,  2.7766],\n",
            "        [-1.2729,  1.7060],\n",
            "        [-1.7022,  1.7773],\n",
            "        [-2.4639,  2.8331],\n",
            "        [-2.3517,  2.5853],\n",
            "        [-1.6460,  1.8619],\n",
            "        [-2.5364,  3.1793],\n",
            "        [ 0.4965,  0.0694],\n",
            "        [-2.6271,  2.9609],\n",
            "        [-2.1275,  2.4774],\n",
            "        [-2.4506,  2.7579],\n",
            "        [-2.5191,  2.9707],\n",
            "        [ 0.8173, -0.4022],\n",
            "        [-2.5220,  2.9280],\n",
            "        [-1.9977,  2.3196],\n",
            "        [-2.5511,  2.9421],\n",
            "        [-2.3600,  2.7132],\n",
            "        [ 1.3482, -1.8261],\n",
            "        [-2.1573,  2.7416],\n",
            "        [ 1.9416, -2.1659],\n",
            "        [-2.4782,  2.8303],\n",
            "        [-1.5304,  1.6909],\n",
            "        [ 1.0791, -0.9761],\n",
            "        [-2.6285,  3.0768],\n",
            "        [ 1.6693, -1.7354],\n",
            "        [-2.6258,  3.0058],\n",
            "        [-2.4984,  2.9258],\n",
            "        [-2.3217,  2.8971],\n",
            "        [-2.4467,  2.8224],\n",
            "        [-1.9502,  2.5419],\n",
            "        [ 1.2317, -1.0544],\n",
            "        [-0.8318,  1.2559]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.8608,  2.3101],\n",
            "        [-1.6220,  1.8563],\n",
            "        [-2.3463,  2.6931],\n",
            "        [-2.6379,  2.9996],\n",
            "        [-2.3986,  2.7036],\n",
            "        [ 1.9726, -2.1602],\n",
            "        [-2.4814,  2.8708],\n",
            "        [-2.3571,  2.6709],\n",
            "        [-2.5596,  2.9481],\n",
            "        [ 0.2798, -0.6036],\n",
            "        [-2.6105,  2.9819],\n",
            "        [-0.8385,  1.2373],\n",
            "        [-1.8391,  2.3846],\n",
            "        [-1.1824,  1.4441],\n",
            "        [-2.4238,  2.7220],\n",
            "        [-2.2386,  2.6266],\n",
            "        [-2.5956,  2.8435],\n",
            "        [-2.4663,  2.8926],\n",
            "        [-2.3632,  2.7051],\n",
            "        [-0.9570,  1.1450],\n",
            "        [ 0.8971, -0.8196],\n",
            "        [-2.5502,  2.9508],\n",
            "        [-2.2908,  2.8186],\n",
            "        [-2.5442,  3.0111],\n",
            "        [ 0.7302, -0.1846],\n",
            "        [-2.2645,  2.8043],\n",
            "        [-2.6152,  2.9147],\n",
            "        [ 1.8191, -2.1235],\n",
            "        [-1.4863,  1.7747],\n",
            "        [-2.4715,  2.8881],\n",
            "        [-2.5279,  2.8539],\n",
            "        [-2.2908,  2.6991]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.4184,  2.7997],\n",
            "        [ 0.7986, -0.2299],\n",
            "        [-1.4767,  2.0896],\n",
            "        [-2.5121,  2.8677],\n",
            "        [-2.4873,  2.7156],\n",
            "        [ 0.9529, -0.6377],\n",
            "        [ 1.3161, -1.6599],\n",
            "        [-2.5527,  2.8590],\n",
            "        [-1.8898,  2.2440],\n",
            "        [-2.3153,  2.7279],\n",
            "        [ 0.9706, -0.9392],\n",
            "        [-2.2419,  2.6853],\n",
            "        [-2.5396,  2.9571],\n",
            "        [ 0.2520, -0.6315],\n",
            "        [ 1.6582, -1.7024],\n",
            "        [-2.6700,  3.0322],\n",
            "        [-2.6228,  2.8525],\n",
            "        [-2.3722,  2.8046],\n",
            "        [ 2.2020, -2.7833],\n",
            "        [-2.5089,  2.8983],\n",
            "        [-1.7966,  2.5350],\n",
            "        [-2.4162,  2.9215],\n",
            "        [ 1.4858, -1.5104],\n",
            "        [-2.2294,  2.6278],\n",
            "        [ 1.8834, -2.6355],\n",
            "        [-2.4681,  2.8092],\n",
            "        [-2.4177,  2.9207],\n",
            "        [-2.0552,  2.3664],\n",
            "        [ 0.9119, -1.2630],\n",
            "        [-2.4520,  2.6752],\n",
            "        [-1.6735,  2.0848],\n",
            "        [ 2.0725, -2.8543]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.5771,  3.1372],\n",
            "        [-2.4893,  2.9442],\n",
            "        [-2.4583,  3.0101],\n",
            "        [ 2.1868, -2.9677],\n",
            "        [-2.2122,  2.5574],\n",
            "        [-2.2774,  2.6446],\n",
            "        [ 2.1101, -2.8753],\n",
            "        [-2.0994,  2.5507],\n",
            "        [-2.3956,  2.7032],\n",
            "        [-2.5278,  2.9407],\n",
            "        [-2.5752,  3.0044],\n",
            "        [-2.1975,  2.8524],\n",
            "        [-2.5831,  2.8214],\n",
            "        [-0.3674,  0.9013],\n",
            "        [ 1.4197, -1.3867],\n",
            "        [ 2.1135, -2.8615],\n",
            "        [-2.5482,  3.0526],\n",
            "        [ 1.9182, -2.2529],\n",
            "        [-2.4374,  2.6656],\n",
            "        [-2.3713,  3.0485],\n",
            "        [-2.2096,  2.6598],\n",
            "        [-2.3959,  2.6162],\n",
            "        [-2.5372,  2.9285],\n",
            "        [-2.4325,  2.7217],\n",
            "        [-2.5520,  2.8412],\n",
            "        [-2.3477,  2.6541],\n",
            "        [-2.3255,  2.5728],\n",
            "        [-1.6443,  2.3038],\n",
            "        [ 2.0167, -2.3479],\n",
            "        [-2.4992,  2.7912],\n",
            "        [ 2.2775, -2.9004],\n",
            "        [ 0.9376, -0.7355]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.3125,  2.6278],\n",
            "        [ 2.3013, -2.9011],\n",
            "        [ 0.9373, -0.6239],\n",
            "        [ 2.1476, -2.8040],\n",
            "        [ 1.3609, -1.3144],\n",
            "        [ 0.7473, -0.7130],\n",
            "        [ 1.9071, -2.2966],\n",
            "        [-2.5799,  3.0053],\n",
            "        [ 2.0965, -2.6766],\n",
            "        [-2.4817,  2.8204],\n",
            "        [-2.1466,  2.4389],\n",
            "        [-2.5216,  2.9549],\n",
            "        [-2.5919,  2.9397],\n",
            "        [ 2.2193, -2.4199],\n",
            "        [-2.5243,  2.8823],\n",
            "        [-2.3218,  2.6290],\n",
            "        [-2.3816,  2.9207],\n",
            "        [-2.5577,  2.9091],\n",
            "        [-2.5037,  2.8076],\n",
            "        [-2.1707,  2.9963],\n",
            "        [-2.4270,  3.0993],\n",
            "        [ 0.9538, -0.9713],\n",
            "        [-2.4387,  2.9126],\n",
            "        [ 0.4602, -0.1623],\n",
            "        [ 0.4620, -0.2253],\n",
            "        [-2.5373,  2.7594],\n",
            "        [-2.2365,  2.5472],\n",
            "        [ 1.8261, -1.9792],\n",
            "        [ 1.9143, -2.5949],\n",
            "        [-2.4280,  2.9262],\n",
            "        [-1.2939,  1.9724],\n",
            "        [-0.0912,  0.6068]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.5494,  2.7763],\n",
            "        [-2.0733,  2.5368],\n",
            "        [ 2.1005, -2.5670],\n",
            "        [-2.3330,  2.8761],\n",
            "        [ 1.3811, -1.1898],\n",
            "        [ 1.9074, -1.9429],\n",
            "        [-2.5884,  2.8882],\n",
            "        [-2.5130,  2.8803],\n",
            "        [-1.8140,  2.0078],\n",
            "        [-0.8350,  1.0087],\n",
            "        [ 0.3768,  0.0801],\n",
            "        [ 0.5213,  0.2271],\n",
            "        [ 1.8498, -2.5378],\n",
            "        [-2.2639,  2.7108],\n",
            "        [-1.9970,  2.2769],\n",
            "        [-2.0814,  2.4985],\n",
            "        [-2.1469,  2.7761],\n",
            "        [ 1.3719, -1.2029],\n",
            "        [-2.3972,  2.8007],\n",
            "        [ 0.3794, -0.2158],\n",
            "        [ 1.2907, -1.1408],\n",
            "        [ 1.0972, -1.5325],\n",
            "        [ 2.1781, -2.8006],\n",
            "        [ 2.0879, -2.6080],\n",
            "        [-2.5095,  2.9145],\n",
            "        [-2.6045,  2.9495],\n",
            "        [-2.2153,  2.4698],\n",
            "        [-2.3948,  2.7717],\n",
            "        [-2.6476,  2.9378],\n",
            "        [-1.5184,  1.6453],\n",
            "        [-1.0787,  1.4875],\n",
            "        [-0.1656,  0.5590]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.4832,  2.7440],\n",
            "        [-2.0303,  2.7259],\n",
            "        [-2.5125,  3.0508],\n",
            "        [-1.2816,  1.6568],\n",
            "        [-2.2461,  2.4390],\n",
            "        [-2.4546,  3.0426],\n",
            "        [-2.3580,  2.7712],\n",
            "        [-2.6328,  2.9811],\n",
            "        [-2.5606,  2.8925],\n",
            "        [-2.2769,  2.7327],\n",
            "        [-2.1560,  2.7588],\n",
            "        [-2.4191,  2.8302],\n",
            "        [-2.4708,  2.8559],\n",
            "        [ 2.0237, -2.2377],\n",
            "        [ 1.0220, -1.3859],\n",
            "        [-2.3773,  2.6470],\n",
            "        [-2.4647,  2.7386],\n",
            "        [ 2.0979, -2.6040],\n",
            "        [-2.5947,  2.9286],\n",
            "        [ 0.5157,  0.0368],\n",
            "        [-1.9258,  2.4404],\n",
            "        [-1.1846,  1.7631],\n",
            "        [-0.4120,  0.2311],\n",
            "        [ 1.3158, -1.3142],\n",
            "        [-2.3550,  2.8755],\n",
            "        [-2.4326,  2.8226],\n",
            "        [-2.4554,  2.8826],\n",
            "        [-2.2703,  2.6069],\n",
            "        [ 0.4091, -0.2141],\n",
            "        [-0.8126,  0.9088],\n",
            "        [ 2.1489, -2.9141],\n",
            "        [-2.4630,  2.9719]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-1.2854,  1.5170],\n",
            "        [-2.4925,  2.8165],\n",
            "        [-2.4720,  2.9150],\n",
            "        [-2.2037,  2.7936],\n",
            "        [-2.5418,  2.8109],\n",
            "        [-0.9796,  1.3963],\n",
            "        [-2.5694,  2.9680],\n",
            "        [-2.5051,  2.7740],\n",
            "        [ 2.1597, -2.7029],\n",
            "        [-2.5435,  2.9602],\n",
            "        [-0.4840,  0.9393],\n",
            "        [-2.5536,  2.9164],\n",
            "        [-0.2481,  0.4145],\n",
            "        [-2.5231,  2.8497],\n",
            "        [-2.2972,  2.9221],\n",
            "        [-2.5520,  2.9930],\n",
            "        [-2.1645,  2.7447],\n",
            "        [-2.5195,  2.7998],\n",
            "        [ 2.1750, -2.7254],\n",
            "        [ 1.3905, -1.8120],\n",
            "        [-1.7874,  2.2243],\n",
            "        [ 0.9440, -0.9679],\n",
            "        [-0.6041,  1.3422],\n",
            "        [-1.9760,  2.0677],\n",
            "        [ 1.0860, -0.9758],\n",
            "        [ 1.6920, -2.0633],\n",
            "        [-2.0025,  2.4801],\n",
            "        [ 1.3012, -1.0428],\n",
            "        [-0.5525,  0.9743],\n",
            "        [-1.9614,  1.8931],\n",
            "        [-2.5904,  2.9304],\n",
            "        [-2.4711,  2.8780]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 1.7712, -1.8012],\n",
            "        [-2.0505,  2.4622],\n",
            "        [-2.5491,  2.9624],\n",
            "        [-2.5366,  2.8163],\n",
            "        [ 1.8949, -2.3025],\n",
            "        [ 2.0023, -2.3470],\n",
            "        [-1.2661,  1.7720],\n",
            "        [-1.1557,  1.4163],\n",
            "        [-2.5244,  2.8187],\n",
            "        [-2.5593,  2.9085],\n",
            "        [-2.1577,  2.6972],\n",
            "        [-2.0489,  2.4188],\n",
            "        [-2.5296,  2.8998],\n",
            "        [ 0.7473, -0.9180],\n",
            "        [ 1.9706, -2.3860],\n",
            "        [-1.9894,  2.6232],\n",
            "        [-2.3088,  2.7067],\n",
            "        [ 2.1194, -2.4724],\n",
            "        [-2.6372,  2.9203],\n",
            "        [-2.5188,  2.8663],\n",
            "        [-2.5841,  2.9244],\n",
            "        [-1.6340,  1.9312],\n",
            "        [-0.1334,  0.8587],\n",
            "        [-2.3542,  2.9029],\n",
            "        [ 1.9256, -2.8705],\n",
            "        [ 1.1651, -0.7747],\n",
            "        [-0.9869,  1.6100],\n",
            "        [-2.1173,  2.8368],\n",
            "        [-1.8071,  2.4332],\n",
            "        [-2.5116,  3.0001],\n",
            "        [-2.3828,  2.8593],\n",
            "        [-2.3491,  2.8354]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.5829,  2.9877],\n",
            "        [-2.1627,  2.6783],\n",
            "        [-1.9617,  2.5208],\n",
            "        [-2.4872,  2.7921],\n",
            "        [-2.4013,  2.6988],\n",
            "        [-2.5679,  2.9238],\n",
            "        [-2.4592,  2.7486],\n",
            "        [-2.5731,  2.9516],\n",
            "        [ 0.6335, -0.1916],\n",
            "        [-2.2541,  2.7201],\n",
            "        [-2.4223,  2.8597],\n",
            "        [-2.4051,  2.8902],\n",
            "        [ 1.8315, -2.6738],\n",
            "        [-2.4494,  2.8091],\n",
            "        [-2.4364,  2.9391],\n",
            "        [-2.4771,  2.7775],\n",
            "        [-2.6001,  2.9619],\n",
            "        [-2.4477,  2.9470],\n",
            "        [ 2.1715, -2.9054],\n",
            "        [-2.4702,  2.7672],\n",
            "        [ 2.2659, -2.9078],\n",
            "        [ 1.4184, -2.0169],\n",
            "        [ 1.9425, -1.9731],\n",
            "        [-2.3404,  2.6774],\n",
            "        [-2.5987,  2.9938],\n",
            "        [ 1.9502, -2.8338],\n",
            "        [ 1.9258, -2.5583],\n",
            "        [ 0.2454,  0.0159],\n",
            "        [ 0.5060, -0.5618],\n",
            "        [-2.6162,  3.0458],\n",
            "        [-0.8448,  1.0430],\n",
            "        [-2.3929,  2.7035]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1760, -2.8341],\n",
            "        [ 2.1293, -2.6281],\n",
            "        [-1.4786,  2.0717],\n",
            "        [-2.2100,  2.8675],\n",
            "        [-2.5703,  3.1188],\n",
            "        [-2.4549,  2.7491],\n",
            "        [-2.5180,  2.8166],\n",
            "        [-0.0569,  0.4882],\n",
            "        [-2.4236,  3.0438],\n",
            "        [-2.2074,  2.6759],\n",
            "        [-2.4793,  2.8077],\n",
            "        [-2.2655,  2.5642],\n",
            "        [-1.8865,  1.9574],\n",
            "        [-1.8219,  2.3114],\n",
            "        [-2.0671,  2.6611],\n",
            "        [-2.5364,  2.9131],\n",
            "        [ 1.6684, -1.5868],\n",
            "        [-1.3978,  1.7799],\n",
            "        [-2.5372,  2.8914],\n",
            "        [-0.2541,  0.3003],\n",
            "        [-2.3183,  2.7470],\n",
            "        [-2.5467,  2.9526],\n",
            "        [-0.9619,  0.9528],\n",
            "        [ 1.2169, -1.6974],\n",
            "        [-1.5033,  2.1117],\n",
            "        [-2.4552,  2.8981],\n",
            "        [-1.2984,  1.6727],\n",
            "        [-2.3535,  2.7653],\n",
            "        [-2.1280,  2.9499],\n",
            "        [-2.4595,  2.7693],\n",
            "        [-2.1932,  2.7894],\n",
            "        [ 1.9825, -2.8032]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.4208,  2.8726],\n",
            "        [ 2.1499, -2.8332],\n",
            "        [-1.5717,  2.0831],\n",
            "        [-1.7778,  2.2424],\n",
            "        [-2.1559,  2.6691],\n",
            "        [-2.3957,  2.7943],\n",
            "        [-1.8262,  2.3041],\n",
            "        [-2.3639,  2.6082],\n",
            "        [ 0.3717, -0.1390],\n",
            "        [-2.3491,  2.8178],\n",
            "        [-1.1236,  1.4223],\n",
            "        [-1.4397,  1.8960],\n",
            "        [-1.7989,  2.4393],\n",
            "        [-1.8004,  2.2504],\n",
            "        [-2.5567,  2.9884],\n",
            "        [ 1.0216, -0.5815],\n",
            "        [-2.5671,  2.9348],\n",
            "        [-2.5970,  2.9372],\n",
            "        [-2.5439,  2.8244],\n",
            "        [-1.9619,  2.5499],\n",
            "        [-2.6596,  3.1255],\n",
            "        [-2.3327,  2.8269],\n",
            "        [-2.5973,  2.9129],\n",
            "        [-2.5620,  2.9208],\n",
            "        [-2.4055,  2.7876],\n",
            "        [-2.3665,  2.5971],\n",
            "        [-2.5916,  2.9146],\n",
            "        [-0.7480,  1.2975],\n",
            "        [ 1.6348, -1.4546],\n",
            "        [-1.4740,  2.0419],\n",
            "        [ 2.3731, -2.8068],\n",
            "        [-2.5265,  2.8710]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.1971,  2.6970],\n",
            "        [-2.3913,  2.8374],\n",
            "        [ 1.3137, -1.0480],\n",
            "        [ 2.1367, -2.6466],\n",
            "        [-2.3998,  2.9187],\n",
            "        [-0.9135,  1.4635],\n",
            "        [-2.3577,  2.7443],\n",
            "        [-1.9146,  2.4822],\n",
            "        [ 0.5250, -0.1226],\n",
            "        [ 0.6720, -0.2988],\n",
            "        [-1.3181,  1.4758],\n",
            "        [-0.2598,  0.4810],\n",
            "        [-2.3134,  2.6811],\n",
            "        [-2.4177,  2.7437],\n",
            "        [ 1.8153, -2.6449],\n",
            "        [ 2.0074, -2.6753],\n",
            "        [ 2.0403, -2.8716],\n",
            "        [-2.5936,  2.9065],\n",
            "        [ 0.2575,  0.1343],\n",
            "        [-1.5702,  1.8860],\n",
            "        [-1.7863,  2.4577],\n",
            "        [-2.2287,  2.8997],\n",
            "        [ 2.0158, -2.8101],\n",
            "        [-2.5122,  2.9436],\n",
            "        [-1.3730,  1.4767],\n",
            "        [-1.0904,  1.6011],\n",
            "        [ 1.3181, -1.5892],\n",
            "        [-2.3089,  2.6661],\n",
            "        [-0.7227,  0.8071],\n",
            "        [-0.1406,  0.6981],\n",
            "        [-2.6114,  3.0410],\n",
            "        [-2.4223,  2.8606]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-0.8396,  1.5629],\n",
            "        [-2.2733,  2.9029],\n",
            "        [-2.5685,  2.9286],\n",
            "        [-2.5473,  2.9570],\n",
            "        [-0.3238,  0.4935],\n",
            "        [-1.4059,  2.1569],\n",
            "        [-2.0456,  2.4011],\n",
            "        [-2.4772,  2.9078],\n",
            "        [-2.0247,  2.4232],\n",
            "        [-1.9584,  2.3168],\n",
            "        [-2.1040,  2.5898],\n",
            "        [-1.7901,  2.0223],\n",
            "        [ 2.0176, -2.3761],\n",
            "        [-1.0427,  1.2079],\n",
            "        [-2.3985,  2.7387],\n",
            "        [-2.4407,  2.8042],\n",
            "        [-2.1022,  2.7455],\n",
            "        [ 1.9188, -1.7369],\n",
            "        [ 1.8550, -2.7631],\n",
            "        [-2.1335,  2.7037],\n",
            "        [-2.4177,  2.7454],\n",
            "        [-1.6328,  1.9711],\n",
            "        [-2.1860,  2.6854],\n",
            "        [ 1.9257, -2.1362],\n",
            "        [-1.8018,  2.2740],\n",
            "        [-2.5882,  2.8692],\n",
            "        [-1.4927,  1.6410],\n",
            "        [-2.4906,  2.7996],\n",
            "        [-2.4889,  2.8739],\n",
            "        [ 1.5576, -1.4874],\n",
            "        [ 1.9925, -2.2854],\n",
            "        [-2.4476,  2.9050]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1197,  0.3071],\n",
            "        [-2.3041,  2.7246],\n",
            "        [-2.5542,  3.0449],\n",
            "        [-2.2062,  2.6140],\n",
            "        [-2.4361,  2.8132],\n",
            "        [-2.4005,  3.1059],\n",
            "        [-2.0055,  2.4077],\n",
            "        [-0.0764,  0.4959],\n",
            "        [-1.9128,  2.1343],\n",
            "        [-1.2066,  1.4926],\n",
            "        [-2.5176,  2.8295],\n",
            "        [-2.4066,  2.7427],\n",
            "        [-2.5574,  3.0030],\n",
            "        [-2.5140,  2.8833],\n",
            "        [-1.1580,  1.8740],\n",
            "        [-1.9733,  2.1614],\n",
            "        [ 2.3316, -2.9379],\n",
            "        [ 1.3327, -1.1777],\n",
            "        [-0.7519,  1.1278],\n",
            "        [-2.0380,  2.5763],\n",
            "        [-2.4230,  2.8319],\n",
            "        [-1.8944,  2.4245],\n",
            "        [-2.1101,  2.4526],\n",
            "        [ 0.3189,  0.1388],\n",
            "        [ 2.1689, -2.9226],\n",
            "        [-2.3160,  2.7325],\n",
            "        [-2.5122,  2.8526],\n",
            "        [-2.5510,  3.0058],\n",
            "        [ 0.6527, -0.4143],\n",
            "        [-2.3195,  2.6270],\n",
            "        [ 1.6590, -2.4145],\n",
            "        [ 1.8598, -2.1249]], device='cuda:0'), hidden_states=None, attentions=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 4/4 [10:28<00:00, 157.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2875, -2.9416],\n",
            "        [-1.4415,  2.1151],\n",
            "        [-2.4415,  2.7395],\n",
            "        [ 1.0677, -0.7926],\n",
            "        [ 2.2810, -2.8993],\n",
            "        [ 1.1811, -0.9806],\n",
            "        [-2.5466,  2.9470],\n",
            "        [-2.0956,  2.4087],\n",
            "        [-2.5639,  2.9345],\n",
            "        [-2.3849,  2.8338],\n",
            "        [ 1.1392, -0.9097],\n",
            "        [-1.4443,  1.8300],\n",
            "        [ 1.9550, -2.7038],\n",
            "        [-1.7766,  2.1257],\n",
            "        [ 2.1590, -2.8152],\n",
            "        [-1.4583,  1.8037],\n",
            "        [-2.1617,  2.2540],\n",
            "        [-2.5076,  2.9987],\n",
            "        [-2.4917,  2.9153],\n",
            "        [-1.4980,  2.1800],\n",
            "        [-2.4459,  2.7135],\n",
            "        [-2.5455,  2.8907],\n",
            "        [-2.5096,  2.9251],\n",
            "        [-1.9236,  2.3773],\n",
            "        [ 1.0269, -0.6266],\n",
            "        [-1.3278,  1.5378],\n",
            "        [-2.4794,  2.7992],\n",
            "        [ 1.7625, -2.3567],\n",
            "        [-0.1507,  0.7282],\n",
            "        [ 0.6723,  0.0201],\n",
            "        [-2.4175,  3.0769],\n",
            "        [-2.3754,  2.8419]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            " the logit is : SequenceClassifierOutput(loss=None, logits=tensor([[-2.4241,  2.7155],\n",
            "        [ 2.0066, -2.5001],\n",
            "        [-2.4412,  2.8224],\n",
            "        [ 0.8619, -0.4325],\n",
            "        [-2.4776,  2.8473],\n",
            "        [-2.5333,  2.9968],\n",
            "        [-0.9079,  1.5559],\n",
            "        [-2.5126,  2.9327],\n",
            "        [ 0.9718, -0.7692],\n",
            "        [-2.5274,  2.8214],\n",
            "        [-2.4304,  2.8663],\n",
            "        [-0.6471,  0.4592],\n",
            "        [-2.4655,  2.8156],\n",
            "        [-2.3039,  2.8031],\n",
            "        [-1.7780,  1.9681],\n",
            "        [-2.2517,  2.6561],\n",
            "        [ 1.0422, -0.8069],\n",
            "        [ 1.9596, -2.2088],\n",
            "        [-1.7994,  2.2122],\n",
            "        [-2.4524,  2.8412],\n",
            "        [-2.4175,  2.9268],\n",
            "        [-1.6986,  1.7109],\n",
            "        [-2.3063,  2.8560],\n",
            "        [-2.4557,  2.6175]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Validation Accuracy: 0.820216049382716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_loss_set)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "-m_IFZ-oPx-R",
        "outputId": "ab60a7da-9fa1-4581-c5ab-17096f3a6eb8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        " predicti modek ( text data )\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "dMI6oP-t7cMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/ArmanArabi/Fine-Tunning-Bert-model/main/out_of_domain_dev.tsv\n"
      ],
      "metadata": {
        "id": "jmLPgCe_7aog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66302d80-b4a8-4429-f7d5-0ed573fb0aa0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-05 22:25:04--  https://raw.githubusercontent.com/PacktPublishing/Transformers-for-Natural-Language-Processing/main/Chapter02/out_of_domain_dev.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28506 (28K) [text/plain]\n",
            "Saving to: ‘out_of_domain_dev.tsv’\n",
            "\n",
            "\rout_of_domain_dev.t   0%[                    ]       0  --.-KB/s               \rout_of_domain_dev.t 100%[===================>]  27.84K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-05 22:25:04 (125 MB/s) - ‘out_of_domain_dev.tsv’ saved [28506/28506]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('/content/out_of_domain_dev.tsv', delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "df_test.shape"
      ],
      "metadata": {
        "id": "XOG26Jom6kdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a62501-9c1b-4ebe-b38e-a702406b12d5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(516, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = df_test.sentence.values\n",
        "sentences = [\"[CLS]\" + sentence + \"[SEP]\" for sentence in sentences]\n",
        "labels = df_test.label.values\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "MAX_LEN = 128\n",
        "\n",
        "## Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "## Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\", dtype=\"long\" )\n",
        "\n",
        "## Create attention masks\n",
        "attention_masks = []\n",
        "## Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "# Print the shapes of the tensors\n",
        "print(f'prediction_inputs shape is: {prediction_inputs.shape}')\n",
        "print(f'prediction_masks shape is: {prediction_masks.shape}')\n",
        "print(f'prediction_labels shape is: {prediction_labels.shape}')\n",
        "\n",
        "assert prediction_inputs.shape[0] == prediction_masks.shape[0] == prediction_labels.shape[0], \"Size mismatch between tensors\"\n",
        "\n",
        "batch_size = 32\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbKJXpxpdmta",
        "outputId": "011033bb-48aa-4324-8464-015fdf9b8dd9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction_inputs shape is: torch.Size([516, 128])\n",
            "prediction_masks shape is: torch.Size([516, 128])\n",
            "prediction_labels shape is: torch.Size([516])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Prediction on test set\n",
        "\n",
        "## Put model in evaluation mode\n",
        "model.eval()\n",
        "## Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "for batch in prediction_dataloader :\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    print(logits)\n",
        "    logits = logits['logits'].detach().cpu().numpy()\n",
        "    print('///////////////')\n",
        "    print(logits)\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaR63fiOarSe",
        "outputId": "a1470ae8-1e2f-4ff0-c233-8016015dcaec"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SequenceClassifierOutput(loss=tensor(1.5943, device='cuda:0'), logits=tensor([[-2.3667,  2.9888],\n",
            "        [-2.3246,  2.6590],\n",
            "        [-2.4487,  2.8577],\n",
            "        [-2.4447,  2.8526],\n",
            "        [-1.4764,  2.1848],\n",
            "        [-2.5237,  2.9601],\n",
            "        [-1.3542,  1.7693],\n",
            "        [-1.9355,  2.3478],\n",
            "        [-2.5263,  2.9035],\n",
            "        [-2.2986,  2.5245],\n",
            "        [-1.5651,  2.1208],\n",
            "        [ 0.5683, -0.1312],\n",
            "        [-2.4480,  2.6592],\n",
            "        [-1.1518,  1.6725],\n",
            "        [-1.8069,  2.4435],\n",
            "        [-1.6855,  1.9718],\n",
            "        [-1.9521,  2.3329],\n",
            "        [ 2.2268, -2.8820],\n",
            "        [-2.0459,  2.5436],\n",
            "        [-1.5197,  2.2030],\n",
            "        [-2.0459,  2.5436],\n",
            "        [-1.4892,  1.9869],\n",
            "        [-1.6769,  2.1783],\n",
            "        [-1.4759,  2.1160],\n",
            "        [-0.8445,  1.5363],\n",
            "        [-2.4340,  2.7424],\n",
            "        [-2.0391,  2.5215],\n",
            "        [-2.2652,  2.7129],\n",
            "        [-2.2498,  2.7703],\n",
            "        [-2.1059,  2.4399],\n",
            "        [-2.2979,  2.6946],\n",
            "        [-1.6741,  2.0105]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[-2.3667161   2.9887593 ]\n",
            " [-2.3246377   2.6589677 ]\n",
            " [-2.448745    2.8577156 ]\n",
            " [-2.4446552   2.8526466 ]\n",
            " [-1.4763736   2.1847947 ]\n",
            " [-2.5237384   2.9600582 ]\n",
            " [-1.3542143   1.7692617 ]\n",
            " [-1.9355425   2.347785  ]\n",
            " [-2.5262933   2.903539  ]\n",
            " [-2.298646    2.524526  ]\n",
            " [-1.5650891   2.120837  ]\n",
            " [ 0.56832767 -0.13119699]\n",
            " [-2.4479601   2.6591785 ]\n",
            " [-1.1517931   1.6724852 ]\n",
            " [-1.8068756   2.4435334 ]\n",
            " [-1.6855415   1.9718019 ]\n",
            " [-1.9521285   2.332906  ]\n",
            " [ 2.226754   -2.882006  ]\n",
            " [-2.045903    2.5435717 ]\n",
            " [-1.5196745   2.2030115 ]\n",
            " [-2.045903    2.5435717 ]\n",
            " [-1.4892464   1.986869  ]\n",
            " [-1.6769371   2.178336  ]\n",
            " [-1.4759094   2.1159623 ]\n",
            " [-0.84453374  1.5363446 ]\n",
            " [-2.434009    2.7423956 ]\n",
            " [-2.0390992   2.5214593 ]\n",
            " [-2.2652073   2.7128537 ]\n",
            " [-2.249759    2.7702518 ]\n",
            " [-2.1058836   2.4399421 ]\n",
            " [-2.297874    2.6946359 ]\n",
            " [-1.6741391   2.0105386 ]]\n",
            "SequenceClassifierOutput(loss=tensor(1.6267, device='cuda:0'), logits=tensor([[-1.2525,  1.5993],\n",
            "        [-1.6065,  2.1681],\n",
            "        [-1.2657,  1.2557],\n",
            "        [-2.4016,  2.7255],\n",
            "        [-2.1229,  2.3315],\n",
            "        [-2.1848,  2.8171],\n",
            "        [-2.3207,  2.7180],\n",
            "        [-2.4669,  2.8661],\n",
            "        [-0.2627,  0.8240],\n",
            "        [-2.0902,  2.5092],\n",
            "        [-2.1211,  2.5086],\n",
            "        [ 0.7019, -0.4744],\n",
            "        [-2.2163,  2.6861],\n",
            "        [-0.3657,  1.0057],\n",
            "        [-0.1521,  0.7179],\n",
            "        [-0.1606,  0.6789],\n",
            "        [-1.6832,  2.2685],\n",
            "        [-2.5207,  2.7595],\n",
            "        [-1.8474,  2.3066],\n",
            "        [-1.5514,  2.0601],\n",
            "        [ 0.9099, -0.6831],\n",
            "        [ 2.0619, -2.4101],\n",
            "        [-2.3958,  2.7918],\n",
            "        [-2.1390,  2.6602],\n",
            "        [-2.3856,  2.9899],\n",
            "        [-2.3856,  2.9899],\n",
            "        [-2.4388,  2.7003],\n",
            "        [-1.7522,  2.3148],\n",
            "        [-2.0444,  2.5385],\n",
            "        [-2.5314,  2.9768],\n",
            "        [-1.7522,  2.3148],\n",
            "        [-2.3479,  2.8589]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[-1.2524724   1.5993099 ]\n",
            " [-1.6065103   2.1681254 ]\n",
            " [-1.2656535   1.2557238 ]\n",
            " [-2.4016063   2.7255192 ]\n",
            " [-2.1229203   2.3315187 ]\n",
            " [-2.1848469   2.8171    ]\n",
            " [-2.3207233   2.718014  ]\n",
            " [-2.4668753   2.8660655 ]\n",
            " [-0.26265442  0.8240412 ]\n",
            " [-2.0902083   2.509218  ]\n",
            " [-2.1211343   2.5085914 ]\n",
            " [ 0.7019284  -0.47444865]\n",
            " [-2.2163396   2.6860769 ]\n",
            " [-0.36573437  1.0057486 ]\n",
            " [-0.1520985   0.7178602 ]\n",
            " [-0.16062757  0.6788744 ]\n",
            " [-1.6832408   2.2685153 ]\n",
            " [-2.5207226   2.7595046 ]\n",
            " [-1.8473712   2.3065584 ]\n",
            " [-1.5513943   2.060084  ]\n",
            " [ 0.9099071  -0.6830937 ]\n",
            " [ 2.0618804  -2.4101422 ]\n",
            " [-2.3957677   2.7918375 ]\n",
            " [-2.1390069   2.6601923 ]\n",
            " [-2.3855705   2.9899354 ]\n",
            " [-2.3855705   2.9899354 ]\n",
            " [-2.4387708   2.7002652 ]\n",
            " [-1.7521982   2.3148139 ]\n",
            " [-2.044423    2.5385242 ]\n",
            " [-2.5313773   2.9767916 ]\n",
            " [-1.7521982   2.3148139 ]\n",
            " [-2.34794     2.8588896 ]]\n",
            "SequenceClassifierOutput(loss=tensor(1.1925, device='cuda:0'), logits=tensor([[-2.3019,  2.8286],\n",
            "        [-2.2857,  2.7054],\n",
            "        [-2.2027,  2.7189],\n",
            "        [-2.3069,  2.7796],\n",
            "        [-2.0899,  2.6221],\n",
            "        [-1.2737,  1.7502],\n",
            "        [-0.7888,  1.3588],\n",
            "        [-1.7664,  2.4928],\n",
            "        [-2.5965,  3.0586],\n",
            "        [-2.1732,  2.5427],\n",
            "        [-2.5769,  3.0061],\n",
            "        [-1.6859,  2.0230],\n",
            "        [-1.6343,  1.8001],\n",
            "        [ 2.3096, -2.8912],\n",
            "        [-2.3643,  2.4426],\n",
            "        [-2.3767,  2.7542],\n",
            "        [-1.4648,  2.1210],\n",
            "        [ 1.7493, -1.8732],\n",
            "        [-2.3798,  2.8911],\n",
            "        [-2.6848,  3.1134],\n",
            "        [-2.1867,  2.5955],\n",
            "        [-2.6075,  2.9415],\n",
            "        [ 2.2306, -2.8579],\n",
            "        [ 1.9853, -2.1681],\n",
            "        [-2.5212,  2.9564],\n",
            "        [-1.5359,  2.0649],\n",
            "        [-2.4491,  2.5977],\n",
            "        [ 1.7955, -2.1326],\n",
            "        [ 1.3558, -1.3215],\n",
            "        [ 2.2246, -2.9094],\n",
            "        [-2.3121,  2.6597],\n",
            "        [ 2.0462, -2.7895]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[-2.3019464  2.8286068]\n",
            " [-2.2856758  2.7053573]\n",
            " [-2.2027304  2.7188652]\n",
            " [-2.3068976  2.7795722]\n",
            " [-2.0899367  2.622093 ]\n",
            " [-1.2736694  1.7501887]\n",
            " [-0.7888065  1.358805 ]\n",
            " [-1.7664025  2.4928472]\n",
            " [-2.5964696  3.0586462]\n",
            " [-2.1731517  2.5427148]\n",
            " [-2.576943   3.0061042]\n",
            " [-1.6858739  2.023012 ]\n",
            " [-1.6342808  1.8001434]\n",
            " [ 2.3095548 -2.8912258]\n",
            " [-2.3643112  2.442576 ]\n",
            " [-2.3767092  2.7542393]\n",
            " [-1.4648154  2.1209931]\n",
            " [ 1.7493402 -1.8731542]\n",
            " [-2.3797858  2.8910751]\n",
            " [-2.6847627  3.1134322]\n",
            " [-2.186676   2.5954692]\n",
            " [-2.60754    2.9414976]\n",
            " [ 2.2306302 -2.8578546]\n",
            " [ 1.9852896 -2.1681285]\n",
            " [-2.5212169  2.9564257]\n",
            " [-1.5359207  2.0649164]\n",
            " [-2.4491353  2.5977132]\n",
            " [ 1.7955464 -2.132559 ]\n",
            " [ 1.3558378 -1.3215417]\n",
            " [ 2.224607  -2.909383 ]\n",
            " [-2.312066   2.6597257]\n",
            " [ 2.04622   -2.7894971]]\n",
            "SequenceClassifierOutput(loss=tensor(0.9803, device='cuda:0'), logits=tensor([[-2.5166,  2.8210],\n",
            "        [ 1.8818, -2.3520],\n",
            "        [ 2.0663, -2.5925],\n",
            "        [-1.5591,  1.9722],\n",
            "        [-2.5451,  3.0226],\n",
            "        [-2.5408,  2.9806],\n",
            "        [-2.2998,  2.7744],\n",
            "        [-0.0715,  0.3940],\n",
            "        [-2.5814,  2.9744],\n",
            "        [-2.5405,  2.8748],\n",
            "        [-2.0527,  2.4215],\n",
            "        [-1.9511,  2.4075],\n",
            "        [-2.3555,  2.6894],\n",
            "        [-1.5359,  1.6247],\n",
            "        [-0.5845,  0.4913],\n",
            "        [-2.2163,  2.5316],\n",
            "        [-2.4377,  2.7212],\n",
            "        [-2.3004,  2.4947],\n",
            "        [-2.1423,  2.4202],\n",
            "        [-2.4817,  2.7222],\n",
            "        [-2.3040,  2.5985],\n",
            "        [ 0.2999,  0.1276],\n",
            "        [ 1.6182, -1.8519],\n",
            "        [-0.6949,  1.3655],\n",
            "        [-2.4205,  2.8583],\n",
            "        [-2.5595,  2.9108],\n",
            "        [-2.5125,  2.7796],\n",
            "        [-1.7823,  2.2881],\n",
            "        [ 1.6849, -2.0757],\n",
            "        [ 1.2544, -1.4880],\n",
            "        [-2.5106,  2.8682],\n",
            "        [-0.6523,  0.9827]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[-2.5166047   2.8210082 ]\n",
            " [ 1.8818079  -2.3519995 ]\n",
            " [ 2.0662682  -2.592456  ]\n",
            " [-1.559131    1.9721855 ]\n",
            " [-2.545121    3.0226142 ]\n",
            " [-2.5408063   2.9806378 ]\n",
            " [-2.2997591   2.7744014 ]\n",
            " [-0.07147191  0.39404032]\n",
            " [-2.5814435   2.9743912 ]\n",
            " [-2.5404603   2.874818  ]\n",
            " [-2.0526612   2.4214833 ]\n",
            " [-1.951072    2.407507  ]\n",
            " [-2.3555424   2.6893554 ]\n",
            " [-1.5358505   1.6246946 ]\n",
            " [-0.584539    0.49131685]\n",
            " [-2.2163484   2.5316076 ]\n",
            " [-2.4377136   2.7212026 ]\n",
            " [-2.3003805   2.4946935 ]\n",
            " [-2.1423476   2.4201727 ]\n",
            " [-2.4816601   2.722178  ]\n",
            " [-2.3039706   2.59849   ]\n",
            " [ 0.2999493   0.12755087]\n",
            " [ 1.6182495  -1.8519318 ]\n",
            " [-0.6949435   1.3655235 ]\n",
            " [-2.4204783   2.8582625 ]\n",
            " [-2.5594757   2.910842  ]\n",
            " [-2.5124547   2.7795732 ]\n",
            " [-1.7822542   2.288105  ]\n",
            " [ 1.6849385  -2.0757153 ]\n",
            " [ 1.2543726  -1.4880413 ]\n",
            " [-2.5106087   2.868245  ]\n",
            " [-0.6522779   0.9826514 ]]\n",
            "SequenceClassifierOutput(loss=tensor(0.8763, device='cuda:0'), logits=tensor([[-1.4011,  1.3003],\n",
            "        [ 0.0578, -0.0349],\n",
            "        [-2.3733,  2.8673],\n",
            "        [ 0.1334,  0.2360],\n",
            "        [-2.2034,  2.5538],\n",
            "        [-1.9060,  2.2144],\n",
            "        [-0.1805,  0.5530],\n",
            "        [-2.5681,  3.0162],\n",
            "        [-2.0730,  2.4218],\n",
            "        [-1.8656,  2.4975],\n",
            "        [-1.9461,  2.0985],\n",
            "        [-2.6098,  2.9819],\n",
            "        [-2.3136,  2.7742],\n",
            "        [-2.5670,  2.9388],\n",
            "        [-2.2424,  2.6701],\n",
            "        [-2.3782,  2.6703],\n",
            "        [-1.5181,  1.9850],\n",
            "        [-0.4845,  0.7593],\n",
            "        [-1.1579,  1.8337],\n",
            "        [-1.2745,  1.8768],\n",
            "        [ 0.3313, -0.1621],\n",
            "        [-0.2533,  0.4549],\n",
            "        [-1.3429,  1.9753],\n",
            "        [-0.6733,  0.8848],\n",
            "        [-2.4291,  2.7979],\n",
            "        [-2.5649,  2.9243],\n",
            "        [-2.2204,  2.6058],\n",
            "        [-2.6124,  2.9515],\n",
            "        [-2.5761,  2.9059],\n",
            "        [-2.2321,  2.5959],\n",
            "        [ 1.6658, -1.8779],\n",
            "        [-2.4444,  2.8083]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[-1.4011167   1.3003362 ]\n",
            " [ 0.05780534 -0.03488655]\n",
            " [-2.3732507   2.8673267 ]\n",
            " [ 0.13340482  0.23595485]\n",
            " [-2.2033713   2.5538168 ]\n",
            " [-1.9059883   2.2143793 ]\n",
            " [-0.18047443  0.552965  ]\n",
            " [-2.5680614   3.01617   ]\n",
            " [-2.0729527   2.42182   ]\n",
            " [-1.8656429   2.4974988 ]\n",
            " [-1.9461141   2.098509  ]\n",
            " [-2.609804    2.981894  ]\n",
            " [-2.313604    2.7741985 ]\n",
            " [-2.5670009   2.938823  ]\n",
            " [-2.2423503   2.6701477 ]\n",
            " [-2.3781908   2.6702778 ]\n",
            " [-1.5180627   1.9850377 ]\n",
            " [-0.4845402   0.7592549 ]\n",
            " [-1.157874    1.8337003 ]\n",
            " [-1.2744694   1.8767926 ]\n",
            " [ 0.33132485 -0.16211963]\n",
            " [-0.25331545  0.45486236]\n",
            " [-1.3428569   1.9752938 ]\n",
            " [-0.6732926   0.8848104 ]\n",
            " [-2.4290528   2.7979462 ]\n",
            " [-2.5648713   2.924318  ]\n",
            " [-2.2204034   2.6057932 ]\n",
            " [-2.6123695   2.9515307 ]\n",
            " [-2.5761      2.9059124 ]\n",
            " [-2.2320845   2.5958512 ]\n",
            " [ 1.6657585  -1.8779436 ]\n",
            " [-2.444434    2.8083475 ]]\n",
            "SequenceClassifierOutput(loss=tensor(0.5487, device='cuda:0'), logits=tensor([[-2.2690,  2.7100],\n",
            "        [-1.8802,  2.3535],\n",
            "        [-2.0506,  2.5168],\n",
            "        [-2.1952,  2.6940],\n",
            "        [-2.4142,  2.9252],\n",
            "        [ 2.0318, -2.1963],\n",
            "        [ 2.3155, -2.8038],\n",
            "        [ 0.9814, -0.6197],\n",
            "        [ 2.1317, -2.5323],\n",
            "        [ 1.5129, -1.3863],\n",
            "        [-2.5827,  2.9618],\n",
            "        [-2.1066,  2.4914],\n",
            "        [-1.1133,  1.7489],\n",
            "        [ 2.0025, -2.1936],\n",
            "        [-2.2399,  2.6792],\n",
            "        [ 1.8898, -2.1474],\n",
            "        [-2.3944,  2.8502],\n",
            "        [-1.0801,  1.5004],\n",
            "        [-2.4501,  2.8527],\n",
            "        [-2.3142,  2.7675],\n",
            "        [ 2.2425, -2.8748],\n",
            "        [ 2.2267, -2.8378],\n",
            "        [ 2.0390, -2.4082],\n",
            "        [-2.0959,  2.7339],\n",
            "        [-2.2376,  2.7600],\n",
            "        [-2.6611,  2.9816],\n",
            "        [-2.2414,  2.7553],\n",
            "        [-1.0477,  1.5596],\n",
            "        [-2.3627,  2.8062],\n",
            "        [-2.5436,  2.8868],\n",
            "        [-2.4499,  2.7413],\n",
            "        [-0.9843,  1.5147]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[-2.2690198  2.7099507]\n",
            " [-1.88016    2.3534865]\n",
            " [-2.0505502  2.516754 ]\n",
            " [-2.1952434  2.6940377]\n",
            " [-2.4142354  2.925248 ]\n",
            " [ 2.0317953 -2.1962786]\n",
            " [ 2.3155313 -2.8038208]\n",
            " [ 0.981411  -0.6196719]\n",
            " [ 2.131661  -2.5323362]\n",
            " [ 1.5128672 -1.3863242]\n",
            " [-2.5827012  2.9617612]\n",
            " [-2.106587   2.491424 ]\n",
            " [-1.1133236  1.7489067]\n",
            " [ 2.002493  -2.193604 ]\n",
            " [-2.2398999  2.6791623]\n",
            " [ 1.8898448 -2.1473985]\n",
            " [-2.3944237  2.8502214]\n",
            " [-1.0800997  1.5003976]\n",
            " [-2.4501307  2.852679 ]\n",
            " [-2.314226   2.767545 ]\n",
            " [ 2.242528  -2.8748457]\n",
            " [ 2.2266743 -2.8377929]\n",
            " [ 2.0390081 -2.4082394]\n",
            " [-2.095926   2.733948 ]\n",
            " [-2.2375534  2.75999  ]\n",
            " [-2.6611004  2.9815836]\n",
            " [-2.2414143  2.7553136]\n",
            " [-1.0477477  1.5596293]\n",
            " [-2.3626544  2.806202 ]\n",
            " [-2.5435941  2.886824 ]\n",
            " [-2.449941   2.741254 ]\n",
            " [-0.9842733  1.5147492]]\n",
            "SequenceClassifierOutput(loss=tensor(0.8792, device='cuda:0'), logits=tensor([[-2.5842,  2.9639],\n",
            "        [ 1.8345, -2.1460],\n",
            "        [-2.5710,  2.9498],\n",
            "        [-2.2012,  2.8750],\n",
            "        [-0.9827,  1.3906],\n",
            "        [-2.3535,  2.7291],\n",
            "        [-1.1309,  1.6650],\n",
            "        [-2.5542,  2.7655],\n",
            "        [-1.7513,  2.1327],\n",
            "        [ 1.1594, -0.9218],\n",
            "        [ 1.6659, -1.9053],\n",
            "        [ 1.5543, -1.5734],\n",
            "        [ 1.3384, -1.4439],\n",
            "        [-2.3485,  2.7621],\n",
            "        [ 0.0149,  0.3294],\n",
            "        [-0.0998, -0.1998],\n",
            "        [-2.4362,  2.8838],\n",
            "        [-2.5227,  2.9720],\n",
            "        [-2.4026,  2.7734],\n",
            "        [-2.4151,  2.9219],\n",
            "        [-2.6004,  2.9822],\n",
            "        [ 0.9368, -0.5821],\n",
            "        [-0.6547,  1.2230],\n",
            "        [-0.5591,  1.0397],\n",
            "        [-2.5566,  2.8092],\n",
            "        [-2.5656,  2.8580],\n",
            "        [-2.5710,  2.8421],\n",
            "        [ 0.6075, -0.4246],\n",
            "        [-2.1127,  2.5760],\n",
            "        [-2.2465,  2.6634],\n",
            "        [ 1.5609, -1.5813],\n",
            "        [-2.6440,  3.0079]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[-2.584223    2.9639466 ]\n",
            " [ 1.8344635  -2.1459773 ]\n",
            " [-2.5710318   2.949796  ]\n",
            " [-2.2011638   2.874994  ]\n",
            " [-0.9826557   1.3905721 ]\n",
            " [-2.353478    2.729145  ]\n",
            " [-1.1308509   1.6649528 ]\n",
            " [-2.554166    2.7654724 ]\n",
            " [-1.7513236   2.1327221 ]\n",
            " [ 1.1594133  -0.921846  ]\n",
            " [ 1.6659251  -1.905319  ]\n",
            " [ 1.554326   -1.5733842 ]\n",
            " [ 1.3384168  -1.4439226 ]\n",
            " [-2.348478    2.7620704 ]\n",
            " [ 0.01491525  0.32937276]\n",
            " [-0.09977321 -0.19982645]\n",
            " [-2.4362004   2.883819  ]\n",
            " [-2.5226707   2.9719565 ]\n",
            " [-2.4025772   2.7733905 ]\n",
            " [-2.4150605   2.9219306 ]\n",
            " [-2.6004243   2.982162  ]\n",
            " [ 0.93679774 -0.582079  ]\n",
            " [-0.6546927   1.2229518 ]\n",
            " [-0.55914825  1.0396991 ]\n",
            " [-2.556602    2.8092124 ]\n",
            " [-2.565649    2.857977  ]\n",
            " [-2.5710285   2.8420653 ]\n",
            " [ 0.60746795 -0.4246336 ]\n",
            " [-2.1126513   2.5759742 ]\n",
            " [-2.246518    2.6633737 ]\n",
            " [ 1.5608515  -1.5812715 ]\n",
            " [-2.6439612   3.0079286 ]]\n",
            "SequenceClassifierOutput(loss=tensor(0.3439, device='cuda:0'), logits=tensor([[-0.0953,  0.3512],\n",
            "        [-2.5201,  2.9380],\n",
            "        [-2.4900,  2.9792],\n",
            "        [-2.2107,  2.5861],\n",
            "        [-1.4347,  2.0812],\n",
            "        [-1.7651,  2.4490],\n",
            "        [-0.3961,  0.5376],\n",
            "        [-2.3964,  2.8935],\n",
            "        [-1.6583,  2.1110],\n",
            "        [-2.5565,  2.9296],\n",
            "        [-2.5719,  3.0088],\n",
            "        [-2.4785,  3.1212],\n",
            "        [-2.5647,  2.9777],\n",
            "        [-1.0578,  1.8210],\n",
            "        [-2.2767,  2.6853],\n",
            "        [-2.4917,  2.7780],\n",
            "        [-2.4211,  2.7162],\n",
            "        [-2.2766,  2.9183],\n",
            "        [-2.5342,  2.8462],\n",
            "        [-2.4627,  2.9186],\n",
            "        [-2.4639,  2.7401],\n",
            "        [-2.2601,  2.8916],\n",
            "        [-2.4906,  2.8632],\n",
            "        [-2.4431,  2.9375],\n",
            "        [-2.4835,  2.7995],\n",
            "        [-2.4047,  2.9932],\n",
            "        [-2.5732,  2.9866],\n",
            "        [-2.4880,  2.9957],\n",
            "        [-0.5387,  1.0464],\n",
            "        [-2.3246,  2.7233],\n",
            "        [-1.3431,  1.5673],\n",
            "        [-2.0710,  2.4083]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[-0.09532613  0.35119095]\n",
            " [-2.5201142   2.9380248 ]\n",
            " [-2.4899867   2.9791913 ]\n",
            " [-2.2106671   2.586136  ]\n",
            " [-1.4347316   2.0812197 ]\n",
            " [-1.7651464   2.4489877 ]\n",
            " [-0.39613587  0.5375689 ]\n",
            " [-2.3963964   2.8935392 ]\n",
            " [-1.6582676   2.1109755 ]\n",
            " [-2.5565388   2.929582  ]\n",
            " [-2.571888    3.0087712 ]\n",
            " [-2.478489    3.1211636 ]\n",
            " [-2.5647368   2.977702  ]\n",
            " [-1.0578138   1.8209714 ]\n",
            " [-2.2766933   2.6852639 ]\n",
            " [-2.4917438   2.778048  ]\n",
            " [-2.4210806   2.7162063 ]\n",
            " [-2.2766218   2.9183252 ]\n",
            " [-2.5342045   2.846176  ]\n",
            " [-2.4627445   2.9185994 ]\n",
            " [-2.463901    2.7400591 ]\n",
            " [-2.260077    2.8915884 ]\n",
            " [-2.4905636   2.86317   ]\n",
            " [-2.4431295   2.9375176 ]\n",
            " [-2.4835107   2.7995331 ]\n",
            " [-2.404706    2.9932258 ]\n",
            " [-2.5731554   2.986612  ]\n",
            " [-2.4880025   2.9957426 ]\n",
            " [-0.5387187   1.0463854 ]\n",
            " [-2.3246305   2.7232845 ]\n",
            " [-1.3431054   1.567346  ]\n",
            " [-2.0709698   2.4082792 ]]\n",
            "SequenceClassifierOutput(loss=tensor(0.1094, device='cuda:0'), logits=tensor([[ 1.2434, -0.9600],\n",
            "        [-2.1713,  2.6232],\n",
            "        [ 1.3377, -1.5614],\n",
            "        [-2.3010,  2.8599],\n",
            "        [-0.1888,  0.1848],\n",
            "        [-1.7108,  2.1230],\n",
            "        [-2.5295,  2.9169],\n",
            "        [-2.4999,  2.8481],\n",
            "        [-2.5347,  2.8838],\n",
            "        [-2.5904,  2.8817],\n",
            "        [-2.5841,  2.8932],\n",
            "        [-2.5796,  2.8705],\n",
            "        [-2.5597,  2.8050],\n",
            "        [-2.5313,  2.8738],\n",
            "        [-2.5554,  2.8174],\n",
            "        [-2.5191,  2.7790],\n",
            "        [-2.5117,  2.8600],\n",
            "        [-2.5112,  2.7747],\n",
            "        [-2.5491,  2.8055],\n",
            "        [-2.5067,  2.7707],\n",
            "        [-2.4620,  2.9321],\n",
            "        [-1.6262,  1.8847],\n",
            "        [-2.2424,  2.6484],\n",
            "        [-2.1028,  2.5471],\n",
            "        [-0.9229,  0.9308],\n",
            "        [-1.8177,  2.5665],\n",
            "        [ 2.2036, -2.8288],\n",
            "        [ 2.0425, -2.4244],\n",
            "        [-2.4972,  2.9925],\n",
            "        [ 1.8047, -2.1829],\n",
            "        [ 0.7752, -0.5341],\n",
            "        [-2.4780,  3.0038]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[ 1.2434013 -0.9599529]\n",
            " [-2.17125    2.623235 ]\n",
            " [ 1.3377014 -1.5613754]\n",
            " [-2.301034   2.8598611]\n",
            " [-0.1887658  0.1848419]\n",
            " [-1.7107624  2.1229975]\n",
            " [-2.5294945  2.9169223]\n",
            " [-2.499924   2.8481321]\n",
            " [-2.5346696  2.8838253]\n",
            " [-2.5903716  2.88168  ]\n",
            " [-2.5841281  2.8932233]\n",
            " [-2.5795808  2.8704636]\n",
            " [-2.5596652  2.8050346]\n",
            " [-2.531268   2.8737805]\n",
            " [-2.5554488  2.8174462]\n",
            " [-2.5190768  2.7790456]\n",
            " [-2.5116699  2.860002 ]\n",
            " [-2.5112083  2.774677 ]\n",
            " [-2.5490777  2.805539 ]\n",
            " [-2.5066538  2.7706842]\n",
            " [-2.4619677  2.9320734]\n",
            " [-1.6262099  1.8846948]\n",
            " [-2.2423842  2.648382 ]\n",
            " [-2.1028     2.5471148]\n",
            " [-0.9229291  0.9307861]\n",
            " [-1.8176697  2.5665472]\n",
            " [ 2.2035682 -2.828811 ]\n",
            " [ 2.0425417 -2.4243917]\n",
            " [-2.4971912  2.9925046]\n",
            " [ 1.804707  -2.1828794]\n",
            " [ 0.7751842 -0.5341287]\n",
            " [-2.4779737  3.0037816]]\n",
            "SequenceClassifierOutput(loss=tensor(0.4119, device='cuda:0'), logits=tensor([[ 1.8102, -2.2686],\n",
            "        [-0.2501,  0.6087],\n",
            "        [-2.6016,  2.8666],\n",
            "        [ 2.1659, -2.6983],\n",
            "        [ 2.1417, -2.6598],\n",
            "        [-2.1670,  2.5737],\n",
            "        [-2.4263,  2.7937],\n",
            "        [-2.5528,  3.0106],\n",
            "        [ 1.9771, -2.1583],\n",
            "        [-1.2804,  1.7951],\n",
            "        [-1.9277,  2.5038],\n",
            "        [-2.4060,  2.6518],\n",
            "        [ 1.7062, -1.8485],\n",
            "        [ 1.3078, -1.4487],\n",
            "        [ 0.0994,  0.2441],\n",
            "        [-2.4117,  2.8496],\n",
            "        [-2.5828,  3.0295],\n",
            "        [ 1.7450, -2.0833],\n",
            "        [-2.3126,  2.9993],\n",
            "        [-2.4990,  2.9014],\n",
            "        [-2.4870,  2.7575],\n",
            "        [ 1.8784, -1.8075],\n",
            "        [-2.5181,  2.8548],\n",
            "        [-2.5226,  2.9638],\n",
            "        [ 1.1637, -1.1202],\n",
            "        [ 0.9662, -0.8744],\n",
            "        [ 0.6732, -0.4617],\n",
            "        [-2.0050,  2.5073],\n",
            "        [-2.3962,  2.9184],\n",
            "        [-1.0169,  1.4186],\n",
            "        [-1.1763,  1.6776],\n",
            "        [-2.4541,  2.8872]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[ 1.8102478  -2.268648  ]\n",
            " [-0.25013387  0.60872054]\n",
            " [-2.6015954   2.866602  ]\n",
            " [ 2.165888   -2.6982667 ]\n",
            " [ 2.1416953  -2.6598387 ]\n",
            " [-2.1669593   2.5737085 ]\n",
            " [-2.4263074   2.7937348 ]\n",
            " [-2.5527968   3.0106218 ]\n",
            " [ 1.9770975  -2.1583056 ]\n",
            " [-1.2804197   1.7951485 ]\n",
            " [-1.9277136   2.503757  ]\n",
            " [-2.4059992   2.6517618 ]\n",
            " [ 1.7061763  -1.8485432 ]\n",
            " [ 1.3078215  -1.4487103 ]\n",
            " [ 0.09939823  0.24414653]\n",
            " [-2.411699    2.8496141 ]\n",
            " [-2.5827734   3.0294802 ]\n",
            " [ 1.745004   -2.0832562 ]\n",
            " [-2.3126285   2.9992824 ]\n",
            " [-2.4989784   2.9014149 ]\n",
            " [-2.4869888   2.7575386 ]\n",
            " [ 1.8784026  -1.8074619 ]\n",
            " [-2.5180535   2.8548086 ]\n",
            " [-2.522571    2.963771  ]\n",
            " [ 1.1636717  -1.1201742 ]\n",
            " [ 0.9662338  -0.8743897 ]\n",
            " [ 0.6731588  -0.46165168]\n",
            " [-2.0049732   2.5073235 ]\n",
            " [-2.3961885   2.9183667 ]\n",
            " [-1.0168866   1.4186207 ]\n",
            " [-1.176335    1.6775942 ]\n",
            " [-2.454068    2.8872473 ]]\n",
            "SequenceClassifierOutput(loss=tensor(0.1207, device='cuda:0'), logits=tensor([[-2.4343,  2.8665],\n",
            "        [ 1.9627, -2.4187],\n",
            "        [-2.4605,  2.9264],\n",
            "        [ 2.0028, -2.4353],\n",
            "        [-2.5441,  2.9343],\n",
            "        [ 0.6415, -0.5251],\n",
            "        [-2.3732,  2.5908],\n",
            "        [-2.4787,  2.8300],\n",
            "        [-2.4798,  2.8569],\n",
            "        [-1.5014,  2.0441],\n",
            "        [-0.7628,  1.3372],\n",
            "        [-2.3823,  2.9757],\n",
            "        [ 1.5162, -1.4923],\n",
            "        [-0.1215,  0.4092],\n",
            "        [-2.4913,  2.9504],\n",
            "        [-2.2392,  2.3739],\n",
            "        [-2.5851,  3.0405],\n",
            "        [-0.6264,  1.3868],\n",
            "        [-2.5036,  2.7341],\n",
            "        [-2.1202,  2.2722],\n",
            "        [-2.5424,  2.9566],\n",
            "        [ 1.6015, -1.7206],\n",
            "        [ 1.6476, -1.8021],\n",
            "        [ 1.3350, -1.4450],\n",
            "        [-2.5661,  2.9408],\n",
            "        [-2.2591,  2.7502],\n",
            "        [-2.2462,  2.6791],\n",
            "        [-2.5728,  2.8399],\n",
            "        [-2.5514,  2.7788],\n",
            "        [-2.4168,  2.6597],\n",
            "        [-2.3245,  2.8158],\n",
            "        [-2.6076,  3.0243]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[-2.4342666   2.8664963 ]\n",
            " [ 1.9626948  -2.4187434 ]\n",
            " [-2.4605086   2.9264061 ]\n",
            " [ 2.0027618  -2.435274  ]\n",
            " [-2.5441191   2.9343455 ]\n",
            " [ 0.64150006 -0.5250755 ]\n",
            " [-2.3731759   2.5908334 ]\n",
            " [-2.4787488   2.8300464 ]\n",
            " [-2.4798458   2.8568876 ]\n",
            " [-1.5014309   2.0441294 ]\n",
            " [-0.76280123  1.3372017 ]\n",
            " [-2.3823466   2.9756765 ]\n",
            " [ 1.5162042  -1.4923283 ]\n",
            " [-0.12146682  0.40923476]\n",
            " [-2.4913242   2.9504216 ]\n",
            " [-2.2392414   2.373893  ]\n",
            " [-2.5850725   3.04047   ]\n",
            " [-0.62641335  1.3867825 ]\n",
            " [-2.5035737   2.7340832 ]\n",
            " [-2.1202145   2.2722156 ]\n",
            " [-2.5423734   2.956632  ]\n",
            " [ 1.6015276  -1.7205802 ]\n",
            " [ 1.6475569  -1.8020505 ]\n",
            " [ 1.3350356  -1.4449826 ]\n",
            " [-2.5661156   2.9408267 ]\n",
            " [-2.2591155   2.7501972 ]\n",
            " [-2.2462285   2.6790652 ]\n",
            " [-2.5727813   2.839896  ]\n",
            " [-2.5513558   2.7787645 ]\n",
            " [-2.4168284   2.659703  ]\n",
            " [-2.3245304   2.8158045 ]\n",
            " [-2.6075583   3.024263  ]]\n",
            "SequenceClassifierOutput(loss=tensor(0.3754, device='cuda:0'), logits=tensor([[-2.5401,  2.8806],\n",
            "        [-1.1692,  1.6264],\n",
            "        [-2.5246,  2.8330],\n",
            "        [-2.6108,  3.1221],\n",
            "        [-1.7755,  2.3767],\n",
            "        [-1.6623,  2.3419],\n",
            "        [-2.3950,  2.7233],\n",
            "        [-2.5401,  2.9228],\n",
            "        [ 1.2686, -1.5314],\n",
            "        [-2.5020,  2.7374],\n",
            "        [ 1.7984, -2.4264],\n",
            "        [ 0.5218, -0.4184],\n",
            "        [-1.2856,  1.6073],\n",
            "        [-2.5833,  2.8847],\n",
            "        [-2.4314,  3.0567],\n",
            "        [-2.4545,  3.0361],\n",
            "        [-2.6238,  3.0175],\n",
            "        [-1.9743,  2.1550],\n",
            "        [-0.8787,  1.2480],\n",
            "        [-2.3468,  2.7318],\n",
            "        [ 0.5672, -0.6648],\n",
            "        [-1.5255,  1.7792],\n",
            "        [-1.9332,  2.1408],\n",
            "        [-2.1142,  2.6892],\n",
            "        [-2.5243,  2.8826],\n",
            "        [-2.5315,  2.9486],\n",
            "        [-2.4163,  2.9115],\n",
            "        [ 2.1413, -2.7196],\n",
            "        [ 2.0423, -2.8047],\n",
            "        [-2.5358,  2.9487],\n",
            "        [-2.5324,  2.8985],\n",
            "        [-2.5581,  2.9766]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[-2.5400774   2.8805785 ]\n",
            " [-1.1692349   1.6263617 ]\n",
            " [-2.524576    2.833034  ]\n",
            " [-2.61077     3.1220675 ]\n",
            " [-1.7754579   2.376715  ]\n",
            " [-1.662295    2.3418725 ]\n",
            " [-2.3949645   2.723319  ]\n",
            " [-2.5401478   2.9228387 ]\n",
            " [ 1.2685584  -1.5313829 ]\n",
            " [-2.5020218   2.737388  ]\n",
            " [ 1.7983698  -2.4263525 ]\n",
            " [ 0.5217818  -0.41836363]\n",
            " [-1.2856157   1.6073245 ]\n",
            " [-2.5833247   2.884745  ]\n",
            " [-2.4313817   3.0566552 ]\n",
            " [-2.4544985   3.0361211 ]\n",
            " [-2.6237981   3.0174522 ]\n",
            " [-1.9743485   2.155021  ]\n",
            " [-0.8786741   1.2479502 ]\n",
            " [-2.3468268   2.7318132 ]\n",
            " [ 0.5672499  -0.6648484 ]\n",
            " [-1.5255464   1.779214  ]\n",
            " [-1.9331943   2.1407685 ]\n",
            " [-2.1142347   2.6892211 ]\n",
            " [-2.5243356   2.8825557 ]\n",
            " [-2.5314887   2.9485872 ]\n",
            " [-2.416313    2.9115222 ]\n",
            " [ 2.1413374  -2.7195866 ]\n",
            " [ 2.0422657  -2.8046618 ]\n",
            " [-2.535787    2.9486926 ]\n",
            " [-2.5324075   2.8985004 ]\n",
            " [-2.5581079   2.9765985 ]]\n",
            "SequenceClassifierOutput(loss=tensor(0.3182, device='cuda:0'), logits=tensor([[ 1.7698, -2.1811],\n",
            "        [ 1.3912, -1.5716],\n",
            "        [-1.9516,  2.2904],\n",
            "        [-2.3976,  2.6510],\n",
            "        [-1.5154,  1.8590],\n",
            "        [-2.4053,  2.8337],\n",
            "        [-2.3701,  2.9186],\n",
            "        [-2.5355,  2.9875],\n",
            "        [ 1.6118, -1.6600],\n",
            "        [-2.5397,  2.9750],\n",
            "        [ 0.5437, -0.3788],\n",
            "        [ 2.2245, -2.7213],\n",
            "        [ 0.1572,  0.3738],\n",
            "        [-2.5208,  3.0562],\n",
            "        [-2.5567,  2.9402],\n",
            "        [-0.2873,  0.2709],\n",
            "        [-2.5589,  3.0186],\n",
            "        [-2.5771,  2.9205],\n",
            "        [-2.5499,  2.8533],\n",
            "        [ 1.8524, -2.6883],\n",
            "        [-2.4989,  2.8418],\n",
            "        [-2.5202,  3.0328],\n",
            "        [-2.3210,  2.7379],\n",
            "        [ 1.7590, -1.6980],\n",
            "        [-2.5276,  2.8186],\n",
            "        [-2.5046,  2.9049],\n",
            "        [-2.6082,  3.0382],\n",
            "        [ 2.0205, -2.7595],\n",
            "        [ 1.1311, -1.0527],\n",
            "        [-2.0701,  2.4601],\n",
            "        [-2.5118,  2.8131],\n",
            "        [-2.4435,  2.6645]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[ 1.7697784  -2.1811    ]\n",
            " [ 1.3912132  -1.5716062 ]\n",
            " [-1.9515799   2.2903926 ]\n",
            " [-2.3975992   2.650985  ]\n",
            " [-1.5154153   1.8589801 ]\n",
            " [-2.4053051   2.833678  ]\n",
            " [-2.3701093   2.918579  ]\n",
            " [-2.5354934   2.9874651 ]\n",
            " [ 1.6117591  -1.6600424 ]\n",
            " [-2.5397432   2.9749744 ]\n",
            " [ 0.5437024  -0.37881166]\n",
            " [ 2.2244713  -2.7212574 ]\n",
            " [ 0.1571526   0.37376887]\n",
            " [-2.52076     3.056219  ]\n",
            " [-2.5566897   2.9402082 ]\n",
            " [-0.2873114   0.270922  ]\n",
            " [-2.5588539   3.0186238 ]\n",
            " [-2.5771112   2.920498  ]\n",
            " [-2.5499213   2.853277  ]\n",
            " [ 1.8524381  -2.6883018 ]\n",
            " [-2.4989016   2.8417974 ]\n",
            " [-2.5201516   3.0328484 ]\n",
            " [-2.320987    2.737944  ]\n",
            " [ 1.7589611  -1.6980315 ]\n",
            " [-2.5276432   2.818649  ]\n",
            " [-2.504594    2.9049485 ]\n",
            " [-2.6082168   3.0381777 ]\n",
            " [ 2.0204515  -2.7594883 ]\n",
            " [ 1.1311356  -1.0527061 ]\n",
            " [-2.0701208   2.4600933 ]\n",
            " [-2.5118425   2.8130755 ]\n",
            " [-2.443529    2.6644638 ]]\n",
            "SequenceClassifierOutput(loss=tensor(0.2774, device='cuda:0'), logits=tensor([[-1.7795,  1.9368],\n",
            "        [-2.4793,  2.8511],\n",
            "        [-2.5694,  2.9467],\n",
            "        [-2.5315,  2.8948],\n",
            "        [ 2.2040, -2.5033],\n",
            "        [ 0.6908, -0.6373],\n",
            "        [-2.4489,  2.8643],\n",
            "        [-2.4880,  2.9373],\n",
            "        [-2.5756,  3.0200],\n",
            "        [-2.5865,  2.9321],\n",
            "        [-2.3493,  2.7568],\n",
            "        [-2.4014,  2.8844],\n",
            "        [-2.4269,  2.7560],\n",
            "        [-0.8938,  1.2061],\n",
            "        [-2.5690,  3.0539],\n",
            "        [-2.2626,  2.7872],\n",
            "        [-2.5376,  2.8944],\n",
            "        [-2.4332,  2.7054],\n",
            "        [-2.3689,  2.7553],\n",
            "        [-0.2657,  0.7178],\n",
            "        [ 2.3293, -2.8091],\n",
            "        [-1.7857,  2.2455],\n",
            "        [-2.5161,  2.9547],\n",
            "        [-2.4654,  2.7292],\n",
            "        [-0.3236,  0.5982],\n",
            "        [ 0.9595, -0.8118],\n",
            "        [-2.5381,  2.7605],\n",
            "        [-2.4901,  2.8532],\n",
            "        [-2.3306,  2.5562],\n",
            "        [ 2.0005, -2.7815],\n",
            "        [ 2.1750, -2.7749],\n",
            "        [ 2.0581, -2.7210]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[-1.7794541   1.9367716 ]\n",
            " [-2.4792945   2.851137  ]\n",
            " [-2.5694354   2.946655  ]\n",
            " [-2.5314784   2.894843  ]\n",
            " [ 2.2039518  -2.503349  ]\n",
            " [ 0.69083047 -0.63727975]\n",
            " [-2.4489124   2.8642952 ]\n",
            " [-2.4879696   2.9373372 ]\n",
            " [-2.5756402   3.0199716 ]\n",
            " [-2.5865371   2.9320748 ]\n",
            " [-2.3493223   2.7568123 ]\n",
            " [-2.4014153   2.884412  ]\n",
            " [-2.4269266   2.756007  ]\n",
            " [-0.8938319   1.2060735 ]\n",
            " [-2.5690477   3.0539103 ]\n",
            " [-2.262633    2.7872217 ]\n",
            " [-2.53762     2.8944192 ]\n",
            " [-2.433213    2.7054014 ]\n",
            " [-2.3688912   2.7552955 ]\n",
            " [-0.26574007  0.7178121 ]\n",
            " [ 2.3292813  -2.8090603 ]\n",
            " [-1.785661    2.2454793 ]\n",
            " [-2.516076    2.9547021 ]\n",
            " [-2.4654238   2.7292187 ]\n",
            " [-0.32360423  0.59817684]\n",
            " [ 0.9594812  -0.8117799 ]\n",
            " [-2.53808     2.760517  ]\n",
            " [-2.4900637   2.8531525 ]\n",
            " [-2.3306491   2.5561996 ]\n",
            " [ 2.0004928  -2.7815413 ]\n",
            " [ 2.1750238  -2.774866  ]\n",
            " [ 2.0580842  -2.7209892 ]]\n",
            "SequenceClassifierOutput(loss=tensor(0.5204, device='cuda:0'), logits=tensor([[-2.4454,  2.7630],\n",
            "        [-2.3774,  2.7823],\n",
            "        [-2.5862,  2.9217],\n",
            "        [-2.5744,  2.9094],\n",
            "        [ 1.9365, -2.2529],\n",
            "        [-1.4744,  1.7948],\n",
            "        [-2.5476,  2.9355],\n",
            "        [-2.3445,  2.6669],\n",
            "        [-2.5106,  2.8049],\n",
            "        [-2.4092,  2.8670],\n",
            "        [ 0.9455, -0.7956],\n",
            "        [-2.4159,  2.8960],\n",
            "        [-2.5334,  2.8887],\n",
            "        [-2.5794,  2.8106],\n",
            "        [-2.4894,  2.7201],\n",
            "        [-0.7171,  1.0126],\n",
            "        [-0.1046,  0.1027],\n",
            "        [ 2.1285, -2.8292],\n",
            "        [-2.4404,  2.7595],\n",
            "        [-2.3662,  2.6232],\n",
            "        [-2.4460,  2.9440],\n",
            "        [-2.6078,  2.9010],\n",
            "        [-2.6310,  2.9467],\n",
            "        [-2.6180,  2.9471],\n",
            "        [ 0.2355, -0.3015],\n",
            "        [ 0.5820, -0.5812],\n",
            "        [ 1.9003, -2.3657],\n",
            "        [-2.5596,  2.8537],\n",
            "        [-2.0689,  2.2595],\n",
            "        [ 1.4632, -1.5397],\n",
            "        [-2.2468,  2.6106],\n",
            "        [-2.0700,  2.3394]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[-2.4454029   2.7629936 ]\n",
            " [-2.3773801   2.7822533 ]\n",
            " [-2.5861964   2.921724  ]\n",
            " [-2.5743659   2.9094057 ]\n",
            " [ 1.9365174  -2.2529252 ]\n",
            " [-1.4743754   1.7947689 ]\n",
            " [-2.5476303   2.9355242 ]\n",
            " [-2.344535    2.666885  ]\n",
            " [-2.5106363   2.8048582 ]\n",
            " [-2.4091856   2.8670099 ]\n",
            " [ 0.94552386 -0.795638  ]\n",
            " [-2.4158902   2.8960333 ]\n",
            " [-2.5333755   2.888676  ]\n",
            " [-2.5793831   2.810593  ]\n",
            " [-2.4893563   2.7201183 ]\n",
            " [-0.71708935  1.0126392 ]\n",
            " [-0.10460181  0.10274541]\n",
            " [ 2.128535   -2.8291662 ]\n",
            " [-2.440439    2.7594817 ]\n",
            " [-2.3662496   2.6231806 ]\n",
            " [-2.446045    2.944045  ]\n",
            " [-2.6078236   2.901006  ]\n",
            " [-2.6310437   2.9466922 ]\n",
            " [-2.6179852   2.9470959 ]\n",
            " [ 0.23550656 -0.30154753]\n",
            " [ 0.5820281  -0.58120483]\n",
            " [ 1.9002702  -2.365688  ]\n",
            " [-2.5596      2.8536518 ]\n",
            " [-2.068906    2.2594588 ]\n",
            " [ 1.4631792  -1.5397264 ]\n",
            " [-2.2468078   2.6106398 ]\n",
            " [-2.0699654   2.3394072 ]]\n",
            "SequenceClassifierOutput(loss=tensor(0.6942, device='cuda:0'), logits=tensor([[ 1.4958, -1.4989],\n",
            "        [-2.3735,  2.6699],\n",
            "        [ 1.2926, -1.3801],\n",
            "        [ 2.0928, -2.7438],\n",
            "        [-2.3053,  2.5944],\n",
            "        [ 2.3277, -2.8939],\n",
            "        [ 2.1873, -2.5289],\n",
            "        [ 2.3496, -2.8369],\n",
            "        [-2.6132,  3.0128],\n",
            "        [ 1.9625, -2.2683],\n",
            "        [ 2.2244, -2.6268],\n",
            "        [-2.3949,  2.5896],\n",
            "        [-1.8944,  2.2457],\n",
            "        [ 1.9519, -2.2768],\n",
            "        [-2.1048,  2.5804],\n",
            "        [-2.2537,  2.8010],\n",
            "        [-1.2965,  1.6502],\n",
            "        [-1.4709,  1.7046],\n",
            "        [ 0.4606, -0.2517],\n",
            "        [-2.1091,  2.6256],\n",
            "        [-2.4582,  2.9816],\n",
            "        [-1.5839,  2.2217],\n",
            "        [-2.5419,  2.8676],\n",
            "        [-1.7691,  2.1884],\n",
            "        [-0.2803,  0.5763],\n",
            "        [-2.3891,  2.6401],\n",
            "        [-2.5931,  2.9777],\n",
            "        [-2.0537,  2.3280],\n",
            "        [-2.2816,  2.5085],\n",
            "        [-2.2619,  2.7024],\n",
            "        [-1.4897,  1.7534],\n",
            "        [-2.6223,  2.9377]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[ 1.495809   -1.4989328 ]\n",
            " [-2.3734863   2.6698823 ]\n",
            " [ 1.2926203  -1.3800982 ]\n",
            " [ 2.0928326  -2.7437625 ]\n",
            " [-2.3052697   2.59445   ]\n",
            " [ 2.3276532  -2.893851  ]\n",
            " [ 2.1872563  -2.5288532 ]\n",
            " [ 2.349581   -2.8369222 ]\n",
            " [-2.6131895   3.0128372 ]\n",
            " [ 1.9624912  -2.2683089 ]\n",
            " [ 2.2243977  -2.626846  ]\n",
            " [-2.3948824   2.589605  ]\n",
            " [-1.8944494   2.2457266 ]\n",
            " [ 1.9519094  -2.2768366 ]\n",
            " [-2.1047885   2.5804362 ]\n",
            " [-2.253699    2.8010461 ]\n",
            " [-1.2965113   1.650174  ]\n",
            " [-1.4708542   1.7046393 ]\n",
            " [ 0.4605977  -0.25165543]\n",
            " [-2.1090848   2.6256037 ]\n",
            " [-2.4581609   2.9816303 ]\n",
            " [-1.583887    2.2216532 ]\n",
            " [-2.5418599   2.8675852 ]\n",
            " [-1.769095    2.1883767 ]\n",
            " [-0.28031904  0.57629955]\n",
            " [-2.3890812   2.6401317 ]\n",
            " [-2.5931385   2.9777114 ]\n",
            " [-2.0536885   2.3279798 ]\n",
            " [-2.2815795   2.5085375 ]\n",
            " [-2.2619004   2.702414  ]\n",
            " [-1.4896598   1.7533674 ]\n",
            " [-2.6223333   2.9376538 ]]\n",
            "SequenceClassifierOutput(loss=tensor(1.3947, device='cuda:0'), logits=tensor([[-2.6226,  2.9575],\n",
            "        [-2.6055,  2.9487],\n",
            "        [-2.1453,  2.4218],\n",
            "        [-2.3383,  2.6657]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "///////////////\n",
            "[[-2.6225567  2.9574597]\n",
            " [-2.6055028  2.9487412]\n",
            " [-2.1452591  2.4218183]\n",
            " [-2.3382673  2.6656775]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(true_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGs6vmxFj5Vh",
        "outputId": "872e9967-90ba-4820-8557-65c947f37c01"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Evaluating Using Matthew's Correlation Coefficient\n",
        "\n",
        "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "matthews_set = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "    matthews = matthews_corrcoef(true_labels[i], np.argmax(predictions[i], axis=1).flatten())\n",
        "    matthews_set.append(matthews)\n",
        "\n",
        "matthews_set\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DxbiIbFfthI",
        "outputId": "b61aba38-3e37-43de-841d-5feb3b8db343"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.049286405809014416,\n",
              " -0.21684543705982773,\n",
              " 0.4040950971038548,\n",
              " 0.23372319715296222,\n",
              " 0.21867346044008387,\n",
              " 0.6777932975034471,\n",
              " 0.3768673314407159,\n",
              " 0.0,\n",
              " 0.8320502943378436,\n",
              " 0.7704873741021288,\n",
              " 0.8459051693633014,\n",
              " 0.647150228929434,\n",
              " 0.7562449037944323,\n",
              " 0.7141684885491869,\n",
              " 0.3268228676411533,\n",
              " 0.5056936741642399,\n",
              " 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Matthew's Evaluation on the Whole Dataset\n",
        "\n",
        "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "matthews_corrcoef(flat_true_labels, flat_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOXY4pE9hHsw",
        "outputId": "b9e4f5c6-c9b3-4e23-8b84-806c649360fd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4928725187819344"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## calcute MCC manually\n",
        "\n",
        "FP = np.array(flat_predictions)\n",
        "FTL = np.array(flat_true_labels)\n",
        "\n",
        "TP = np.sum( (FP == 1) & (FTL == 1) )\n",
        "TN = np.sum( (FP == 0) & (FTL == 0) )\n",
        "FP = np.sum( (FP == 1) & (FTL == 0) )\n",
        "FN = np.sum( (FP == 0) & (FTL == 1) )\n",
        "print(f'TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}')\n",
        "\n",
        "\n",
        "numerator = (TP * TN) - (FP * FN)\n",
        "denominator = np.sqrt( (TP + FP) * (TN + FN) * (TP + FN) * (TN + FP)  )\n",
        "\n",
        "if denominator==0 :\n",
        "    MCC = 0\n",
        "else :\n",
        "    MCC = numerator / denominator\n",
        "print(MCC)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHJHHMbvjQZY",
        "outputId": "31641c02-61bc-4160-d552-7d2234e7e7a0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TP: 330, TN: 80, FP: 82, FN: 0\n",
            "0.628920640653653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fR50u9kfVK14"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}